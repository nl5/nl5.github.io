WEBVTT
Kind: captions
Language: en

00:00:00.500 --> 00:00:01.229
(speaker) Array.

00:00:01.229 --> 00:00:03.846
So now we're going
to build the graph.

00:00:03.846 --> 00:00:05.220
So first thing
that we need to do

00:00:05.219 --> 00:00:07.330
is define our hyperparameters.

00:00:07.330 --> 00:00:09.730
The first one is the LSTM size.

00:00:09.730 --> 00:00:11.880
So this is the number
of units in the hidden

00:00:11.880 --> 00:00:14.170
layers in the LSTM cells.

00:00:14.169 --> 00:00:17.039
So LSTM cells actually have
four different network layers

00:00:17.039 --> 00:00:17.759
in them.

00:00:17.760 --> 00:00:21.100
There's like three sigma
layers and one TNH layer.

00:00:21.100 --> 00:00:24.609
So this is the number of
units in each of those layers.

00:00:24.609 --> 00:00:28.710
So if you set this
to 256, then there's

00:00:28.710 --> 00:00:32.310
256 units in each of
those four layers.

00:00:32.310 --> 00:00:34.560
Otherwise, you can
basically just think of this

00:00:34.560 --> 00:00:37.380
as like setting the number of
units in your hidden layer.

00:00:37.380 --> 00:00:39.990
It's kind of all it is,
except LSTM cells are

00:00:39.990 --> 00:00:42.900
way more complicated than
just a normal hidden layer.

00:00:42.899 --> 00:00:44.640
However, that's the same idea.

00:00:44.640 --> 00:00:46.230
I mean, you're just
setting the number

00:00:46.229 --> 00:00:47.709
of units in your hidden layer.

00:00:47.710 --> 00:00:51.840
And as you probably know
by now, that the more units

00:00:51.840 --> 00:00:54.420
you have in your hidden layer,
then the better performance

00:00:54.420 --> 00:00:56.670
you're going to get
out of your network

00:00:56.670 --> 00:01:00.060
at the expense of
computation, of course.

00:01:00.060 --> 00:01:03.910
Next is our LSTM
layers hyperparameter.

00:01:03.909 --> 00:01:06.209
So this just says
the number of layers

00:01:06.209 --> 00:01:09.779
that you are using
for your LSTMs.

00:01:09.780 --> 00:01:11.640
So this is deep learning.

00:01:11.640 --> 00:01:14.769
So this is just setting the
number of layers we have,

00:01:14.769 --> 00:01:17.369
the number of hidden layers
in LSTM cells we're using.

00:01:17.370 --> 00:01:19.710
So again, as usual, the
more layers you have,

00:01:19.709 --> 00:01:21.959
typically the better performance
you're going to have.

00:01:21.959 --> 00:01:24.599
But then also you
run into overfitting.

00:01:24.599 --> 00:01:27.059
So what I've seen is
that a good practice

00:01:27.060 --> 00:01:33.750
is to make your network large
and be fine with overfitting,

00:01:33.750 --> 00:01:36.810
but then add regularization
like dropout to it

00:01:36.810 --> 00:01:39.760
to control overfitting.

00:01:39.760 --> 00:01:43.359
That being said, I'd start
with 1 and see how it does.

00:01:43.359 --> 00:01:49.599
And then if you feel like you
need to add another layer,

00:01:49.599 --> 00:01:51.879
you can do that and then
see how the validation

00:01:51.879 --> 00:01:53.469
accuracy responds.

00:01:53.469 --> 00:01:55.438
So next is the batch size.

00:01:55.438 --> 00:01:56.979
So this is just the
number of reviews

00:01:56.980 --> 00:02:00.130
that we're going to feed to our
network in one training pass.

00:02:00.129 --> 00:02:02.530
Typically, you want is
as high as you can go

00:02:02.530 --> 00:02:05.000
without running out of memory.

00:02:05.000 --> 00:02:08.288
The reason this is because
our networks and TensorFlow

00:02:08.288 --> 00:02:13.170
is really good at doing
calculations on matrices.

00:02:13.169 --> 00:02:14.829
And so then the
fewer matrices we

00:02:14.830 --> 00:02:18.130
have to pass in, then
the less computation

00:02:18.129 --> 00:02:19.539
time it's going to take.

00:02:19.539 --> 00:02:23.469
However, if you have a lot of
weights, a lot of connections

00:02:23.469 --> 00:02:26.199
in your network,
then you're going

00:02:26.199 --> 00:02:32.709
to quickly hit this memory
limit in your GPU or computer.

00:02:32.710 --> 00:02:35.230
So basically, the way
to do this is just

00:02:35.229 --> 00:02:38.569
to make this as large as you can
before you run out of memory.

00:02:38.569 --> 00:02:40.389
And finally, we have
a learning rate.

00:02:40.389 --> 00:02:44.589
So here is the first step
in creating your network.

00:02:44.590 --> 00:02:48.460
So just defining placeholders
for the inputs and the labels.

00:02:48.460 --> 00:02:51.159
And then we're also going
to be using dropout here,

00:02:51.159 --> 00:02:54.939
so we want to make a placeholder
for the key probability

00:02:54.939 --> 00:02:57.349
in our dropout layer.

00:02:57.349 --> 00:02:59.669
OK, I'll leave this up to you.

00:02:59.669 --> 00:03:02.449
So the next thing is going
to be our embedding layer.

00:03:02.449 --> 00:03:06.919
So this layer here is colored
yellow in our diagram.

00:03:06.919 --> 00:03:08.619
So again, we're
dealing with something

00:03:08.620 --> 00:03:11.900
like 74,000 words
in our vocabulary.

00:03:11.900 --> 00:03:13.599
So if you try to
[INAUDIBLE] code this,

00:03:13.599 --> 00:03:16.000
it's going to be
massively inefficient.

00:03:16.000 --> 00:03:18.762
So instead, what we're going
to do is just pass in integers

00:03:18.762 --> 00:03:20.469
and we're going to
use an embedding layer

00:03:20.469 --> 00:03:23.199
and use that layer
as a lookup table.

00:03:23.199 --> 00:03:26.239
So you saw how we did this
in the word2vec lesson.

00:03:26.240 --> 00:03:29.469
So what you could do is actually
train up an embedding layer

00:03:29.469 --> 00:03:32.319
with word2vec and then
just import that in here.

00:03:32.319 --> 00:03:36.250
However, it turns out
that for most cases,

00:03:36.250 --> 00:03:38.419
unless you have like
massive amounts of data,

00:03:38.419 --> 00:03:42.009
then it's usually just fine
to make a new embedding layer

00:03:42.009 --> 00:03:44.469
and then let the network
train up those connections

00:03:44.469 --> 00:03:47.449
and find representations
on its own.

00:03:47.449 --> 00:03:51.519
So here I'll let you create
the embedding matrix with

00:03:51.520 --> 00:03:58.310
tf.variable and use
tf.nn.embedding_lookup to get

00:03:58.310 --> 00:04:02.900
the vectors that were going
to pass to our LSTM cells.

00:04:02.900 --> 00:04:04.390
So if you need some
help with this,

00:04:04.389 --> 00:04:07.659
go ahead and go back
to the word2vec lesson

00:04:07.659 --> 00:04:08.859
and see how I did it there.

00:04:08.860 --> 00:04:11.650
Also, you can read documentation
on embedding_lookup

00:04:11.650 --> 00:04:13.490
by clicking this.

00:04:13.490 --> 00:04:16.879
Next, we're going to
define our LSTM cells.

00:04:16.879 --> 00:04:18.990
So we're not actually
going to be building

00:04:18.990 --> 00:04:20.490
the network in this
part, we're just

00:04:20.490 --> 00:04:24.079
going to be building what's
inside these LSTM cells.

00:04:24.079 --> 00:04:28.389
So here you can check out this
documentation on TensorFlow's

00:04:28.389 --> 00:04:30.769
recurrent network functions.

00:04:30.769 --> 00:04:33.129
So the first one that
we're going to use

00:04:33.129 --> 00:04:34.959
is just a basic LSTM cell.

00:04:34.959 --> 00:04:39.489
So you can get that with
tf.contrib.rnn.BasicLSTMCell.

00:04:39.490 --> 00:04:43.000
So this is what the function
documentation looks like

00:04:43.000 --> 00:04:45.279
and basically you
just create the cell

00:04:45.279 --> 00:04:47.529
and then you tell the
number of units to use.

00:04:47.529 --> 00:04:53.089
In our case, I called this
LSTM size, above there.

00:04:53.089 --> 00:04:57.259
So you can just say,
LSTM is equal to basic,

00:04:57.259 --> 00:05:02.069
all those basic LSTM cell, LSTM
size, and that's all you need.

00:05:02.069 --> 00:05:03.689
It looks something like that.

00:05:03.689 --> 00:05:08.160
So next, we're going to add
dropout to our LSTM cells.

00:05:08.160 --> 00:05:12.020
And that's really useful
because it just kind of makes

00:05:12.019 --> 00:05:17.069
your network better
with very little effort.

00:05:17.069 --> 00:05:20.629
So if you remember, dropout
as a regularization technique

00:05:20.629 --> 00:05:23.060
that can help us
prevent overfitting.

00:05:23.060 --> 00:05:26.360
With these LSTM units,
it's really easy

00:05:26.360 --> 00:05:31.400
to use dropout with this
class called dropout_wrapper.

00:05:31.399 --> 00:05:33.419
So basically, you
just pass in cell

00:05:33.420 --> 00:05:35.810
and you pass in your
key probability, which

00:05:35.810 --> 00:05:38.030
we defined as a
placeholder above,

00:05:38.029 --> 00:05:41.449
and then you just
have this drop.

00:05:41.449 --> 00:05:44.420
So it basically acts
exactly like an LSTM cell,

00:05:44.420 --> 00:05:48.750
except now, your LSTM cell
has dropout on the output.

00:05:48.750 --> 00:05:52.670
So again, you have this
option to add more layers.

00:05:52.670 --> 00:05:57.410
And TensorFlow also
makes this really simple

00:05:57.410 --> 00:06:01.550
using
tf.contrib.rnn.MultiRNNCell.

00:06:01.550 --> 00:06:06.290
So here you just pass
in a list of cells.

00:06:06.290 --> 00:06:09.050
So in our case, since we
have this dropout, like I

00:06:09.050 --> 00:06:11.555
was saying, this is
just a normal LSTM cell,

00:06:11.555 --> 00:06:13.699
this is like wrapped
in like dropout,

00:06:13.699 --> 00:06:15.589
it is just adding
dropout on the end.

00:06:15.589 --> 00:06:17.899
So now we can just
create this list that

00:06:17.899 --> 00:06:20.810
is however many
layers long and then

00:06:20.810 --> 00:06:23.209
pass this to multi RNN cell.

00:06:23.209 --> 00:06:27.889
So this is just going to
create a list of cells

00:06:27.889 --> 00:06:30.870
and then it's going to
just build layers for us.

00:06:30.870 --> 00:06:35.480
So here use all of this
to build the LSTM cell.

00:06:35.480 --> 00:06:38.165
So here you're going to create
just your basic LSTM cell,

00:06:38.165 --> 00:06:40.040
then you're going to
add dropout to it,

00:06:40.040 --> 00:06:43.220
and then you can stack it
up in multiple LSTM layers.

00:06:43.220 --> 00:06:46.980
So here I'm just using the
cell to get an initial state.

00:06:46.980 --> 00:06:50.120
So if you remember,
LSTMs have cell states

00:06:50.120 --> 00:06:54.050
that are passed
between our LSTM cells.

00:06:54.050 --> 00:06:57.031
And this is just creating that
initial cell state that's all

00:06:57.031 --> 00:06:57.530
0's.

00:06:57.529 --> 00:07:00.049
And this is going to get
updated through training

00:07:00.050 --> 00:07:03.170
and through passing
data in as a sequence.

00:07:03.170 --> 00:07:08.223
So finally, we're going to
build our RNN forward pass.

00:07:08.223 --> 00:07:10.639
So what this means is that
we're going to pass in our data

00:07:10.639 --> 00:07:12.289
and then it's going
to go through here

00:07:12.290 --> 00:07:15.170
and then actually
calculate the outputs.

00:07:15.170 --> 00:07:19.370
So the whole reason that
I'm using an RNN here

00:07:19.370 --> 00:07:21.860
is that we can actually
get information

00:07:21.860 --> 00:07:28.400
about the sequence of our
words and our reviews.

00:07:28.399 --> 00:07:31.279
So you might remember
from Andrew Trask's lesson

00:07:31.279 --> 00:07:33.109
that he just used a
feedforward network.

00:07:33.110 --> 00:07:35.150
So in that case,
the network only

00:07:35.149 --> 00:07:38.419
knows about the individual
words that were in the review.

00:07:38.420 --> 00:07:41.069
But in this case, we
have the individual words

00:07:41.069 --> 00:07:43.519
that were in the review,
but also the sequence

00:07:43.519 --> 00:07:44.689
that they appeared in.

00:07:44.689 --> 00:07:48.110
So the individual words
are coming through here,

00:07:48.110 --> 00:07:51.590
through the embed layer, so they
have this kind of vertical path

00:07:51.589 --> 00:07:53.419
through the network.

00:07:53.420 --> 00:07:56.300
But then, since we're using
a recurrent neural network,

00:07:56.300 --> 00:08:00.079
we also have this
sequential information

00:08:00.079 --> 00:08:02.509
that's getting passed
through the hidden layer.

00:08:02.509 --> 00:08:05.539
So our output here,
"positive," knows

00:08:05.540 --> 00:08:09.010
that "best" and "movie"
came before "ever,"

00:08:09.009 --> 00:08:13.789
and now we have this information
about the sequence of words

00:08:13.790 --> 00:08:14.720
in our review.

00:08:14.720 --> 00:08:17.600
So this makes it such that
recurrent neural networks

00:08:17.600 --> 00:08:19.010
actually perform
better than just

00:08:19.009 --> 00:08:20.959
normal feedforward networks.

00:08:20.959 --> 00:08:23.659
So the way we're going
to implement this actual

00:08:23.660 --> 00:08:25.850
like running our
data through the RNN

00:08:25.850 --> 00:08:29.879
is with tf.nn.DynamicRNN.

00:08:29.879 --> 00:08:31.550
So what you need
to do here is just

00:08:31.550 --> 00:08:34.788
pass in your cell, which
we already created,

00:08:34.788 --> 00:08:38.629
and the inputs, which
are, in our case,

00:08:38.629 --> 00:08:40.610
coming from our
embedding layer and going

00:08:40.610 --> 00:08:42.206
into the hidden layer.

00:08:42.206 --> 00:08:43.580
So that's what
those inputs mean.

00:08:43.580 --> 00:08:45.600
It's not the inputs
to the network,

00:08:45.600 --> 00:08:48.639
but it's the inputs
to the LSTM cells.

00:08:48.639 --> 00:08:51.259
It's the input to
this hidden layer.

00:08:51.259 --> 00:08:53.240
Then you also give
it the initial state.

00:08:53.240 --> 00:08:56.720
What this does is it actually
goes through this network

00:08:56.720 --> 00:09:00.769
and then calculates the
state for each hidden layer

00:09:00.769 --> 00:09:04.100
and passes it to the next one,
and also calculates the output

00:09:04.100 --> 00:09:06.262
of each of the hidden layers.

00:09:06.261 --> 00:09:07.719
And then when it's
done doing that,

00:09:07.720 --> 00:09:10.279
it gives us this list
outputs and it gives us

00:09:10.279 --> 00:09:11.399
the final state.

00:09:11.399 --> 00:09:17.480
So here I'll leave it up to
you to build this dynamic RNN

00:09:17.480 --> 00:09:21.250
and calculate the outputs
and the final state.

