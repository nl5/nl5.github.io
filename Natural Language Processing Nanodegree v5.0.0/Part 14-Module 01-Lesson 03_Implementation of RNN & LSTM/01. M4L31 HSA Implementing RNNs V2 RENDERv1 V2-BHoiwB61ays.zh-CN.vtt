WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.820
大家好 在前几节课

00:00:02.820 --> 00:00:07.155
Ortel 和 Louise 介绍了递归神经网络和 LSTM

00:00:07.155 --> 00:00:11.515
在这节课 我和 Matt 将演示如何实现这些网络

00:00:11.515 --> 00:00:14.340
因为 RNN 具有内置记忆功能

00:00:14.340 --> 00:00:18.035
因此非常适合对时间或序列有依赖性的任务

00:00:18.035 --> 00:00:22.260
例如 RNN 可以用于时间序列预测等任务

00:00:22.260 --> 00:00:24.630
比如天气预报或者文本生成

00:00:24.630 --> 00:00:27.150
对于文本生成来说

00:00:27.150 --> 00:00:30.285
句子中的单词和字符顺序很重要

00:00:30.285 --> 00:00:35.590
设计和实现任何 RNN 都涉及两大步骤

00:00:35.590 --> 00:00:38.505
首先 如何预处理序列数据

00:00:38.505 --> 00:00:40.200
例如句子序列

00:00:40.200 --> 00:00:44.285
并转换为神经网络能理解的数字数据？

00:00:44.285 --> 00:00:47.520
其次 如何用代码表示记忆？

00:00:47.520 --> 00:00:50.635
我将通过几个代码示例讲解这些挑战

00:00:50.635 --> 00:00:52.100
这节课的目标是

00:00:52.100 --> 00:00:55.245
学习构建和训练字符级 RNN

00:00:55.245 --> 00:01:00.075
我们将构建一个使用托尔斯泰的《安娜·卡列尼娜》文字内容训练过的 LSTM

00:01:00.075 --> 00:01:05.545
该 LSTM 将接受一个输入字符并生成下个预测字符

00:01:05.545 --> 00:01:08.735
你可以使用任何文本训练此类 LSTM

00:01:08.735 --> 00:01:12.019
例如公开的书籍或电视脚本

00:01:12.019 --> 00:01:14.110
并生成很有趣的文字内容

00:01:14.110 --> 00:01:18.920
接下来 我将深入讲解如何构建从序列数据中学习规律的 RNN

