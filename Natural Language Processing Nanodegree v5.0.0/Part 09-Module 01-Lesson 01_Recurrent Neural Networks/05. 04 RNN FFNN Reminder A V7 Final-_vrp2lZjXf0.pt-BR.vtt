WEBVTT
Kind: captions
Language: pt-BR

00:00:00.243 --> 00:00:02.412
Antes de explorarmos
as RNNs,

00:00:02.445 --> 00:00:06.627
vamos recordar o processo que usamos
nas redes neurais feedforward.

00:00:06.660 --> 00:00:10.614
Podemos ter muitas camadas ocultas
entre as entradas e as saídas,

00:00:10.647 --> 00:00:14.491
mas, para simplificar, começaremos
com uma única camada oculta.

00:00:14.524 --> 00:00:18.568
Vamos relembrar por que,
quando e como ela é usada.

00:00:19.577 --> 00:00:24.020
Depois que também tivermos entendido
claramente o contexto matemático,

00:00:24.053 --> 00:00:28.376
vamos dar mais um passo
e abrir as portas para as RNNs.

00:00:28.409 --> 00:00:33.577
Você verá que, após entender bem
as bases da rede feedforward,

00:00:33.610 --> 00:00:36.677
o passo em direção às RNNs
é simples.

00:00:37.933 --> 00:00:39.922
Alguns de vocês
já devem conhecer

00:00:39.955 --> 00:00:44.931
o conceito das redes neurais
convolucionais, ou CNNs.

00:00:44.964 --> 00:00:47.319
Ao implementar
sua rede neural,

00:00:47.352 --> 00:00:50.337
você verá que pode combinar
essas técnicas.

00:00:50.370 --> 00:00:54.817
Por exemplo, você pode usar as CNNs
nas primeiras camadas

00:00:54.850 --> 00:00:57.527
para fins
de extração de recursos,

00:00:57.560 --> 00:01:00.458
depois usar as RNNs
na última camada,

00:01:00.491 --> 00:01:02.746
onde é preciso
considerar a memória.

00:01:02.779 --> 00:01:06.785
Há um programa popular para isso
na área de reconhecimento de gestos.

00:01:06.818 --> 00:01:10.486
Mas não se preocupe.
Se você não conhece CNNs, tudo bem.

00:01:10.519 --> 00:01:12.776
Não é o foco desta aula.

00:01:13.788 --> 00:01:16.666
Ao trabalhar
numa rede neural feedforward,

00:01:16.699 --> 00:01:19.914
nós simulamos
uma rede neural artificial

00:01:19.947 --> 00:01:23.432
usando um cálculo aproximado
de função não linear.

00:01:23.465 --> 00:01:28.755
Essa função atuará como um sistema
que possui um número n de entradas,

00:01:28.788 --> 00:01:32.567
pesos
e um número k de saídas.

00:01:32.600 --> 00:01:35.532
Usaremos x
para o vetor de entrada

00:01:35.565 --> 00:01:38.668
e y para o vetor de saída.

00:01:38.701 --> 00:01:43.254
Entradas e saídas também podem ser
many-to-many, many-to-one,

00:01:43.287 --> 00:01:45.210
e one-to-many.

00:01:45.816 --> 00:01:48.170
Então a rede neural funciona
basicamente

00:01:48.203 --> 00:01:51.672
como um calculador
de funções não lineares.

00:01:51.705 --> 00:01:54.709
O que fazemos é tentar inserir
uma função suave

00:01:54.742 --> 00:01:56.441
entre determinados pontos,

00:01:56.474 --> 00:02:01.343
como (x1,y1), (x2,y2)
e assim por diante,

00:02:01.376 --> 00:02:06.159
de modo que, quando tivermos
uma nova entrada x',

00:02:06.192 --> 00:02:09.811
nós possamos descobrir
a nova entrada y'.

00:02:09.844 --> 00:02:13.262
Explicaremos melhor o cálculo
aproximado de função não linear

00:02:13.295 --> 00:02:14.743
mais adiante na aula.

00:02:15.130 --> 00:02:17.760
Há dois tipos principais
de aplicação.

00:02:17.793 --> 00:02:20.221
Uma delas é a classificação,

00:02:20.254 --> 00:02:23.321
em que identificamos
a categoria específica

00:02:23.354 --> 00:02:25.466
à qual uma nova entrada
pertence.

00:02:25.499 --> 00:02:28.330
Por exemplo,
a classificação de imagens,

00:02:28.363 --> 00:02:33.045
em que a rede neural recebe
uma imagem como entrada

00:02:33.078 --> 00:02:35.295
e consegue identificar
se é um gato.

00:02:35.328 --> 00:02:38.019
A outra aplicação é
a regressão,

00:02:38.052 --> 00:02:40.323
em que calculamos
uma função,

00:02:40.356 --> 00:02:43.285
de modo que a rede gere
valores contínuos,

00:02:43.318 --> 00:02:46.799
seguindo um processo de treinamento
supervisionado.

00:02:46.832 --> 00:02:50.651
Um exemplo simples disso é
a previsão de séries temporais,

00:02:50.684 --> 00:02:53.651
em que prevemos
o preço de uma ação de amanhã

00:02:53.684 --> 00:02:57.869
com base no preço da ação
nos últimos cinco dias.

00:02:57.902 --> 00:03:01.138
A entrada para a rede seriam
5 valores,

00:03:01.171 --> 00:03:05.593
para representar o preço da ação
de cada um dos últimos cinco dias,

00:03:05.626 --> 00:03:08.945
e a saída que queremos é
o preço de amanhã.

00:03:10.028 --> 00:03:14.604
Nossa tarefa nas redes neurais é
encontrar o melhor conjunto de pesos

00:03:14.637 --> 00:03:17.076
que gerem uma boa saída,

00:03:17.109 --> 00:03:20.815
em que x representa
as entradas

00:03:20.848 --> 00:03:24.609
e W representa os pesos.

00:03:24.642 --> 00:03:26.972
Começamos
com pesos aleatórios.

00:03:27.005 --> 00:03:28.826
Nas redes neurais
feedforward,

00:03:28.859 --> 00:03:33.359
temos um mapeamento estático
das entradas para as saídas.

00:03:33.392 --> 00:03:36.919
Usamos a palavra "estático"
porque não temos memória,

00:03:36.952 --> 00:03:42.331
e a saída só depende
das entradas e dos pesos.

00:03:42.364 --> 00:03:45.448
Em outras palavras,
para a mesma entrada

00:03:45.481 --> 00:03:47.883
e os mesmos pesos,

00:03:47.916 --> 00:03:51.796
sempre recebemos
a mesma saída.

00:03:51.829 --> 00:03:55.320
Em geral, quando trabalhamos
com redes neurais,

00:03:55.353 --> 00:04:00.627
temos duas fases principais:
treinamento e avaliação.

00:04:00.660 --> 00:04:01.818
Na fase de treinamento,

00:04:01.851 --> 00:04:04.907
pegamos o conjunto de dados,
ou "conjunto de treinamento",

00:04:04.940 --> 00:04:10.694
que inclui muitos pares de entradas
e os respectivos alvos, ou saídas,

00:04:10.727 --> 00:04:13.970
e o objetivo é encontrar
o conjunto de pesos

00:04:14.003 --> 00:04:18.515
que melhor converterá as entradas
nas saídas desejadas.

00:04:18.548 --> 00:04:21.588
Em outras palavras,
o objetivo da fase de treinamento

00:04:21.621 --> 00:04:26.528
é criar uma rede que se estenda para
além do conjunto de treinamento.

00:04:26.561 --> 00:04:28.522
Na fase de avaliação,

00:04:28.555 --> 00:04:32.852
usamos a rede que foi criada
na fase de treinamento,

00:04:32.885 --> 00:04:35.258
aplicamos
nossas novas entradas

00:04:35.291 --> 00:04:39.510
e esperamos obter
as saídas desejadas.

