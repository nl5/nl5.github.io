WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.924
We still have to adjust Wx.

00:00:02.924 --> 00:00:07.849
The weight matrix connecting the input layer to the hidden or state layer.

00:00:07.849 --> 00:00:12.324
Let's simplify the sketch and leave only what we need.

00:00:12.324 --> 00:00:16.679
You will see that the process we follow to adjust Wx will

00:00:16.679 --> 00:00:21.164
be very similar to the one we used when updating Ws.

00:00:21.164 --> 00:00:25.230
Having said that, let's go over the process in detail.

00:00:25.230 --> 00:00:28.710
If we look at timestep, t equals three,

00:00:28.710 --> 00:00:33.750
the error with respect to the Matrix Wx depends not only on

00:00:33.750 --> 00:00:40.920
vector S3 but also on S2 and its predecessor S1,

00:00:40.920 --> 00:00:46.285
which are all affected by the same matrix Wx.

00:00:46.284 --> 00:00:53.154
At first glance, it may seem that we need to consider only vector S3.

00:00:53.155 --> 00:00:56.910
So the derivative of timestep t equals three,

00:00:56.909 --> 00:00:59.250
by using the chain rule of course,

00:00:59.250 --> 00:01:03.480
simply equals to the derivative of the square error with respect to

00:01:03.479 --> 00:01:11.129
the output y3 multiplied by the derivative of the output with respect to S3.

00:01:11.129 --> 00:01:18.784
And finally, multiplied by the derivative of S3 with respect to the matrix Wx.

00:01:18.784 --> 00:01:23.109
But let's go back for a bit, as we said,

00:01:23.109 --> 00:01:28.799
S3 also depends on S2 and S1,

00:01:28.799 --> 00:01:33.384
which are all affected by the same matrix Wx.

00:01:33.385 --> 00:01:36.930
So the gradient that we're looking for is not only

00:01:36.930 --> 00:01:40.680
the product of the three derivatives we just saw but

00:01:40.680 --> 00:01:42.720
it is the accumulation of all of

00:01:42.719 --> 00:01:48.015
the contributions originating from each of the previous states.

00:01:48.015 --> 00:01:52.039
So let's consider the previous state, S2.

00:01:52.039 --> 00:01:55.427
Again, by using the chain rule,

00:01:55.427 --> 00:01:58.770
we can see the following path giving us

00:01:58.769 --> 00:02:04.125
an additional contribution to the overall gradient.

00:02:04.125 --> 00:02:06.629
But we are not done yet.

00:02:06.629 --> 00:02:10.207
We have one more state, S1, to consider.

00:02:10.207 --> 00:02:15.074
And we will add its contribution to the overall accumulative gradient.

00:02:15.074 --> 00:02:20.122
Starting from the output and back propagating to the first state,

00:02:20.122 --> 00:02:28.185
we will provide the following additional component to the overall gradient.

00:02:28.185 --> 00:02:32.890
Let's look again at the accumulative gradient we have using backpropagation through

00:02:32.889 --> 00:02:38.125
time which we calculated considering all the state vectors we have,

00:02:38.125 --> 00:02:43.000
state S3 S2 and S1.

00:02:43.000 --> 00:02:51.500
This is the complete gradient needed for the purpose of correctly updating the matrix Wx.

00:02:51.500 --> 00:02:57.330
Generally speaking, we need to consider multiple past timesteps and not just three,

00:02:57.330 --> 00:03:01.260
as in this example and need a general framework to define

00:03:01.259 --> 00:03:07.049
backpropagation through time for the purpose of updating Wx.

