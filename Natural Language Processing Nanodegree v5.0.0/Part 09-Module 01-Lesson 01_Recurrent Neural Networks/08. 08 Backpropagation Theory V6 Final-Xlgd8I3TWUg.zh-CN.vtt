WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:03.569
既然我们完成了前馈传导

00:00:03.569 --> 00:00:07.419
得到了输出 计算了误差

00:00:07.419 --> 00:00:10.469
我们准备退回去 改变权重

00:00:10.470 --> 00:00:15.519
从而降低网络误差

00:00:15.519 --> 00:00:20.839
在改变权重的同时 从输出到输入的反向过程

00:00:20.839 --> 00:00:23.884
叫做反向传播

00:00:23.885 --> 00:00:29.600
从本质上说是利用链式法则计算随机梯度下降

00:00:29.600 --> 00:00:33.484
如果你还不熟悉反向传播

00:00:33.484 --> 00:00:35.950
这部分可以助你一臂之力

00:00:35.950 --> 00:00:39.955
现在我们会更加数学化

00:00:39.954 --> 00:00:44.083
我非常有趣地发现数学的生动性

00:00:44.084 --> 00:00:48.340
数学计算如何最终引导我们运行

00:00:48.340 --> 00:00:50.650
人工智能的主要基础部分

00:00:50.649 --> 00:00:53.875
即神经网络

00:00:53.875 --> 00:00:57.320
为了运行基础的神经网络

00:00:57.320 --> 00:00:58.369
人们实际上不需要

00:00:58.369 --> 00:01:03.019
深厚的数学知识 因为现在我们有开源工具

00:01:03.020 --> 00:01:07.975
不过为了真正理解它的工作原理和最优化应用的方法

00:01:07.974 --> 00:01:11.129
了解数学是非常重要的

00:01:11.129 --> 00:01:13.689
因此我再次向你提出要求

00:01:13.689 --> 00:01:17.920
在我进行数学推导时 你要利用原来在学校里的方法记笔记

00:01:17.920 --> 00:01:20.245
我认为只有你记下自己的笔记

00:01:20.245 --> 00:01:24.870
才能对数学更有把握 因为这是你自己手写的

00:01:24.870 --> 00:01:28.155
对我来说 这样做总是非常有效

00:01:28.155 --> 00:01:34.484
我们想要找到一组权重 最小化网络的误差

00:01:34.484 --> 00:01:36.060
那么我们如何做到呢？

00:01:36.060 --> 00:01:39.420
我们使用迭代过程

00:01:39.420 --> 00:01:43.109
通过每次给神经网络提供一个训练集中的输入项

00:01:43.109 --> 00:01:44.984
正如我之前提到的一样

00:01:44.984 --> 00:01:47.594
在每个输入的前馈传导中

00:01:47.594 --> 00:01:49.694
我们计算网络的误差

00:01:49.694 --> 00:01:54.569
可以利用这个误差 在正确的方向稍微改变权重

00:01:54.569 --> 00:01:57.689
每次降低一点误差

00:01:57.689 --> 00:02:02.129
一直这样做 直到我们确定误差足够小

00:02:02.129 --> 00:02:05.039
那么多小算是足够小呢？

00:02:05.040 --> 00:02:09.905
我们怎么知道是不是实现从输入到输出的较好映射呢？

00:02:09.905 --> 00:02:12.759
实际上这个答案并不简单

00:02:12.759 --> 00:02:14.909
在接下来讨论过拟合部分

00:02:14.909 --> 00:02:18.664
你可以从中找到关于这问题实际的解法

00:02:18.664 --> 00:02:21.949
想象一下只包含一个权重 W 的网络

00:02:21.949 --> 00:02:25.829
假设训练过程中某个点上

00:02:25.830 --> 00:02:32.230
权重值为 WA 点 A 的误差是 EA

00:02:32.229 --> 00:02:33.879
为了降低误差

00:02:33.879 --> 00:02:38.229
我们需要增加权重值 WA

00:02:38.229 --> 00:02:40.560
由于梯度

00:02:40.560 --> 00:02:45.900
即点 A 的曲线斜率导数为负数

00:02:45.900 --> 00:02:47.930
由于这是向下的

00:02:47.930 --> 00:02:49.379
我们需要改变负方向的权重

00:02:49.379 --> 00:02:55.060
这样实际上可以增加 WA 的值

00:02:55.060 --> 00:02:57.465
另一方面

00:02:57.465 --> 00:03:03.060
如果权重值为 WB 误差是 EB

00:03:03.060 --> 00:03:04.485
为了降低误差

00:03:04.485 --> 00:03:06.915
我们需要降低权重 WB

00:03:06.914 --> 00:03:09.194
如果你观察梯度和点 B

00:03:09.194 --> 00:03:11.189
你会发现这是正值

00:03:11.189 --> 00:03:15.509
在这个例子中 通过在负梯度方向采取步骤改变权重

00:03:15.509 --> 00:03:20.935
表明我们可以正确降低权重值

00:03:20.935 --> 00:03:24.060
我们所观察这个例子只包括一个权重

00:03:24.060 --> 00:03:28.740
是对实际情况的过度简化 实际上神经网络包括许多权重

00:03:28.740 --> 00:03:31.379
我们可以使用这个方程式 归纳权重更新过程

00:03:31.379 --> 00:03:36.995
其中 α 是学习率或步长

00:03:36.995 --> 00:03:38.250
我们也看到这个权重 W

00:03:38.250 --> 00:03:40.710
至于原因我已经说过了

00:03:40.710 --> 00:03:44.219
改变了关于 W 误差偏导数的反方向

00:03:44.219 --> 00:03:48.465
你可能会问自己

00:03:48.465 --> 00:03:50.939
为什么我们要观察偏导数呢？

00:03:50.939 --> 00:03:55.259
答案很简单 只因为误差是许多变量的函数

00:03:55.259 --> 00:03:57.569
而偏导数可以让我们测量

00:03:57.569 --> 00:04:00.789
误差如何受到每个权重的单独影响

00:04:00.789 --> 00:04:05.669
在这节视频的结尾 你会找到如何调优学习率的资源

00:04:05.669 --> 00:04:10.659
这节在线课程结束后 你也会找到完整的超参数内容

00:04:10.659 --> 00:04:14.419
既然许多权重决定网络的输出

00:04:14.419 --> 00:04:18.094
我们可以使用网络误差偏导数的向量

00:04:18.095 --> 00:04:20.505
每个都与不同的权重相关

00:04:20.504 --> 00:04:23.045
如果你之前没有见过

00:04:23.045 --> 00:04:25.985
这个奇怪的倒三角符号是梯度

00:04:25.985 --> 00:04:28.985
梯度是针对每个权重的

00:04:28.985 --> 00:04:32.485
误差偏导数的向量

00:04:32.485 --> 00:04:35.639
为了使用恰当的符号表示

00:04:35.639 --> 00:04:37.919
我们也有一些指标

00:04:37.920 --> 00:04:41.259
这个图解如今应该很熟悉

00:04:41.259 --> 00:04:46.504
我们重点关注 K 层和 K+1 层的连接

00:04:46.504 --> 00:04:55.314
权重 Wij 连接了 K 层的神经元 I 和 K+1 层的神经元 j

00:04:55.314 --> 00:04:57.199
与我们之前见到的一样

00:04:57.199 --> 00:05:04.625
我们把改变或更新的权重 Wij 称为 △Wij

00:05:04.625 --> 00:05:09.024
上标 k 表明权重连接了 K 层

00:05:09.024 --> 00:05:10.989
和 K+1 层

00:05:10.990 --> 00:05:14.620
换句话说 它起始于 K 层

00:05:14.620 --> 00:05:19.340
计算这个 △Wij 非常简单

00:05:19.339 --> 00:05:20.949
等于学习率

00:05:20.949 --> 00:05:24.985
乘以每层权重 Wij 的

00:05:24.985 --> 00:05:28.030
误差偏导数

00:05:28.029 --> 00:05:33.144
我们得到这一项的负数 至于原因之前我已经说过

00:05:33.144 --> 00:05:35.979
基本上来说 反向传播可以归纳为

00:05:35.980 --> 00:05:39.465
计算每个权重的

00:05:39.464 --> 00:05:41.619
误差偏导数 E

00:05:41.620 --> 00:05:48.280
然后根据计算的值 △Wij 调整权重

00:05:48.279 --> 00:05:52.052
每层都完成这些计算

00:05:52.052 --> 00:05:55.209
我们再次来观察这个系统

00:05:55.209 --> 00:05:59.259
对于误差 我们可以使用损失函数

00:05:59.259 --> 00:06:03.985
即理想的输出减去网络输出 然后求出平方

00:06:03.985 --> 00:06:06.775
这里为了计算简便 我们把误差这一项除以 2

00:06:06.774 --> 00:06:09.879
随后你会看到

00:06:09.879 --> 00:06:13.519
既然我们定义了所有方程式

00:06:13.519 --> 00:06:16.000
我们可以使用一个例子探究数学问题

