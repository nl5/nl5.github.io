WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:01.830
在我们探究循环神经网络之前

00:00:01.830 --> 00:00:06.759
先回忆一下我们在前馈神经网络中使用的过程

00:00:06.759 --> 00:00:10.629
我们在输入和输出之间可能有许多隐藏层

00:00:10.630 --> 00:00:14.625
不过为了简便 我们从一个隐藏层开始

00:00:14.625 --> 00:00:16.397
我们会提醒自己使用它的原因

00:00:16.397 --> 00:00:19.009
时间和方式

00:00:19.010 --> 00:00:24.234
在我们对数学背景也有了清晰认识之后

00:00:24.234 --> 00:00:28.640
我们会采取下一步 开启通向循环神经网络的大门

00:00:28.640 --> 00:00:33.625
你会发现一旦扎实掌握前馈网络的基础知识

00:00:33.625 --> 00:00:37.274
通往循环神经网络之路非常简单

00:00:37.274 --> 00:00:43.000
你可能熟悉卷积神经网络概念

00:00:43.000 --> 00:00:44.875
简称 CNN

00:00:44.875 --> 00:00:47.229
执行神经网络时

00:00:47.229 --> 00:00:50.495
你会发现可以综合利用这些技巧

00:00:50.496 --> 00:00:53.710
例如人们可以在起初几层使用卷积神经网络

00:00:53.710 --> 00:00:57.520
进行特征提取

00:00:57.520 --> 00:01:02.725
然后在需要考虑存储的最后一层使用循环神经网络

00:01:02.725 --> 00:01:06.969
这方面常见应用是手势识别

00:01:06.969 --> 00:01:10.522
不过不必担心 如果你不了解卷积神经网络 也没关系

00:01:10.522 --> 00:01:13.332
这不是这堂课的重点

00:01:13.332 --> 00:01:16.969
利用前馈神经网络时

00:01:16.969 --> 00:01:19.935
我们实际上模拟一个人工神经网络

00:01:19.935 --> 00:01:23.284
通过使用非线性函数逼近

00:01:23.284 --> 00:01:28.864
这个函数作为一个包含 n 个输入

00:01:28.864 --> 00:01:32.554
权重 和 k 个输出的系统

00:01:32.555 --> 00:01:38.600
我们会使用 x 作为输入向量 y 作为输出向量

00:01:38.599 --> 00:01:45.849
输入和输出可以是多对多 多对一和一对多

00:01:45.849 --> 00:01:51.454
所以如果神经网络实质上是非线性函数逼近器时

00:01:51.454 --> 00:01:59.840
我们是要在给定的点 (x1,y1) (x2,y2) 等拟合一个光滑函数

00:01:59.840 --> 00:02:06.094
这样当给我们一个新的输入x时 

00:02:06.094 --> 00:02:10.004
我们能够通过这个函数算出对应的新的输出y

00:02:10.004 --> 00:02:15.289
我们随后在这节课中会详细阐述这些非线性函数逼近

00:02:15.289 --> 00:02:18.049
这是应用的两种主要类型

00:02:18.050 --> 00:02:20.810
一个是分类 我们可以确定新的输入

00:02:20.810 --> 00:02:25.590
属于哪个系列的类别

00:02:25.590 --> 00:02:28.909
例如图像分类

00:02:28.909 --> 00:02:33.342
其中神经网络接收的图像作为输入

00:02:33.342 --> 00:02:35.509
可以知道它是不是猫

00:02:35.509 --> 00:02:38.030
另一个应用是回归

00:02:38.030 --> 00:02:40.384
其中我们可以逼近一个函数

00:02:40.384 --> 00:02:46.634
那么这个网络经过监督学习过程后 产生连续值

00:02:46.634 --> 00:02:50.644
时间序列预测就是一个简单的例子

00:02:50.645 --> 00:02:53.689
其中我们根据前五天的股票价格

00:02:53.689 --> 00:02:57.656
预测明天的股票价格

00:02:57.656 --> 00:03:01.349
向网络中输入五个值

00:03:01.349 --> 00:03:05.909
表示过去五天的股票价格

00:03:05.909 --> 00:03:09.395
我们想要输出的是明天的股票价格

00:03:09.395 --> 00:03:15.040
神经网络的任务是找到最适合的权重

00:03:15.039 --> 00:03:20.694
可以产生较好的输出 其中 x 代表输入

00:03:20.694 --> 00:03:24.430
W 表示权重

00:03:24.430 --> 00:03:27.270
我们从随机权重开始

00:03:27.270 --> 00:03:28.945
在前馈神经网络中

00:03:28.944 --> 00:03:33.424
我们拥有从输入到输出的静态映射

00:03:33.425 --> 00:03:37.410
使用静态这个词是因为我们没有存储

00:03:37.409 --> 00:03:42.629
并且输出只依靠输入和权重

00:03:42.629 --> 00:03:47.759
换句话说 对于相同的输入和权重

00:03:47.759 --> 00:03:52.004
我们总是得到相同的输出结果

00:03:52.004 --> 00:03:55.500
一般来说 使用神经网络时

00:03:55.500 --> 00:04:00.599
我们包括两个主要阶段 训练和估值

00:04:00.599 --> 00:04:01.974
在训练阶段

00:04:01.974 --> 00:04:05.849
我们得到的数据集 叫做训练集

00:04:05.849 --> 00:04:10.629
其中包括多对输入及其对应的目标或输出

00:04:10.629 --> 00:04:14.460
目标是找到一系列权重

00:04:14.460 --> 00:04:18.720
可以实现输入到理想输出的最佳映射

00:04:18.720 --> 00:04:22.410
换句话说 训练阶段旨在生成

00:04:22.410 --> 00:04:26.910
一个不仅仅能够泛化训练集的网络

00:04:26.910 --> 00:04:28.715
在评估阶段

00:04:28.714 --> 00:04:32.944
我们使用训练阶段创建的网络

00:04:32.944 --> 00:04:35.235
应用新的输入

00:04:35.235 --> 00:04:39.000
旨在得到理想的输出

