WEBVTT
Kind: captions
Language: pt-BR

00:00:00.446 --> 00:00:04.571
Terminamos o 1º passo
e agora começaremos o 2º passo,

00:00:04.604 --> 00:00:06.939
que é descobrir a saída y

00:00:06.972 --> 00:00:11.001
usando os valores de h
que acabamos de calcular.

00:00:11.034 --> 00:00:15.420
Como temos mais de uma saída,
y também será um vetor.

00:00:15.453 --> 00:00:17.820
Nós temos
as nossas entradas iniciais h

00:00:17.853 --> 00:00:21.816
e queremos encontrar
os valores da saída y.

00:00:21.849 --> 00:00:23.034
Matematicamente,

00:00:23.067 --> 00:00:27.670
a ideia é idêntica
à que acabamos de ver no 1º passo.

00:00:27.703 --> 00:00:31.842
Agora temos entradas diferentes,
que chamamos de h,

00:00:31.875 --> 00:00:35.891
e uma matriz de peso diferente,
que chamamos de W2.

00:00:35.924 --> 00:00:38.626
A saída será o vetor y.

00:00:38.659 --> 00:00:41.320
Observe que a matriz de peso
tem 3 linhas -

00:00:41.353 --> 00:00:44.457
já que temos 3 neurônios
na camada oculta -

00:00:44.490 --> 00:00:48.363
e 2 colunas -
já que temos apenas 2 saídas.

00:00:48.396 --> 00:00:52.701
De novo, teremos a multiplicação
de um vetor por uma matriz.

00:00:52.734 --> 00:00:57.006
O vetor h multiplicado
pela matriz de peso W2

00:00:57.039 --> 00:00:59.569
resultará
no vetor de saída y.

00:00:59.602 --> 00:01:01.903
Podemos colocar tudo isso
numa equação simples,

00:01:01.936 --> 00:01:05.879
em que y é igual
a h vezes W.

00:01:06.382 --> 00:01:07.966
Depois de obtermos
as saídas,

00:01:07.999 --> 00:01:11.523
não precisamos necessariamente
de uma função de ativação.

00:01:11.556 --> 00:01:13.090
Em alguns programas,

00:01:13.123 --> 00:01:17.485
pode ser vantajoso usar,
por exemplo, uma função softmax,

00:01:17.518 --> 00:01:19.857
que chamamos de "sigma x",

00:01:19.890 --> 00:01:24.591
se quisermos que os valores de saída
estejam entre 0 e 1.

00:01:24.624 --> 00:01:27.220
Você pode ver mais informações
sobre este assunto

00:01:27.253 --> 00:01:29.743
no texto depois deste vídeo.

00:01:29.776 --> 00:01:32.943
Para obter uma boa estimativa
da saída y,

00:01:32.976 --> 00:01:36.167
precisamos de mais de um nível
de camadas ocultas -

00:01:36.200 --> 00:01:38.639
talvez 10 ou mais.

00:01:38.672 --> 00:01:41.702
Nesta imagem,
usei o número genérico P.

00:01:41.735 --> 00:01:43.687
O número de neurônios
em cada camada

00:01:43.720 --> 00:01:46.190
pode mudar de uma camada
para a camada seguinte

00:01:46.223 --> 00:01:49.811
e, como mencionei antes,
pode chegar a milhares.

00:01:49.844 --> 00:01:52.045
Em essência, podemos ver
esses neurônios

00:01:52.078 --> 00:01:54.491
como blocos de construção
ou peças de Lego

00:01:54.524 --> 00:01:56.269
que podem ser empilhados.

00:01:56.302 --> 00:01:58.473
Como isso é feito
matematicamente?

00:01:58.506 --> 00:02:00.087
Do mesmo jeito de antes.

00:02:00.120 --> 00:02:02.823
Pela simples multiplicação
de um vetor por uma matriz.

00:02:02.856 --> 00:02:05.421
seguida
de uma função de ativação,

00:02:05.454 --> 00:02:08.510
em que o vetor indica
as entradas,

00:02:08.543 --> 00:02:10.894
e a matriz indica os pesos

00:02:10.927 --> 00:02:13.213
que conectam uma camada
à camada seguinte.

00:02:14.604 --> 00:02:19.643
Para generalizar este modelo,
vamos ver uma camada K-F aleatória.

00:02:19.676 --> 00:02:24.995
O peso Wij da camada K-F
para a camada K+1

00:02:25.028 --> 00:02:30.971
corresponde à entrada i-F
que entra na saída j-F.

00:02:31.004 --> 00:02:33.499
É bom você anotar
estas informações,

00:02:33.532 --> 00:02:36.853
pois são derivações matemáticas
importantes.

00:02:36.886 --> 00:02:42.537
Enquanto você faz os cálculos,
pause o vídeo e anote as derivações.

00:02:42.570 --> 00:02:45.989
Tente fazer isso
ao longo de toda a aula.

00:02:46.022 --> 00:02:50.621
Vamos pensar na camada K
como as entradas x

00:02:50.654 --> 00:02:55.909
e na camada K+1
como as saídas da camada oculta h.

00:02:55.942 --> 00:02:59.027
Teremos n entradas x

00:02:59.060 --> 00:03:02.709
e m saídas h.

00:03:02.742 --> 00:03:05.796
Por sinal, se não estamos lidando
com as entradas,

00:03:05.829 --> 00:03:09.904
mas com as saídas
da camada oculta anterior,

00:03:09.937 --> 00:03:14.244
a única coisa que vai mudar
é a letra que escolhemos usar.

00:03:14.277 --> 00:03:16.674
Mas os cálculos serão
os mesmos.

00:03:16.707 --> 00:03:20.914
Então, para simplificar a notação,
vamos usar x.

00:03:20.947 --> 00:03:26.601
h1 é a saída de uma função
de ativação de uma soma,

00:03:26.634 --> 00:03:30.817
em que a soma é a multiplicação
de cada entrada xi

00:03:30.850 --> 00:03:35.430
pelo seu componente de peso
correspondente Wi1.

00:03:35.463 --> 00:03:36.884
Da mesma forma,

00:03:36.917 --> 00:03:41.110
hm é a saída de uma função
de ativação de uma soma,

00:03:41.143 --> 00:03:45.223
e a soma é a multiplicação
de cada entrada xi

00:03:45.256 --> 00:03:49.438
pelo seu componente de peso
correspondente Wim.

00:03:49.471 --> 00:03:51.075
Por exemplo,

00:03:51.108 --> 00:03:55.983
se tivermos 3 entradas
e quisermos calcular h1,

00:03:56.016 --> 00:03:59.251
ele será a saída
de uma função de ativação

00:03:59.284 --> 00:04:02.144
da combinação linear
seguinte.

00:04:03.197 --> 00:04:06.392
Esses cálculos de elemento único
serão úteis

00:04:06.425 --> 00:04:08.485
para entender
a retropropagação,

00:04:08.518 --> 00:04:11.250
e é por isso também
que precisamos entendê-los.

00:04:11.283 --> 00:04:15.003
Mas, assim como antes,
também podemos ver esses cálculos

00:04:15.036 --> 00:04:17.976
como uma multiplicação de um vetor
por uma matriz.

00:04:18.719 --> 00:04:23.486
Aliás, você deve ter reparado
que eu não enfatizei a entrada viés.

00:04:23.519 --> 00:04:26.817
O viés não muda
nenhum desses cálculos.

00:04:26.850 --> 00:04:29.309
Considere-o apenas
como uma entrada constante -

00:04:29.342 --> 00:04:31.046
em geral, 1 -,

00:04:31.079 --> 00:04:34.662
que também está conectada
a cada neurônio da camada oculta

00:04:34.695 --> 00:04:36.132
através de um peso.

00:04:36.223 --> 00:04:40.110
A única diferença entre o viés
e qualquer outra entrada

00:04:40.143 --> 00:04:45.228
é o fato de que ele permanece igual,
enquanto as outras entradas mudam.

00:04:45.261 --> 00:04:47.744
E, assim como ocorre
com todas as outras entradas,

00:04:47.777 --> 00:04:50.039
os pesos que o conectam
à camada seguinte

00:04:50.072 --> 00:04:52.173
também são atualizados.

00:04:52.206 --> 00:04:53.986
Agora vamos parar
por um instante.

00:04:54.019 --> 00:04:56.088
Qual é mesmo
o nosso objetivo?

00:04:56.121 --> 00:04:59.203
Nosso objetivo é encontrar
o melhor conjunto de pesos

00:04:59.236 --> 00:05:02.719
que no fim vão nos dar
as saídas desejadas, certo?

00:05:02.752 --> 00:05:07.630
No fim, queremos encontrar
um sistema que gere a saída certa

00:05:07.663 --> 00:05:09.850
para uma entrada específica.

00:05:10.761 --> 00:05:11.990
Na fase de treinamento,

00:05:12.023 --> 00:05:15.743
sabemos qual é a saída
de uma determinada entrada.

00:05:15.776 --> 00:05:20.732
Nós calculamos a saída do sistema
a fim de ajustar os pesos.

00:05:20.765 --> 00:05:25.636
Fazemos isso encontrando o erro
e tentando minimizá-lo.

00:05:25.669 --> 00:05:30.251
Cada iteração da fase de treinamento
diminuirá um pouco o erro,

00:05:30.284 --> 00:05:34.346
até considerarmos que o erro
é suficientemente pequeno.

00:05:34.825 --> 00:05:38.224
Vamos nos concentrar
em um cálculo de erro intuitivo,

00:05:38.257 --> 00:05:40.285
que é simplesmente descobrir
a diferença

00:05:40.318 --> 00:05:44.373
entre a saída calculada
e a saída desejada.

00:05:44.765 --> 00:05:47.138
Este é o nosso erro básico.

00:05:47.171 --> 00:05:49.708
Para os nossos cálculos
de retropropagação,

00:05:49.741 --> 00:05:51.802
usaremos o erro quadrático,

00:05:51.835 --> 00:05:54.663
que também é chamado
de "função de perda".

