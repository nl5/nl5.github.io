<!-- udacimak v1.4.4 -->
<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="ie=edge" http-equiv="X-UA-Compatible"/>
  <title>
   Backpropagation - Example (part a)
  </title>
  <link href="../assets/css/bootstrap.min.css" rel="stylesheet"/>
  <link href="../assets/css/plyr.css" rel="stylesheet"/>
  <link href="../assets/css/katex.min.css" rel="stylesheet"/>
  <link href="../assets/css/jquery.mCustomScrollbar.min.css" rel="stylesheet"/>
  <link href="../assets/css/styles.css" rel="stylesheet"/>
  <link href="../assets/img/udacimak.png" rel="shortcut icon" type="image/png">
  </link>
 </head>
 <body>
  <div class="wrapper">
   <nav id="sidebar">
    <div class="sidebar-header">
     <h3>
      Recurrent Neural Networks
     </h3>
    </div>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled components">
     <li class="">
      <a href="01. Introducing Ortal .html">
       01. Introducing Ortal
      </a>
     </li>
     <li class="">
      <a href="02. RNN Introduction.html">
       02. RNN Introduction
      </a>
     </li>
     <li class="">
      <a href="03. RNN History.html">
       03. RNN History
      </a>
     </li>
     <li class="">
      <a href="04. RNN Applications.html">
       04. RNN Applications
      </a>
     </li>
     <li class="">
      <a href="05. Feedforward Neural Network-Reminder.html">
       05. Feedforward Neural Network-Reminder
      </a>
     </li>
     <li class="">
      <a href="06. The Feedforward Process.html">
       06. The Feedforward Process
      </a>
     </li>
     <li class="">
      <a href="07. Feedforward Quiz.html">
       07. Feedforward Quiz
      </a>
     </li>
     <li class="">
      <a href="08. Backpropagation- Theory.html">
       08. Backpropagation- Theory
      </a>
     </li>
     <li class="">
      <a href="09. Backpropagation - Example (part a).html">
       09. Backpropagation - Example (part a)
      </a>
     </li>
     <li class="">
      <a href="10. Backpropagation- Example (part b).html">
       10. Backpropagation- Example (part b)
      </a>
     </li>
     <li class="">
      <a href="11. Backpropagation Quiz.html">
       11. Backpropagation Quiz
      </a>
     </li>
     <li class="">
      <a href="12.  RNN (part a).html">
       12.  RNN (part a)
      </a>
     </li>
     <li class="">
      <a href="13. RNN (part b).html">
       13. RNN (part b)
      </a>
     </li>
     <li class="">
      <a href="14. RNN-  Unfolded Model.html">
       14. RNN-  Unfolded Model
      </a>
     </li>
     <li class="">
      <a href="15. Unfolded Model Quiz.html">
       15. Unfolded Model Quiz
      </a>
     </li>
     <li class="">
      <a href="16. RNN- Example.html">
       16. RNN- Example
      </a>
     </li>
     <li class="">
      <a href="17. Backpropagation Through Time (part a).html">
       17. Backpropagation Through Time (part a)
      </a>
     </li>
     <li class="">
      <a href="18. Backpropagation Through Time (part b).html">
       18. Backpropagation Through Time (part b)
      </a>
     </li>
     <li class="">
      <a href="19. Backpropagation Through Time (part c).html">
       19. Backpropagation Through Time (part c)
      </a>
     </li>
     <li class="">
      <a href="20. BPTT Quiz 1.html">
       20. BPTT Quiz 1
      </a>
     </li>
     <li class="">
      <a href="21. BPTT Quiz 2.html">
       21. BPTT Quiz 2
      </a>
     </li>
     <li class="">
      <a href="22. BPTT Quiz 3.html">
       22. BPTT Quiz 3
      </a>
     </li>
     <li class="">
      <a href="23. Some more math.html">
       23. Some more math
      </a>
     </li>
     <li class="">
      <a href="24. RNN Summary.html">
       24. RNN Summary
      </a>
     </li>
     <li class="">
      <a href="25. From RNN to LSTM.html">
       25. From RNN to LSTM
      </a>
     </li>
     <li class="">
      <a href="26. Wrap Up.html">
       26. Wrap Up
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
   </nav>
   <div id="content">
    <header class="container-fluild header">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <div class="align-items-middle">
         <button class="btn btn-toggle-sidebar" id="sidebarCollapse" type="button">
          <div>
          </div>
          <div>
          </div>
          <div>
          </div>
         </button>
         <h1 style="display: inline-block">
          09. Backpropagation - Example (part a)
         </h1>
        </div>
       </div>
      </div>
     </div>
    </header>
    <main class="container">
     <div class="row">
      <div class="col-12">
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h1 id="backpropagation--example-part-a">
          Backpropagation- Example (part a)
         </h1>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          We will now continue with an example focusing on the backpropagation process, and consider a network having two inputs
          <span class="mathquill ud-math">
           [x_1, x_2]
          </span>
          , three neurons in a single hidden layer
          <span class="mathquill ud-math">
           [h_1, h_2, h_3]
          </span>
          and a single output
          <span class="mathquill ud-math">
           y
          </span>
          .
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="" class="img img-fluid" src="img/screen-shot-2017-11-17-at-5.38.55-pm.png"/>
          <figcaption class="figure-caption">
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          The weight matrices to update are
          <span class="mathquill ud-math">
           W^1
          </span>
          from the input to the hidden layer,  and
          <span class="mathquill ud-math">
           W^2
          </span>
          from the hidden layer to the output. Notice that in our case
          <span class="mathquill ud-math">
           W^2
          </span>
          is a vector, not a matrix, as we only have one output.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
         <p>
          10 Backpropagation Example A V3 Final
         </p>
        </h3>
        <video controls="">
         <source src="09. 10 Backpropagation Example A V3 Final-3k72z_WaeXg.mp4" type="video/mp4"/>
         <track default="true" kind="subtitles" label="en" src="09. 10 Backpropagation Example A V3 Final-3k72z_WaeXg.en.vtt" srclang="en"/>
         <track default="false" kind="subtitles" label="zh-CN" src="09. 10 Backpropagation Example A V3 Final-3k72z_WaeXg.zh-CN.vtt" srclang="zh-CN"/>
         <track default="false" kind="subtitles" label="pt-BR" src="09. 10 Backpropagation Example A V3 Final-3k72z_WaeXg.pt-BR.vtt" srclang="pt-BR"/>
        </video>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          The chain of thought in the weight updating process is as follows:
         </p>
         <p>
          To update the weights, we need the network error. To find the network error, we need the network output, and to find the network output we need the value of the hidden layer, vector
          <span class="mathquill ud-math">
           \bar {h}
          </span>
          .
         </p>
         <p>
          <span class="mathquill ud-math">
           \bar{h}=[h_1, h_2, h_3]
          </span>
         </p>
         <p>
          <em>
           Equation 8
          </em>
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          Each element of vector
          <span class="mathquill ud-math">
           \bar {h}
          </span>
          is calculated by a simple linear combination of the input vector with its corresponding weight matrix
          <span class="mathquill ud-math">
           W^1
          </span>
          , followed by an activation function.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="_Equation 9_" class="img img-fluid" src="img/screen-shot-2017-12-05-at-12.09.13-pm.png"/>
          <figcaption class="figure-caption">
           <p>
            <em>
             Equation 9
            </em>
           </p>
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          We now need to find the network's output,
          <span class="mathquill ud-math">
           y
          </span>
          .
          <span class="mathquill ud-math">
           y
          </span>
          is calculated in a similar way by using a linear combination of the vector
          <span class="mathquill ud-math">
           \bar{h}
          </span>
          with its corresponding elements of the weight vector
          <span class="mathquill ud-math">
           W^2
          </span>
          .
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="_Equation 10_" class="img img-fluid" src="img/screen-shot-2017-11-01-at-11.43.26-am.png"/>
          <figcaption class="figure-caption">
           <p>
            <em>
             Equation 10
            </em>
           </p>
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          After computing the output, we can finally find the network error.
         </p>
         <p>
          As a reminder, the two Error functions most commonly used are the
          <a href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener noreferrer" target="_blank">
           Mean Squared Error (MSE)
          </a>
          (usually used in regression problems) and the
          <a href="https://www.ics.uci.edu/~pjsadows/notes.pdf" rel="noopener noreferrer" target="_blank">
           cross entropy
          </a>
          (often used in classification problems).
         </p>
         <p>
          In this example, we use a variation of the MSE:
         </p>
         <p>
          <span class="mathquill ud-math">
           E=\frac{(d-y)^2}{2}
          </span>
          ,
         </p>
         <p>
          where
          <span class="mathquill ud-math">
           d
          </span>
          is the desired output and
          <span class="mathquill ud-math">
           y
          </span>
          is the calculated one. Notice that
          <strong>
           y
          </strong>
          and
          <strong>
           d
          </strong>
          are not vectors in this case, as we have a single output.
         </p>
         <p>
          The error is their squared difference,
          <span class="mathquill ud-math">
           E=(d-y)^2
          </span>
          , and is also called the network's
          <strong>
           Loss Function
          </strong>
          .  We are dividing the error term by 2 to simplify notation, as will become clear soon.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          The aim of the backpropagation process is to minimize the error, which in our case is the Loss Function. To do that we need to calculate its partial derivative with respect to all of the weights.
         </p>
         <p>
          Since we just found  the output y, we can now minimize the error by finding the updated values
          <span class="mathquill ud-math">
           \Delta W_{ij}^k
          </span>
          .
          <br/>
          The superscript k indicates that we need to update each and every layer k.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          As we noted before, the weight update value
          <span class="mathquill ud-math">
           \Delta W_{ij}^k
          </span>
          is calculated with the use of the gradient the following way:
         </p>
         <p>
          <span class="mathquill ud-math">
           \Delta W_{ij}^k=\alpha (-\frac{\partial E}{\partial W})
          </span>
         </p>
         <p>
          Therefore:
         </p>
         <p>
          <span class="mathquill ud-math">
           \Delta W_{ij}^k=\alpha (-\frac{\partial E}{\partial W})=-\frac{\alpha}{2} \frac{\partial (d-y)^2}{\partial W_{ij}}=-2 \frac{\alpha}{2}(d-y) \large  \frac{\partial (d-y)}{\partial W_{ij}}
          </span>
         </p>
         <p>
          which can be simplified as:
         </p>
         <p>
          <span class="mathquill ud-math">
           \Delta W_{ij}^k=\alpha(d-y) \frac{\partial y}{\partial W_{ij}}
          </span>
         </p>
         <p>
          <em>
           Equation 11
          </em>
         </p>
         <p>
          (Notice that
          <span class="mathquill ud-math">
           d
          </span>
          is a  constant value, so it’s partial derivative is simply a zero)
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          This partial derivative of the output with respect to each weight, defines the gradient and is often denoted by the Greek letter
          <span class="mathquill ud-math">
           \delta
          </span>
          .
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="_Equation 12_" class="img img-fluid" src="img/screen-shot-2017-12-05-at-12.16.55-pm.png"/>
          <figcaption class="figure-caption">
           <p>
            <em>
             Equation 12
            </em>
           </p>
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          We will find all the elements of the gradient using the chain rule.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          If you are feeling confident with the
          <strong>
           chain rule
          </strong>
          and understand how to apply it, skip the next video and continue with our example. Otherwise, give Luis a few minutes of your time as he takes you through the process!
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
         <p>
          Regra da cadeia
         </p>
        </h3>
        <video controls="">
         <source src="09. Regra da cadeia-YAhIBOnbt54.mp4" type="video/mp4"/>
         <track default="false" kind="subtitles" label="pt-BR" src="09. Regra da cadeia-YAhIBOnbt54.pt-BR.vtt" srclang="pt-BR"/>
         <track default="false" kind="subtitles" label="zh-CN" src="09. Regra da cadeia-YAhIBOnbt54.zh-CN.vtt" srclang="zh-CN"/>
         <track default="true" kind="subtitles" label="en" src="09. Regra da cadeia-YAhIBOnbt54.en.vtt" srclang="en"/>
         <track default="false" kind="subtitles" label="ja-JP" src="09. Regra da cadeia-YAhIBOnbt54.ja-JP.vtt" srclang="ja-JP"/>
        </video>
       </div>
       <div class="divider">
       </div>
      </div>
      <div class="col-12">
       <p class="text-right">
        <a class="btn btn-outline-primary mt-4" href="10. Backpropagation- Example (part b).html" role="button">
         Next Concept
        </a>
       </p>
      </div>
     </div>
    </main>
    <footer class="footer">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <p class="text-center">
         udacity2.0 If you need the newest courses Plase add me wechat: udacity6
        </p>
       </div>
      </div>
     </div>
    </footer>
   </div>
  </div>
  <script src="../assets/js/jquery-3.3.1.min.js">
  </script>
  <script src="../assets/js/plyr.polyfilled.min.js">
  </script>
  <script src="../assets/js/bootstrap.min.js">
  </script>
  <script src="../assets/js/jquery.mCustomScrollbar.concat.min.js">
  </script>
  <script src="../assets/js/katex.min.js">
  </script>
  <script>
   // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });

      // scroll to first video on page loading
      if ($('video').length) {
        $('html,body').animate({ scrollTop: $('div.plyr').prev().offset().top});
      }

      // auto play first video: this may not work with chrome/safari due to autoplay policy
      if (players && players.length > 0) {
        players[0].play();
      }

      // scroll sidebar to current concept
      const currentInSideBar = $( "ul.sidebar-list.components li a:contains('09. Backpropagation - Example (part a)')" )
      currentInSideBar.css( "text-decoration", "underline" );
      $("#sidebar").mCustomScrollbar('scrollTo', currentInSideBar);
    });
  </script>
 </body>
</html>
