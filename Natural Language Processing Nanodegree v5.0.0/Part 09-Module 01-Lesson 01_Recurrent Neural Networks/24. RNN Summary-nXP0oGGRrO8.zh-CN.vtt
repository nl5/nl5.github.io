WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.782
总结下我们讨论的内容

00:00:02.782 --> 00:00:04.515
我们现在明白 在 RNN 中

00:00:04.514 --> 00:00:10.259
当前状态取决于输入和前面的状态

00:00:10.259 --> 00:00:13.094
并应用激活函数

00:00:13.095 --> 00:00:15.060
例如双曲正切函数

00:00:15.060 --> 00:00:18.750
S 型函数或 ReLU 函数

00:00:18.750 --> 00:00:22.829
当前输出是当前状态元素

00:00:22.829 --> 00:00:27.359
和相应权重矩阵的简单线性组合

00:00:27.359 --> 00:00:33.084
我们还可以使用 softmax 函数计算输出

00:00:33.084 --> 00:00:37.892
这是 RNN 架构的收起形式

00:00:37.892 --> 00:00:40.340
只有一个隐藏层时

00:00:40.340 --> 00:00:43.100
需要更新三个权重矩阵

00:00:43.100 --> 00:00:47.870
Wy 将状态与输出相连

00:00:47.869 --> 00:00:55.820
Ws 将状态与本身相连 Wx 将输入与状态相连

00:00:55.820 --> 00:01:01.844
这是展开后的模型 也是我们最经常使用的形式

00:01:01.844 --> 00:01:05.658
我们再来看看梯度计算

00:01:05.658 --> 00:01:10.090
计算平方误差梯度（称为损失函数）

00:01:10.090 --> 00:01:15.143
相对于权重矩阵 Wy 的梯度比较简单

00:01:15.143 --> 00:01:19.500
当然 你也可以选择其他误差函数

00:01:19.500 --> 00:01:25.489
在计算相对于 Ws 和 Wx 的梯度时

00:01:25.489 --> 00:01:28.949
我们需要更谨慎

00:01:28.950 --> 00:01:33.645
应该考虑之前时间步中发生的情况 综合考虑所有这些影响

00:01:33.644 --> 00:01:39.734
对于这些计算 我们用到了一个叫做基于时间的反向传播流程

00:01:39.734 --> 00:01:42.171
正如之前讨论的

00:01:42.171 --> 00:01:47.129
在使用反向传播时 我们可以选择迷你批次

00:01:47.129 --> 00:01:50.069
在 RNN 中也可以这么做

00:01:50.069 --> 00:01:53.354
在基于时间的反向传播中

00:01:53.355 --> 00:01:56.404
也可以分批次定期更新权重

00:01:56.403 --> 00:01:59.564
而不是每次输入样本都要更新

00:01:59.564 --> 00:02:03.299
提醒下 我们计算每个时间步的梯度

00:02:03.299 --> 00:02:07.185
但是并不立即更新权重

00:02:07.185 --> 00:02:11.384
我们可以选择每隔固定数量的时间步更新权重

00:02:11.384 --> 00:02:13.475
例如 每隔 20 步

00:02:13.475 --> 00:02:15.840
这样可以简化训练流程的复杂性

00:02:15.840 --> 00:02:20.800
并且可以消除权重更新的噪点

00:02:20.800 --> 00:02:26.155
因为对一组噪点样本取平均值 可以生成噪点更少的值

00:02:26.155 --> 00:02:28.330
你可能会疑问

00:02:28.330 --> 00:02:31.758
如果有很多时间步 

00:02:31.758 --> 00:02:35.275
而不像上个示例中只有几个时间步 会怎样？

00:02:35.275 --> 00:02:38.200
还记得基于时间的反向传播示例吗？

00:02:38.199 --> 00:02:40.269
只有三个时间步

00:02:40.270 --> 00:02:43.075
可能还会有一个以上的隐藏层

00:02:43.074 --> 00:02:44.964
会发生什么情况？

00:02:44.965 --> 00:02:50.080
可以将每个时间步的梯度影响积累到一起吗？

00:02:50.080 --> 00:02:54.074
答案是否定的

00:02:54.074 --> 00:02:58.885
研究表明 如果只有少数时间步

00:02:58.884 --> 00:03:03.069
例如 8 个或 10 个 我们可以有效地利用该方法

00:03:03.069 --> 00:03:05.784
但是如果继续反向传播的话

00:03:05.784 --> 00:03:11.155
梯度将变得非常小 称为梯度消失问题

00:03:11.155 --> 00:03:13.810
随着时间的推移

00:03:13.810 --> 00:03:17.349
梯度信息的影响随时间推移按几何级数地降低

00:03:17.349 --> 00:03:21.775
换句话说 跨多个时间步的时间依赖性

00:03:21.775 --> 00:03:25.176
将被网络实际忽略掉

00:03:25.175 --> 00:03:28.169
例如跨 8 个 9 个或 10 个时间步

00:03:28.169 --> 00:03:32.189
如何处理梯度消失问题呢？

00:03:32.189 --> 00:03:36.509
为了解决这个问题

00:03:36.509 --> 00:03:38.729
我们专门发明了长短期记忆单元 简称 LSTM

00:03:38.729 --> 00:03:43.169
在后面的几个视频中 我们将专门讲解 LSTM

00:03:43.169 --> 00:03:48.030
另一个要注意的情形是梯度激增问题

00:03:48.030 --> 00:03:51.599
即梯度值不受控制地增大

00:03:51.599 --> 00:03:58.314
幸运的是 一个叫做梯度裁剪的简单方法可以实际解决这个问题

00:03:58.314 --> 00:04:00.215
我们要做的是

00:04:00.215 --> 00:04:05.795
检查在每个时间步 梯度是否超过了某个阈值

00:04:05.794 --> 00:04:09.724
如果超过 则标准化后续梯度

00:04:09.724 --> 00:04:13.984
标准化是指对极大梯度的惩罚程度

00:04:13.985 --> 00:04:18.453
极大梯度比稍微大于阈值的梯度要更加严重

00:04:18.452 --> 00:04:23.869
像这样裁剪梯度可以帮助我们避免梯度激增问题

00:04:23.870 --> 00:04:29.290
你可以在视频下方的文本中详细了解梯度裁剪

