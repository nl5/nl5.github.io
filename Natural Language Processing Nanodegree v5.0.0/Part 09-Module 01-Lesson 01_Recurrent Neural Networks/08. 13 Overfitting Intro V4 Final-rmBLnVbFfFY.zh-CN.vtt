WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.035
我们使用反向传播最小化网络误差时

00:00:04.035 --> 00:00:09.530
我们可能会恰当地拟合模型和数据 也会出现过拟合

00:00:09.529 --> 00:00:12.714
一般来说 我们有特定训练集时

00:00:12.714 --> 00:00:15.300
存在过拟合的风险

00:00:15.300 --> 00:00:20.780
过拟合是指我们的模型过于接近训练数据

00:00:20.780 --> 00:00:25.304
换句话说 我们过度训练了模型或网络 来拟合数据

00:00:25.304 --> 00:00:28.370
因为我们也意外地

00:00:28.370 --> 00:00:31.970
模拟了训练集的噪音或随机要素

00:00:31.969 --> 00:00:39.030
如果发生了这种情况 我们的模型在测试新的输入时 无法较好地泛化

00:00:39.030 --> 00:00:44.500
解决过拟合问题主要通过两种方法

00:00:44.500 --> 00:00:48.619
一是尽早停止训练过程

00:00:48.619 --> 00:00:52.489
二是使用正则化

00:00:52.490 --> 00:00:55.295
我们尽早停止训练过程时

00:00:55.295 --> 00:00:59.465
是在网络出现过拟合的位置停止训练

00:00:59.465 --> 00:01:05.030
通过这样做 我们减少测试集性能的下降

00:01:05.030 --> 00:01:09.635
如果我们准确知道停止训练过程的时间 这样会是理想的

00:01:09.635 --> 00:01:13.555
不过通常很难确定这一点

00:01:13.555 --> 00:01:18.360
确定训练过程停止时间的一种方法是

00:01:18.359 --> 00:01:24.980
从训练集中选取一小部分数据集 我们把它叫做验证集

00:01:24.980 --> 00:01:30.293
假设验证集的准确性与测试集类似

00:01:30.293 --> 00:01:33.790
我们可以使用它估算训练过程应该停止的时间

00:01:33.790 --> 00:01:39.290
这一方法的缺点在于我们最后用更少的样本训练模型

00:01:39.290 --> 00:01:42.160
所以训练集更小

00:01:42.159 --> 00:01:49.159
另一种消除过拟合的主要方法是使用正则化

00:01:49.159 --> 00:01:53.259
正则化表示我们为网络的训练增加约束条件

00:01:53.260 --> 00:01:58.010
这样可以实现更好的泛化

00:01:58.010 --> 00:02:04.000
丢弃 (dropout) 广泛应用于正则化框架中 可以有所帮助

