WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.895
还记得我们前馈的图解吗？

00:00:02.895 --> 00:00:04.560
我们有 n 个输入

00:00:04.559 --> 00:00:08.309
隐藏层中 3 个神经元和 2 个输出

00:00:08.310 --> 00:00:10.980
在这个例子中 我们需要简化

00:00:10.980 --> 00:00:14.609
更多内容 观察包含 2 个输入 x1 和 x2

00:00:14.609 --> 00:00:18.100
以及 1 个输出 y 的模型

00:00:18.100 --> 00:00:21.635
我们有一个权重矩阵 W1

00:00:21.635 --> 00:00:23.955
表示从输入到隐藏层

00:00:23.954 --> 00:00:28.695
矩阵要素也用之前的 Wij

00:00:28.695 --> 00:00:31.320
我们也有一个权重矩阵向量 W2

00:00:31.320 --> 00:00:35.064
表示从隐藏层到输出

00:00:35.064 --> 00:00:40.619
注意这是一个向量 而不是一个矩阵 因为我们只有一个输出

00:00:40.619 --> 00:00:42.934
不要忘记你的笔和笔记

00:00:42.935 --> 00:00:48.179
如有需要 暂停视频 确保你能跟上这里的数学计算

00:00:48.179 --> 00:00:53.170
我们首先在整个网络中进行输入的前馈传导

00:00:53.170 --> 00:00:55.425
然后计算输出

00:00:55.424 --> 00:00:57.195
根据误差

00:00:57.195 --> 00:01:02.189
使用反向传播计算偏导数

00:01:02.189 --> 00:01:08.838
计算三个隐藏神经元的激活值比较简单

00:01:08.838 --> 00:01:11.099
我们利用矩阵 W1 对应的权重要素

00:01:11.099 --> 00:01:15.919
得到输入的线性组合

00:01:15.920 --> 00:01:20.950
这些都位于激活函数后面

00:01:20.950 --> 00:01:26.760
输出是激活上一层向量 h 与

00:01:26.760 --> 00:01:30.546
权重 W2 的点积

00:01:30.546 --> 00:01:33.070
正如我之前提到的

00:01:33.069 --> 00:01:36.879
在我们这个例子中 反向传播过程的目的在于最小化

00:01:36.879 --> 00:01:40.109
损失函数或平方差

00:01:40.109 --> 00:01:43.224
为了做到这一点 我们需要计算每个权重

00:01:43.224 --> 00:01:46.929
平方差的偏导数

00:01:46.930 --> 00:01:48.895
既然我们刚才找到了输出

00:01:48.894 --> 00:01:56.649
现在可以通过找到 △Wij 的更新值 最小化误差

00:01:56.650 --> 00:01:58.150
当然在每个例子中

00:01:58.150 --> 00:02:05.969
我们需要为每个单独的 K 层进行这样的操作 这个值等于 -α

00:02:05.969 --> 00:02:09.840
乘以损失函数 E 的偏导数

00:02:09.840 --> 00:02:14.314
由于误差是多项式

00:02:14.314 --> 00:02:16.965
找到它的导数很容易

00:02:16.965 --> 00:02:18.965
通过利用基本的微积分

00:02:18.965 --> 00:02:21.719
我们会发现这个递增值是

00:02:21.719 --> 00:02:26.919
学习率 α 乘以 (d - y)

00:02:26.919 --> 00:02:32.009
乘以每个权重的偏导数 y

00:02:32.009 --> 00:02:33.405
你可能会问自己

00:02:33.405 --> 00:02:36.125
这对理想的输出 d 会产生什么影响？

00:02:36.125 --> 00:02:38.490
好 这是个常数值

00:02:38.490 --> 00:02:42.125
所以偏导数为 0

00:02:42.125 --> 00:02:44.354
注意到我这里使用了链式法则了吗？

00:02:44.354 --> 00:02:46.034
在这个视频的开头部分

00:02:46.034 --> 00:02:49.145
我提到反向传播实际上是

00:02:49.145 --> 00:02:52.740
利用链式法则的随机梯度下降

00:02:52.740 --> 00:02:55.330
现在我们得到了梯度

00:02:55.330 --> 00:02:59.660
梯度的符号一般是小写形式的 δ

00:02:59.659 --> 00:03:03.389
计算得到输出的偏导数定义了

00:03:03.389 --> 00:03:07.634
梯度 我们可以通过使用链式规则找到它

00:03:07.634 --> 00:03:09.419
我们先暂停一下

00:03:09.419 --> 00:03:14.039
提醒自己链式法则的定义和使用方法

00:03:14.039 --> 00:03:16.169
当你能够熟练运用这个概念后

00:03:16.169 --> 00:03:19.000
我们会继续实际计算过程

