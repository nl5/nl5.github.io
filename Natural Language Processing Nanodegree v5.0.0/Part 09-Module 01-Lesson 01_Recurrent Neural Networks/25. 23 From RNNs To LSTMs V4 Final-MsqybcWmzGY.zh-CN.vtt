WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.549
1997年 Sepp Hochreiter 和 Jürgen Schmidhuber 提出了

00:00:02.549 --> 00:00:09.969
长短期记忆单元 又称 LSTM 单元

00:00:09.970 --> 00:00:16.170
这个单元的目标是为了解决梯度消失问题

00:00:16.170 --> 00:00:20.520
你会发现它可以长时间锁存或存储特定输入

00:00:20.519 --> 00:00:26.734
而不是像循环神经网络一样遗忘它们

00:00:26.734 --> 00:00:30.725
使用基于时间的反向传播算法计算梯度时

00:00:30.725 --> 00:00:35.405
追溯许多时间步长的梯度会变得微不足道

00:00:35.405 --> 00:00:37.112
同样的道理

00:00:37.112 --> 00:00:41.490
误差的导数也会变得微不足道

00:00:41.490 --> 00:00:45.475
这是循环神经网络的基本问题

00:00:45.475 --> 00:00:48.260
你会发现在长短期记忆单元中

00:00:48.259 --> 00:00:50.765
我们想要避免信息损失

00:00:50.765 --> 00:00:52.850
或梯度消失问题

00:00:52.850 --> 00:00:58.320
通过故意锁定许多时间步长中的一些信息

00:00:58.320 --> 00:01:05.359
利用这些方法可以很容易记忆长时间内的信息

00:01:05.359 --> 00:01:10.170
现实世界中许多应用 如谷歌语言翻译工具

00:01:10.170 --> 00:01:14.359
或亚马逊的 Alexa 都是受到 LSTMs 的驱动

00:01:14.359 --> 00:01:18.090
在实际中 我们提到的循环神经网络应用

00:01:18.090 --> 00:01:22.508
推动着长短期记忆单元的应用发展

00:01:22.507 --> 00:01:26.609
为了理解长短期记忆单元和循环神经网络的区别

00:01:26.609 --> 00:01:29.224
我们来看循环神经网络系统

00:01:29.224 --> 00:01:32.509
放大某个隐藏层的神经元

00:01:32.510 --> 00:01:36.995
理解我们如何计算 St+1

00:01:36.995 --> 00:01:42.064
继续放大可以帮助我们回忆如何得到这些计算

00:01:42.064 --> 00:01:43.864
你可能回想起

00:01:43.864 --> 00:01:48.419
通过一个简单的激活函数 计算下一个状态

00:01:48.420 --> 00:01:50.719
例如双曲正切

00:01:50.719 --> 00:01:54.230
不同输入

00:01:54.230 --> 00:01:57.424
及其对应权重矩阵的线性组合

00:01:57.424 --> 00:02:01.355
通过一个简单的线性组合也可以计算得到输出

00:02:01.355 --> 00:02:08.060
我们通过使用长短期记忆单元 不再只是像这里只有一个神经元时的简单运算

00:02:08.060 --> 00:02:14.015
通过再次缩小 我们的系统布局非常类似

00:02:14.014 --> 00:02:19.399
隐藏状态的神经元替代为长短期记忆单元

00:02:19.400 --> 00:02:23.530
可以跟之前一样 像乐高积木一样堆叠起来

00:02:23.530 --> 00:02:27.814
如果放大一个单元

00:02:27.814 --> 00:02:31.925
我们会发现不再是一个单独的计算

00:02:31.925 --> 00:02:34.650
而是有四个单独的计算

00:02:34.650 --> 00:02:36.800
这是我们的长短期记忆单元

00:02:36.800 --> 00:02:40.535
这是我们的第一个计算

00:02:40.534 --> 00:02:43.859
第二个 第三个

00:02:43.860 --> 00:02:46.865
和第四个计算

00:02:46.865 --> 00:02:51.830
长短期记忆单元网络让循环的系统

00:02:51.830 --> 00:02:58.400
在许多时间步长中学习 同时使用相同的反向传播原则训练网络

00:02:58.400 --> 00:03:03.770
在这些例子中 我们不再只是像循环神经网络中那样 只追溯少数几个时间步长

00:03:03.770 --> 00:03:07.980
而是要考虑 1000 多个时间步长

00:03:07.979 --> 00:03:11.689
这个单元是完全可以微分的

00:03:11.689 --> 00:03:16.895
也就是说所有函数都有我们可以计算的梯度或导数

00:03:16.895 --> 00:03:19.195
这些函数包括 sigmoid

00:03:19.194 --> 00:03:24.500
双曲正切 乘法和加法

00:03:24.500 --> 00:03:27.004
这可以让我们使用反向传播算法

00:03:27.004 --> 00:03:29.044
或随机梯度下降

00:03:29.044 --> 00:03:31.574
更新权重

00:03:31.574 --> 00:03:33.964
长短期记忆单元的主要观点是

00:03:33.965 --> 00:03:38.104
它们可以决定删除或遗忘哪些信息

00:03:38.104 --> 00:03:40.069
存储哪些信息

00:03:40.069 --> 00:03:42.000
以及何时使用这些信息

00:03:42.000 --> 00:03:48.365
这个单元也可以帮助确定何时从之前的状态信息移到下一个状态

00:03:48.365 --> 00:03:52.300
我们刚刚看到长短期记忆单元包括三个 sigmoid

00:03:52.300 --> 00:03:56.685
每个 sigmoid 的输出都在 0 到 1 之间

00:03:56.685 --> 00:04:01.830
通过这个 sigmoid 得到数据流动可以直接回答下列问题

00:04:01.830 --> 00:04:06.725
sigmoid 输出为 1 时或接近 1 时 我们能不能让

00:04:06.724 --> 00:04:08.340
所有数据流动呢？

00:04:08.340 --> 00:04:11.085
另外没有数据流动时

00:04:11.085 --> 00:04:13.260
输出必须为 0 吗？

00:04:13.259 --> 00:04:18.159
如果 sigmoid 输出为 0 或接近 0 可能是这样的

00:04:18.160 --> 00:04:24.145
这三个 sigmoid 发挥了过滤进入单元内容的作用

00:04:24.144 --> 00:04:27.404
即单元保留什么

00:04:27.404 --> 00:04:30.989
哪些可以传递到输出

00:04:30.990 --> 00:04:35.430
长短期记忆单元的关键要点是

00:04:35.430 --> 00:04:41.155
使用反向传播算法 通过调整添加的权重 也可以训练这三个门控函数

00:04:41.154 --> 00:04:45.000
我们接下来的视频会帮助你进一步理解长短期记忆单元

