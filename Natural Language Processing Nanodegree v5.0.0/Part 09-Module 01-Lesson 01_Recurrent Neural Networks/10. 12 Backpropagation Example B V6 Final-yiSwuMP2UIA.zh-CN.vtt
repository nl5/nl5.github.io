WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:03.109
我们现在需要计算梯度

00:00:03.109 --> 00:00:05.730
我们会一次进行一个步骤

00:00:05.730 --> 00:00:08.880
在这个例子中 我们只有一个隐藏层

00:00:08.880 --> 00:00:12.570
因此反向传播过程会包括两个步骤

00:00:12.570 --> 00:00:17.460
现在我们会更加精确 确定矩阵中每个要素 Ij 的计算

00:00:17.460 --> 00:00:23.580
叫做 δIj

00:00:23.579 --> 00:00:25.715
在第一步中

00:00:25.716 --> 00:00:28.170
我们会计算从输出到隐藏层的权重向量 W2

00:00:28.170 --> 00:00:33.400
的梯度

00:00:33.399 --> 00:00:35.339
在第二步中

00:00:35.340 --> 00:00:37.740
我们会计算从隐藏层到输入的权重矩阵 W1

00:00:37.740 --> 00:00:44.075
的梯度

00:00:44.075 --> 00:00:47.115
好 我们首先从第一步开始

00:00:47.115 --> 00:00:49.150
我们已经计算了 y

00:00:49.149 --> 00:00:54.879
我们要找到权重向量 W2 的偏导数

00:00:54.880 --> 00:00:58.935
考虑到 y 是各项的线性求和

00:00:58.935 --> 00:01:04.980
你会发现梯度等于对应激活的值 h

00:01:04.980 --> 00:01:08.503
因为其他项为 0

00:01:08.503 --> 00:01:11.495
这也适用于梯度 1

00:01:11.495 --> 00:01:15.070
梯度 2 和梯度 3

00:01:15.069 --> 00:01:18.559
你可能注意到我们这里只有一个 δ 指标

00:01:18.560 --> 00:01:22.730
因为我们只有一个输出

00:01:22.730 --> 00:01:28.430
我们之前见到的递增值 △Wij 等于

00:01:28.430 --> 00:01:34.700
学习率 α 乘以 (d - y) 再乘以梯度

00:01:34.700 --> 00:01:37.204
在第二层

00:01:37.204 --> 00:01:48.875
△Wi 的递增值等于 α 乘以 (d - y) 再乘以 hi

00:01:48.875 --> 00:01:51.575
在我们第二步中

00:01:51.575 --> 00:01:55.490
我们想要更新第一层的权重

00:01:55.489 --> 00:02:00.414
通过计算权重矩阵 W1 的偏导数 y

00:02:00.415 --> 00:02:04.040
这里是非常有趣的事情

00:02:04.040 --> 00:02:05.983
计算权重矩阵 W1

00:02:05.983 --> 00:02:08.905
的梯度时

00:02:08.905 --> 00:02:11.270
我们需要使用链式法则

00:02:11.270 --> 00:02:17.475
这种方法是得到关于 h 的偏导数 y

00:02:17.474 --> 00:02:20.682
把它乘以 W1 中

00:02:20.682 --> 00:02:25.039
对应要素的 h 偏导数

00:02:25.039 --> 00:02:30.549
在这个例子中 我们只有包括 3 个神经元的一个隐藏层

00:02:30.550 --> 00:02:35.295
所以这是三个要素的线性组合

00:02:35.294 --> 00:02:39.125
我们来单独计算每个导数

00:02:39.125 --> 00:02:44.439
由于 y 是 h 及其对应权重的线性组合

00:02:44.439 --> 00:02:53.004
那么对于 h 的偏导数是权重要素向量 W2

00:02:53.004 --> 00:02:56.340
现在矩阵 W1 中对应权重

00:02:56.340 --> 00:03:00.133
向量 h 每个要素的偏导数是多少呢？

00:03:00.133 --> 00:03:01.740
我们来看一下

00:03:01.740 --> 00:03:05.564
我们已经单独考虑了向量 h 的每个要素

00:03:05.564 --> 00:03:07.365
这里是 h1

00:03:07.366 --> 00:03:12.156
我们也有 h2 和 h3

00:03:12.156 --> 00:03:14.145
如果我们进行泛化

00:03:14.145 --> 00:03:20.515
每个要素 j 是对应线性组合的激活函数

00:03:20.514 --> 00:03:25.789
找到它的偏导数是指找到激活函数的

00:03:25.789 --> 00:03:28.609
偏导数

00:03:28.610 --> 00:03:32.925
把它乘以线性组合的偏导数

00:03:32.925 --> 00:03:37.520
当然所有都是权重 W1 的正确要素

00:03:37.520 --> 00:03:43.450
如果你需要记笔记 可以随时暂停视频

00:03:43.449 --> 00:03:45.854
正如我前面提到的

00:03:45.854 --> 00:03:49.019
存在各种激活函数

00:03:49.020 --> 00:03:54.510
我们可以把它们叫做激活函数的偏导数 Φ‘j

00:03:54.509 --> 00:03:59.712
每个神经元都有自己的值  Φ 和 Φ‘

00:03:59.712 --> 00:04:02.699
根据你使用的激活函数

00:04:02.699 --> 00:04:06.599
对于 Wij 线性组合的偏导数

00:04:06.599 --> 00:04:12.460
等于 xi 因为所有项为 0

00:04:12.460 --> 00:04:18.660
所以权重矩阵 W1 中 h 的偏导数 y

00:04:18.660 --> 00:04:26.052
等于神经元 j 的 Φ‘ 乘以 xi

00:04:26.052 --> 00:04:30.980
我们现在得到了第二步所需的全部内容

00:04:30.980 --> 00:04:34.595
得到梯度 δIj

00:04:34.595 --> 00:04:39.830
我们知道矩阵 W1 中每个要素输出 y 的梯度

00:04:39.829 --> 00:04:48.719
是我们刚刚计算的两个偏导数乘积

00:04:48.720 --> 00:04:51.690
由于在这个例子中

00:04:51.689 --> 00:04:55.185
我们有 2 个输入和 3 个隐藏神经元

00:04:55.185 --> 00:05:00.261
我们要计算 6 个梯度 δ11

00:05:00.261 --> 00:05:04.189
δ12 δ13

00:05:04.189 --> 00:05:12.204
δ21 δ22 δ23

00:05:12.204 --> 00:05:14.945
在第二步找到梯度后

00:05:14.946 --> 00:05:19.695
得出 Wij 的递增值非常简单

00:05:19.694 --> 00:05:23.134
所以在我们这里使用的损失函数中

00:05:23.134 --> 00:05:30.045
等于梯度与 α 和 (d - y) 的乘积

00:05:30.045 --> 00:05:33.660
在反向传播算法的最后

00:05:33.660 --> 00:05:36.990
我们使用这两个步骤计算的递增值

00:05:36.990 --> 00:05:41.280
更新了权重矩阵的每个要素

00:05:41.279 --> 00:05:44.159
如果我们有更多层 实际情况中也是如此

00:05:44.160 --> 00:05:45.900
我们会需要更多步骤

00:05:45.899 --> 00:05:49.214
你可以想象一下 过程会更加复杂

00:05:49.214 --> 00:05:52.834
幸好我们有编程工具可以进行计算

00:05:52.834 --> 00:05:56.909
在这些计算中 我们没有强调偏差输入

00:05:56.910 --> 00:06:02.064
因为它没有改变我们涉及到的概念

00:06:02.064 --> 00:06:03.975
正如我之前提到的

00:06:03.975 --> 00:06:07.770
只把偏差作为一个常量输入

00:06:07.769 --> 00:06:12.329
通过权重连接到隐藏层的每个神经元

00:06:12.329 --> 00:06:16.229
偏差和其他输入的唯一差别在于

00:06:16.230 --> 00:06:20.835
偏差保持不变 而其他输入会发生变化

00:06:20.834 --> 00:06:24.129
在这个例子中 对于每个新输入

00:06:24.129 --> 00:06:27.894
我们在每次计算输出后 更新权重

00:06:27.894 --> 00:06:32.954
这有利于每次结束步骤中更新权重

00:06:32.954 --> 00:06:36.969
这叫做迷你批量训练 包括

00:06:36.970 --> 00:06:43.110
在实际更新权重前 对多个步骤中的权重变化求出平均值

00:06:43.110 --> 00:06:47.020
使用迷你批量训练主要原因包括两个

00:06:47.019 --> 00:06:49.779
一是减少训练过程的复杂性

00:06:49.779 --> 00:06:54.759
因为需要更少的计算

00:06:54.759 --> 00:06:59.334
二是我们对权重的多个

00:06:59.334 --> 00:07:01.870
潜在的噪音变化求出平均值

00:07:01.870 --> 00:07:05.069
最后得到较低噪音的校正值 这是更为重要的一点

00:07:05.069 --> 00:07:11.000
这表示学习过程实际上聚合更快 也更加准确

