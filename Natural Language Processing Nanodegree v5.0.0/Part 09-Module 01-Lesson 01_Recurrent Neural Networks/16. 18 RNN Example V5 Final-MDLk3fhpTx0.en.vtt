WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.690
Let's continue with a conceptual RNN example.

00:00:03.690 --> 00:00:06.870
Assume that we want to build a sequence detector,

00:00:06.870 --> 00:00:11.280
and let's decide that our sequence detector will track letters.

00:00:11.279 --> 00:00:14.429
So we will actually build a word detector.

00:00:14.429 --> 00:00:20.202
And more specifically, we want our network to detect the word, Udacity.

00:00:20.202 --> 00:00:22.769
Just the word.

00:00:22.769 --> 00:00:24.064
So before we start,

00:00:24.065 --> 00:00:26.798
we need to make a few decisions.

00:00:26.797 --> 00:00:30.914
We can define the input by using a one-hot vector encoding,

00:00:30.914 --> 00:00:33.570
containing seven binary values.

00:00:33.570 --> 00:00:39.134
Here, I use the letters in ascending order, but that's arbitrary.

00:00:39.134 --> 00:00:44.515
Each letter will be represented by one in the index it corresponds to,

00:00:44.515 --> 00:00:46.890
and zeros everywhere else.

00:00:46.890 --> 00:00:54.230
So for example, the letter A will be represented by one followed by six zeros.

00:00:54.229 --> 00:00:58.099
Here are the rest of the letters that we need.

00:00:58.100 --> 00:01:02.350
The letters not appearing in the word Udacity can be

00:01:02.350 --> 00:01:05.769
addressed by a simple vector containing all zeros,

00:01:05.769 --> 00:01:08.185
like these letters, for example.

00:01:08.185 --> 00:01:11.409
The sequence we are trying to detect will be of length

00:01:11.409 --> 00:01:16.405
seven with inputs in the following order: U,

00:01:16.405 --> 00:01:21.909
D, A, C, I,

00:01:21.909 --> 00:01:26.209
T, and finally, Y.

00:01:26.209 --> 00:01:32.644
We can train the system by feeding it random letters at each timestep.

00:01:32.644 --> 00:01:37.619
Creating a sequence of inputs demonstrated here from left to right.

00:01:37.620 --> 00:01:41.905
Occasionally, we will also insert the word Udacity.

00:01:41.905 --> 00:01:43.515
We set the target values,

00:01:43.515 --> 00:01:47.310
the outputs to zero all the time except for when

00:01:47.310 --> 00:01:52.454
the last letter Y of the sequence Udacity enters the system.

00:01:52.454 --> 00:01:56.435
Only then, the target will be set to one.

00:01:56.435 --> 00:02:00.650
Again, the system will acknowledge all inputs and will respond with

00:02:00.650 --> 00:02:05.790
a target output of one only when the desired sequence is detected.

00:02:05.790 --> 00:02:10.265
Sketching our network in an unfolded form,

00:02:10.264 --> 00:02:13.154
we have an input vector of seven values,

00:02:13.155 --> 00:02:16.669
a single output, and a state.

00:02:16.669 --> 00:02:19.530
The state can have any number of hidden neurons.

00:02:19.530 --> 00:02:24.495
In this illustration, we will use n to leave things generic.

00:02:24.495 --> 00:02:28.009
The first state vector is usually set to zero,

00:02:28.009 --> 00:02:32.159
allowing the next state to evolve as inputs come in.

00:02:32.159 --> 00:02:33.902
When training the network,

00:02:33.902 --> 00:02:36.614
we set the targets to either zero or one.

00:02:36.615 --> 00:02:40.335
The target will be zero when the word Udacity is not detected,

00:02:40.335 --> 00:02:42.000
and one when it is.

00:02:42.000 --> 00:02:46.050
If we train our system on targets that are either zero or one,

00:02:46.050 --> 00:02:51.165
we expect that the output will also take on values between zero and one.

00:02:51.164 --> 00:02:54.584
After training our system and optimizing the weights,

00:02:54.585 --> 00:02:57.900
we would expect that when the sequence Udacity appears,

00:02:57.900 --> 00:03:00.420
the output will signal that a sequence has been

00:03:00.419 --> 00:03:03.659
detected by taking on a value close to one,

00:03:03.659 --> 00:03:06.564
like 0.9 in this case.

00:03:06.564 --> 00:03:12.300
Practically speaking, we can set a threshold of, say, 0.9,

00:03:12.300 --> 00:03:16.305
and decide that whenever the output exceeds this threshold value,

00:03:16.305 --> 00:03:20.550
the sequence of interest has been detected.

00:03:20.550 --> 00:03:23.314
The number 0.9 in this case, by the way,

00:03:23.314 --> 00:03:27.003
was selected as an arbitrary example.

00:03:27.003 --> 00:03:32.924
In our example, the RNN was trained to recognize sequences of seven inputs,

00:03:32.925 --> 00:03:35.825
like the letters in the word Udacity.

00:03:35.824 --> 00:03:37.864
However, we could have trained the RNN to recognize

00:03:37.865 --> 00:03:40.950
sequences of letters that have a different lengths,

00:03:40.949 --> 00:03:43.569
like five in the word happy.

00:03:43.569 --> 00:03:48.084
Generally speaking, the RNN can deal with varying sequence lengths.

00:03:48.085 --> 00:03:50.129
So, how do we train this network?

00:03:50.129 --> 00:03:51.615
Or in other words,

00:03:51.615 --> 00:03:55.585
how do we optimize its weights to minimize the output error?

00:03:55.585 --> 00:03:59.030
We do that using backpropagation through time,

00:03:59.030 --> 00:04:02.289
and we'll be learning all about that in the next set of videos.

