WEBVTT
Kind: captions
Language: zh-CN

00:00:00.100 --> 00:00:02.100
在这一部分 我们将运用所有学到的知识

00:00:02.100 --> 00:00:04.720
来构建我们的第一个神经网络 用来训练

00:00:04.719 --> 00:00:06.129
我们刚刚创建的数据集

00:00:06.129 --> 00:00:07.689
这里 我想让大家做的就是

00:00:07.689 --> 00:00:10.887
从上一章做的成果开始搭建神经网络

00:00:10.887 --> 00:00:14.568
我猜大家在上一个模块 已经构建了一个基本神经网络

00:00:14.567 --> 00:00:16.929
来对结构化的数据集做预测

00:00:16.929 --> 00:00:19.367
现在 我将运用三层神经网络

00:00:19.367 --> 00:00:21.375
去除隐藏层的非线性

00:00:21.375 --> 00:00:22.870
稍后我再说明原因

00:00:22.870 --> 00:00:26.240
然后 我要大家利用之前创建的函数

00:00:26.239 --> 00:00:29.199
快速生成训练数据

00:00:29.199 --> 00:00:32.390
也就是 每输入一条评论和一个标签

00:00:32.390 --> 00:00:36.939
就会分别转换成我们所需的输入和输出两个向量

00:00:36.939 --> 00:00:39.885
然后通过正向传播和反向传播

00:00:39.886 --> 00:00:42.185
快速训练数据

00:00:42.185 --> 00:00:44.362
接下来要做的 就是创建一个函数

00:00:44.362 --> 00:00:45.570
来预处理数据

00:00:45.570 --> 00:00:49.039
这样 让所有这些词汇变量

00:00:49.039 --> 00:00:53.140
word2index 变量 都在这一类中

00:00:53.140 --> 00:00:55.579
这样 这个类本身特征齐全 相对独立

00:00:55.579 --> 00:00:59.530
然后 来改进训练变量 通过针对整个语料库进行训练

00:00:59.530 --> 00:01:03.520
而不只是针对一组 inputs_list 和 targets_list 来训练

00:01:03.520 --> 00:01:06.147
所以 这些就是我希望大家做的

00:01:06.147 --> 00:01:07.866
你可以从这个 shell 开始做

00:01:07.867 --> 00:01:10.543
可在上周的章节开始处找到

00:01:10.543 --> 00:01:13.050
或者从你上次所完成的神经网络开始做

00:01:14.459 --> 00:01:17.913
如果你还需要帮助 可以先观看

00:01:17.914 --> 00:01:21.430
之前的优达学城讲座 

00:01:21.430 --> 00:01:24.435
熟悉一下反向传播、梯度下降和误差测量

00:01:24.435 --> 00:01:28.280
以及如何修正反向传播来摆脱非线性 这些都是这里会用到的

00:01:28.280 --> 00:01:30.370
如果还不够的话

00:01:30.370 --> 00:01:32.939
你可以查看《Grokking深度学习》 一书的第 3 ~ 5 章

00:01:32.939 --> 00:01:35.689
我已经给出了 40% 的代码

00:01:35.689 --> 00:01:39.801
书上全面综述了正向传播、反向传播、误差梯度

00:01:39.801 --> 00:01:41.495
和随机梯度下降

00:01:41.495 --> 00:01:43.903
一会儿 我将向大家展示 我是如何完成这个网络的

00:01:43.903 --> 00:01:47.210
然后 我们也会聊一聊我们所做的一些不同的更改

00:01:47.209 --> 00:01:48.539
好的 一会儿见

