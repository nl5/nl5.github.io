WEBVTT
Kind: captions
Language: pt-BR

00:00:00.000 --> 00:00:05.834
Nesta seção falaremos
sobre ruído versus sinal.

00:00:05.868 --> 00:00:10.801
Queremos buscar correlações,
e redes neurais são boas para isso.

00:00:10.834 --> 00:00:13.968
Mas temos que
enquadrar o problema

00:00:14.000 --> 00:00:16.334
para que as redes neurais
ganhem mais

00:00:16.367 --> 00:00:20.934
e possam treinar e entender
os padrões mais complicados.

00:00:21.501 --> 00:00:23.634
O que vimos na última seção

00:00:23.667 --> 00:00:26.634
foi que isso não estava
treinando muito rápido.

00:00:26.667 --> 00:00:30.200
Pareceu que havia mais sinal
nestes dados de sentimentos

00:00:30.234 --> 00:00:33.767
do que "excelente" versus
"terrível" versus "movimento"

00:00:33.801 --> 00:00:35.267
versus...

00:00:35.300 --> 00:00:38.267
algumas outras palavras
positivas ou negativas.

00:00:38.300 --> 00:00:41.567
Parece que eu, como humano,
olhando este texto,

00:00:41.601 --> 00:00:46.234
poderia prever com mais de 60%
de precisão usando só as palavras.

00:00:46.267 --> 00:00:48.701
Então vamos voltar...

00:00:49.334 --> 00:00:52.501
Mas antes de começarmos
com regularizações malucas

00:00:52.534 --> 00:00:54.300
ou coisas sofisticadas
na rede neural,

00:00:54.334 --> 00:00:57.767
vamos voltar aos dados;
é onde está todo o ouro.

00:00:57.801 --> 00:01:00.868
A rede neural é só
a retroescavadeira

00:01:00.901 --> 00:01:03.334
com a qual tiraremos
o ouro do solo.

00:01:03.367 --> 00:01:06.667
Se não achamos muito ouro,
principalmente no início,

00:01:06.701 --> 00:01:09.133
o problema não é
na retroescavadeira,

00:01:09.167 --> 00:01:11.501
deve ser um problema
de onde cavamos,

00:01:11.534 --> 00:01:15.067
da terra que escolhemos
e de como a manipulamos.

00:01:15.100 --> 00:01:18.067
Quero falar aqui
de ruído versus sinal,

00:01:18.100 --> 00:01:19.968
porque isso é tudo.

00:01:20.000 --> 00:01:23.801
Temos uma tonelada de dados,
sabemos que há um padrão aqui,

00:01:23.834 --> 00:01:26.400
queremos que a rede neural
possa achá-lo.

00:01:26.434 --> 00:01:27.868
Antes, por pura sorte,

00:01:27.901 --> 00:01:30.934
quando criamos
esta camada de atualizar entrada,

00:01:30.968 --> 00:01:32.267
vi um 18 aqui.

00:01:32.300 --> 00:01:35.067
Na hora,
isso me chamou a atenção.

00:01:35.100 --> 00:01:36.834
Puxa, é muito alto.

00:01:36.868 --> 00:01:41.000
Pensem comigo como seria
ter um 18 aqui.

00:01:42.601 --> 00:01:46.267
Esta propagação para a frente
é uma soma ponderada.

00:01:46.300 --> 00:01:49.467
Há 4 pesos saindo
desta camada de entrada, certo?

00:01:49.501 --> 00:01:52.267
Um é multiplicado por 4,

00:01:52.300 --> 00:01:54.567
um é multiplicado
por estes 4,

00:01:54.601 --> 00:01:57.601
e estes dois vetores
são somados aqui.

00:01:57.634 --> 00:01:59.367
Então é uma soma ponderada.

00:01:59.400 --> 00:02:02.534
Uma soma ponderada
de somas ponderadas.

00:02:02.567 --> 00:02:05.634
Este vetor
representa "horrível".

00:02:05.667 --> 00:02:10.434
Quero dizer a lista de pesos,
este peso, este peso, este peso...

00:02:10.467 --> 00:02:14.701
Eles têm um certo impacto
nestes 4 nós.

00:02:15.901 --> 00:02:19.300
É engraçado,
eles meio que se interpretam.

00:02:19.334 --> 00:02:24.634
O valor deste número afeta
o quão dominantemente estes pesos

00:02:24.667 --> 00:02:26.734
controlam
esta camada oculta.

00:02:26.767 --> 00:02:29.601
E estes pesos controlam
o quão dominantemente

00:02:29.634 --> 00:02:32.033
esta entrada afeta
esta camada oculta.

00:02:32.067 --> 00:02:34.767
São multiplicados,
é uma coisa associativa,

00:02:34.801 --> 00:02:37.267
os dois meio que interagem
desta forma.

00:02:37.300 --> 00:02:41.167
Mas se isto
for multiplicado por 18,

00:02:41.200 --> 00:02:45.000
e isto for multiplicado por 1,
isto será dominante.

00:02:45.033 --> 00:02:49.367
Este vetor será exatamente igual
a esses 4 pesos

00:02:49.400 --> 00:02:51.734
para "horrível",
multiplicado por 18,

00:02:51.767 --> 00:02:56.667
é um porcentagem do montante
de energia nestes nós,

00:02:56.701 --> 00:02:58.901
e vai ser basicamente
esta palavra.

00:02:58.934 --> 00:03:01.801
Pensei: "Qual está
com o peso 18?"

00:03:01.834 --> 00:03:04.100
Se olharmos
no índice de palavras...

00:03:06.734 --> 00:03:11.334
Desculpem...
Vocab 0, pois é a posição 0.

00:03:12.033 --> 00:03:17.033
Ou é review vocab?
reviews? reviews vocab?

00:03:17.067 --> 00:03:19.534
Pode ser,
devo ter deletado.

00:03:19.567 --> 00:03:21.067
Volte aqui.

00:03:25.667 --> 00:03:27.701
List Vocab.

00:03:28.300 --> 00:03:29.567
É isso.

00:03:29.601 --> 00:03:31.601
Então, não é nada.

00:03:31.634 --> 00:03:35.033
Fui ver, e uma das palavras
não é nada.

00:03:35.067 --> 00:03:36.067
Olho aqui e...

00:03:36.100 --> 00:03:38.834
Tinha 18 delas.
Uma só não deve ser problema,

00:03:38.868 --> 00:03:40.801
a rede neural
pode organizar isso.

00:03:40.834 --> 00:03:43.534
Mas está vendo
basicamente nada neste vetor.

00:03:43.567 --> 00:03:47.167
E as outras palavras
falando baixinho.

00:03:47.200 --> 00:03:50.033
E olho para o resto
das coisas.

00:03:50.067 --> 00:03:52.901
Reviews.split...

00:03:53.868 --> 00:03:55.033
Espaço.

00:03:55.067 --> 00:03:58.367
review 0 split espaço.

00:04:02.267 --> 00:04:03.467
Certo.

00:04:05.033 --> 00:04:07.200
Vazio, vazio, vazio.

00:04:07.234 --> 00:04:09.033
Vejam, vazio, vazio.

00:04:09.067 --> 00:04:12.033
Pensei que fosse
um erro de tokenização,

00:04:12.067 --> 00:04:15.467
mas também há
vários pontos aqui.

00:04:15.501 --> 00:04:17.734
Qual será a distribuição?

00:04:17.767 --> 00:04:21.767
Então vamos ver
o review_counter.

00:04:21.801 --> 00:04:25.801
Para todas as palavras
em reviews(0).split,*

00:04:26.501 --> 00:04:29.033
review_counter word...

00:04:31.400 --> 00:04:34.467
De novo, usei esses countadores.
Adoro eles.

00:04:34.501 --> 00:04:37.934
Review contador,
mais comum, mostrar.

00:04:37.968 --> 00:04:40.067
Puxa! Vejam isso.

00:04:40.601 --> 00:04:41.901
Vejam isso.

00:04:42.801 --> 00:04:45.000
As palavras dominantes.

00:04:45.033 --> 00:04:47.767
Não têm nada a ver
com sentimento.

00:04:47.801 --> 00:04:53.534
Então, haverá algumas
palavras de sentimento aqui...

00:04:53.567 --> 00:04:56.400
"Revelador" está aqui.

00:04:56.434 --> 00:04:58.467
Mas quando olhamos isto,

00:04:58.501 --> 00:05:03.367
a maior parte desta resenha
são palavras irrelevantes,

00:05:03.400 --> 00:05:06.534
como "o", "para", "eu",
"é", "de", "um".

00:05:06.567 --> 00:05:10.501
E esses pesos
estão causando

00:05:10.534 --> 00:05:12.868
um efeito dominante
na camada oculta

00:05:12.901 --> 00:05:16.334
que a camada de saída usa
para fazer uma previsão.

00:05:16.367 --> 00:05:20.367
Se a camada oculta não for rica,
a camada de saída sofre.

00:05:20.400 --> 00:05:23.968
Mas espere, decidimos
olhar as contagens antes.

00:05:24.000 --> 00:05:25.734
Talvez seja uma má ideia,

00:05:25.767 --> 00:05:29.434
pois as contagens
não destacam o sinal.

00:05:29.467 --> 00:05:33.033
Olhando-as,
parecem destacar o ruído.

00:05:33.067 --> 00:05:36.067
Com "destacar",
quero dizer dar mais peso.

00:05:36.100 --> 00:05:38.934
Redes neurais são só
pesos e funções.

00:05:38.968 --> 00:05:43.133
Pegamos este conjunto de valores,
mudamos os pesos deles

00:05:43.167 --> 00:05:46.033
para estes 4 nós,
e rodamos uma função.

00:05:46.067 --> 00:05:48.701
Neste caso, não,
mas é uma função linear.

00:05:48.734 --> 00:05:50.901
Usamos pesos de novo
e fazemos uma função,

00:05:50.934 --> 00:05:52.667
e é a nossa previsão,
certo?

00:05:52.701 --> 00:05:55.968
Se os pesos estiverem errados
para os dados de entrada,

00:05:56.000 --> 00:05:58.300
dificultará muito
achar o sinal.

00:05:58.334 --> 00:06:01.534
Isso é ruído, a forma
como enquadramos o problema

00:06:01.567 --> 00:06:04.634
está acrescentado
muito ruído.

00:06:04.667 --> 00:06:08.300
Com estes pesos, a rede neural
deve aprender a dizer:

00:06:08.334 --> 00:06:11.167
"ponto, fique quieto."
"vazio, fique quieto."

00:06:11.200 --> 00:06:14.100
"o", "dois", "eu", "alto", "é",
"de", "uma"...

00:06:14.133 --> 00:06:15.133
Quietos.

00:06:15.167 --> 00:06:17.200
Quero ouvir "revelador".

00:06:17.234 --> 00:06:19.234
Preciso ouvir "oportuno",

00:06:19.267 --> 00:06:23.133
Preciso de palavras positivas,
pois a resenha é positiva.

00:06:23.167 --> 00:06:27.567
Não sei se devemos olhar também
para uma resenha negativa,

00:06:27.601 --> 00:06:31.901
mas esta rede neural está tentando
silenciar palavras não relevantes

00:06:31.934 --> 00:06:34.400
e ouvir com mais atenção
as relevantes.

00:06:34.434 --> 00:06:36.901
Mas não a ajudamos
fazendo o peso maior

00:06:36.934 --> 00:06:38.968
ser nas coisas
mais frequentes.

00:06:39.000 --> 00:06:42.167
Devemos tentar
eliminar isto.

00:06:42.200 --> 00:06:47.567
No projeto 4,
acho que vamos tentar isso.

00:06:47.601 --> 00:06:50.400
Vou descrever como vai ser.

00:06:50.434 --> 00:06:53.534
Vamos pegar...

00:06:54.033 --> 00:06:56.801
o código que tínhamos
no Projeto 3.

00:06:59.400 --> 00:07:01.901
Vamos ao Projeto 4.

00:07:02.701 --> 00:07:07.801
Projeto 4: reduzir ruído
nos nossos dados de entrada.

00:07:07.834 --> 00:07:08.968
Certo?

00:07:09.000 --> 00:07:11.367
Vamos pegar isso
e dizer: tudo bem.

00:07:11.400 --> 00:07:14.000
Como podemos modificar isto

00:07:14.033 --> 00:07:17.868
para não dar mais os pesos
de acordo com as contagens?

00:07:17.901 --> 00:07:21.934
Se fizermos isso,
isto sempre seria 1 ou 0.

00:07:21.968 --> 00:07:24.300
Vamos mudar para que seja

00:07:24.334 --> 00:07:27.434
só uma representação
de vocabulário em geral.

00:07:27.467 --> 00:07:29.968
Deve funcionar, por que...

00:07:30.000 --> 00:07:34.100
O ponto, "o", "para" e "eu"
ainda estarão aqui.

00:07:34.133 --> 00:07:37.033
A rede deve decidir
que palavras são importantes.

00:07:37.067 --> 00:07:40.968
Mas não precisa dizer,
ponto vezes 27,

00:07:41.000 --> 00:07:44.901
para que o peso do ponto vezes 27
vá para a camada oculta.

00:07:44.934 --> 00:07:47.400
É muito sinal
para empurrar para baixo.

00:07:47.434 --> 00:07:49.934
Mas se dissermos só 1 e 0,

00:07:49.968 --> 00:07:52.534
e fizermos
uma representação binária,

00:07:52.567 --> 00:07:54.300
deverá ter bem menos ruído

00:07:54.334 --> 00:07:56.968
e ser mais fácil
para o neurônio entender.

00:07:57.000 --> 00:08:00.667
Acho que vai ser
bem fácil mudar.

00:08:00.701 --> 00:08:03.534
Vou fazer o projeto aqui.

00:08:04.267 --> 00:08:06.901
Está na nossa camada
de atualizar entrada.

00:08:06.934 --> 00:08:11.567
Estávamos incrementando,
mas se tirarmos este +,

00:08:11.601 --> 00:08:13.467
vamos fazer igual a 1.

00:08:13.501 --> 00:08:17.400
Assim, cada valor na camada zero,
vamos colocar igual a 1

00:08:17.434 --> 00:08:19.267
se esse termo existir.

00:08:19.300 --> 00:08:20.701
Vamos refazer isso.

00:08:20.734 --> 00:08:24.968
Vamos pegar o valor de treinamento
daqui de cima.

00:08:25.567 --> 00:08:27.467
Faremos o nosso original.

00:08:29.467 --> 00:08:32.534
E também precisamos
do treinado.

00:08:32.567 --> 00:08:35.133
Copie isso
e livre-se disso.

00:08:35.801 --> 00:08:39.100
Vamos manter um registro
aqui embaixo.

00:08:40.767 --> 00:08:42.634
Vamos criar a nossa rede.

00:08:43.200 --> 00:08:47.100
Esta é a nossa nova classe.
Vamos treinar.

00:08:47.667 --> 00:08:49.767
Vejam isso, vejam isso!

00:08:50.901 --> 00:08:55.734
Já chegou a 60%
após 2% de progresso.

00:08:55.767 --> 00:08:59.000
Um progresso incrível,
vejam.

00:08:59.033 --> 00:09:02.634
Eliminamos muito ruído,

00:09:02.667 --> 00:09:05.067
por nos livrarmos
destes pesos.

00:09:05.100 --> 00:09:09.367
E a rede neural achou correlações
muito mais rápido, vejam, 70%.

00:09:09.400 --> 00:09:11.601
E só treinamos 9%.

00:09:11.634 --> 00:09:17.334
Isto é aumentar o sinal
e reduzir o ruído.

00:09:17.367 --> 00:09:20.367
É tornar mais óbvio
para a rede neural,

00:09:20.400 --> 00:09:25.501
para lidar com o sinal,
combiná-lo de formas interessantes,

00:09:25.534 --> 00:09:27.767
e buscar
padrões mais difíceis.

00:09:27.801 --> 00:09:30.501
Livre-se do ruído
nos dados de treinamento.

00:09:30.534 --> 00:09:33.801
Podíamos ter passado
dias e dias aqui

00:09:33.834 --> 00:09:36.701
ajustando o alfa,
movendo-o,

00:09:36.734 --> 00:09:39.501
baixando-o, tentando
um treinamento devagar,

00:09:39.534 --> 00:09:42.767
mas podemos ter um grande alfa
e dar passos enormes

00:09:42.801 --> 00:09:44.367
progredindo rapidamente,

00:09:44.400 --> 00:09:46.834
se nos livrarmos
deste ruído tolo.

00:09:46.868 --> 00:09:50.467
Queremos que nossa rede neural
faça coisas interessantes.

00:09:50.501 --> 00:09:53.133
Coisas interessantes
não é ignorar pontos.

00:09:53.167 --> 00:09:56.400
É identificar quais palavras
são relevantes.

00:09:56.434 --> 00:09:59.267
Quais combinações
de palavras são relevantes.

00:09:59.300 --> 00:10:03.167
Queremos que a rede neural
ache representações interessantes,

00:10:03.200 --> 00:10:06.367
que faça coisas interessantes
nesta camada oculta,

00:10:06.400 --> 00:10:13.300
que entenda o vocabulário
do que é dito na resenha.

00:10:13.334 --> 00:10:16.634
É o que queremos.
Vejam isso. Quase 80%.

00:10:16.667 --> 00:10:18.767
Vou deixar
continuar a treinar.

00:10:18.801 --> 00:10:21.367
Faça isso também.

00:10:23.534 --> 00:10:27.667
E...
sim, está parecendo ótimo.

00:10:27.701 --> 00:10:31.567
Estou notando...
talvez por que seja um vídeo...

00:10:31.601 --> 00:10:34.300
Queria que o treinamento
fosse mais rápido.

00:10:34.334 --> 00:10:36.267
A próxima coisa
que quero fazer

00:10:36.300 --> 00:10:38.934
é ver o que acontece
dentro da rede neural

00:10:38.968 --> 00:10:41.300
e ver se conseguimos
mais velocidade.

00:10:41.334 --> 00:10:44.267
Por agora, vou deixar
isso continuar a treinar.

