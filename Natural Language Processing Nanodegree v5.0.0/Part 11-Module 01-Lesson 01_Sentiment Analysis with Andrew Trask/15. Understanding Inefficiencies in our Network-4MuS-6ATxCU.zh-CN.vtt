WEBVTT
Kind: captions
Language: zh-CN

00:00:01.690 --> 00:00:02.120
好

00:00:02.120 --> 00:00:05.570
在上一节 我们优化了神经网络

00:00:05.570 --> 00:00:11.060
通过消除干扰噪音 更好地找到数据集的相关性

00:00:11.060 --> 00:00:14.661
结果是 神经网络处理信号的能力大大提高了

00:00:14.660 --> 00:00:16.451
它在训练数据中的准确率达到了 83%

00:00:16.451 --> 00:00:17.780
测试数据的准确率达到了 85%

00:00:17.780 --> 00:00:20.920
而这只是进行了一次迭代之后的表现

00:00:20.920 --> 00:00:24.020
我们可以继续训练下去 再有所提升

00:00:24.019 --> 00:00:26.859
但我们现在会继续观察这一迭代基准

00:00:26.859 --> 00:00:30.149
看看能让神经网络的训练速度达到多快

00:00:30.149 --> 00:00:33.390
之前准确率只有 60% 所以我们的神经网络

00:00:33.390 --> 00:00:37.609
在准确率和训练速度方面已经有了大幅提升

00:00:37.609 --> 00:00:39.299
进步是巨大的

00:00:39.299 --> 00:00:42.070
但是 它的原始计算速度

00:00:42.070 --> 00:00:47.920
从它进行全程所需的秒数来说 还是很慢

00:00:47.920 --> 00:00:53.490
我想要做的 就是研究网络本身 想一想

00:00:53.490 --> 00:00:56.600
这个系统里 究竟哪部分是在浪费计算力？

00:00:56.600 --> 00:00:58.579
之前 我们发现了数据里面的冗余

00:00:58.579 --> 00:01:00.929
现在 我们想找到神经网络内部的冗余部分

00:01:00.929 --> 00:01:02.679
因为你知道 这很有意思

00:01:02.679 --> 00:01:05.019
你可以在理论方面想各种办法

00:01:05.019 --> 00:01:07.130
试着如何让网络学得更快一些

00:01:07.129 --> 00:01:10.199
但事实上 另一个问题就是学习时间

00:01:10.200 --> 00:01:11.969
训练过程确实要花太久时间

00:01:11.969 --> 00:01:15.257
所以 我们也可以尝试在计算方面做一些优化

00:01:15.257 --> 00:01:16.572
让它训练速度大大提高

00:01:16.572 --> 00:01:21.609
同时也能够学习我们想让它学习的东西

00:01:21.609 --> 00:01:25.609
你的网络训练速度越快

00:01:25.609 --> 00:01:27.510
说实话 你就会训练得更久一点 才会厌倦

00:01:27.510 --> 00:01:29.740
去找别的有趣的事情去做

00:01:29.739 --> 00:01:31.929
对于训练神经网络的人来说

00:01:31.930 --> 00:01:33.050
你可以一直训练下去

00:01:33.049 --> 00:01:34.489
没有自然意义上的结束点

00:01:34.489 --> 00:01:36.280
它不像概率图模型

00:01:36.280 --> 00:01:36.939
或者其他许多模型

00:01:36.939 --> 00:01:40.969
你对很多不同的事物进行离散计数

00:01:40.969 --> 00:01:42.260
然后统计完了 就结束了

00:01:42.260 --> 00:01:45.170
但在准确率方面 神经网络可以一直训练精益求精

00:01:45.171 --> 00:01:48.414
但是 它的训练速度越快 你就能训练越多的数据

00:01:48.414 --> 00:01:49.439
你的网络也会越强大

00:01:49.439 --> 00:01:52.310
那么 现在我们要做的 就是分析

00:01:52.310 --> 00:01:55.359
网络中所出现的状况 

00:01:55.359 --> 00:01:58.069
寻找可以剔除的东西 让神经网络变得更快

00:01:59.200 --> 00:02:03.629
然后 我发现了第一个问题：

00:02:03.629 --> 00:02:05.959
我们为第 0 层创建了一个非常巨大的向量

00:02:05.959 --> 00:02:08.599
含有 74,000 多个值

00:02:08.599 --> 00:02:15.250
而只有少数几个位置的值是 1 

00:02:15.250 --> 00:02:16.990
为什么它会有影响呢？

00:02:16.990 --> 00:02:20.030
因为这里的正向传播是加权求和

00:02:20.030 --> 00:02:24.759
我们用这个 1 乘以这些权重 再将它们加到第 1 层

00:02:24.759 --> 00:02:27.229
然后 我们取下一个 0

00:02:27.229 --> 00:02:31.569
再乘以这些权重 再加到第一层上

00:02:31.569 --> 00:02:32.879
哦 稍等一下

00:02:32.879 --> 00:02:35.889
我们将 0 乘以这些权值

00:02:35.889 --> 00:02:37.419
再将结果添加到这里

00:02:37.419 --> 00:02:41.479
这意味着每当有 0 时 当我们对这个向量

00:02:41.479 --> 00:02:46.619
做一个巨大的矩阵乘法去创建第一层时 所有这些 0

00:02:46.620 --> 00:02:51.250
都没起任何作用 因为 0 乘以任何数都是 0

00:02:51.250 --> 00:02:54.360
所以 0 乘以这个向量 再加到第 1 层

00:02:54.360 --> 00:02:58.920
丝毫没有改变第一层的值

00:02:58.919 --> 00:03:03.349
所以对我来说 这就是这个网络中最大的低效率来源

00:03:03.349 --> 00:03:06.879
我要从计算的角度

00:03:06.879 --> 00:03:10.370
向你证明这一情况 看这里

00:03:10.370 --> 00:03:13.030
我们假设建一个第 0 层 它只有 10 个值

00:03:13.030 --> 00:03:16.240
我们将在这里描述它

00:03:16.240 --> 00:03:20.272
我们说 layer_0……

00:03:20.271 --> 00:03:27.584
layer_0 [4] = 1 layer_0 假装我们放入了一些词

00:03:29.277 --> 00:03:35.219
现在再看一下第 0 层 它看起来是这样的 对吧？

00:03:35.219 --> 00:03:39.792
那么 weights_0_1 我们假设它只是一个随机的矩阵

00:03:42.840 --> 00:03:47.454
然后我们看一看 layer_0.dot[weights_0_1]

00:03:47.454 --> 00:03:50.120
好 这就是输出值

00:03:50.120 --> 00:03:55.640
那么 如果我们换一种做法 只是将这些向量加起来呢？

00:03:55.639 --> 00:03:58.449
我们说 1 乘以这些值 放到这里

00:03:58.449 --> 00:04:01.951
如果我们有这两个索引值 indices

00:04:01.951 --> 00:04:06.895
4 和 9 我们还得有一个新层 对吧？

00:04:06.895 --> 00:04:10.529
那么 layer_1 = np.zeros() 它有 5 个值

00:04:10.530 --> 00:04:15.234
然后我们说：

00:04:15.234 --> 00:04:19.490
for index in indices:

00:04:19.490 --> 00:04:25.994
weights_0_1[index] 对了 layer_1

00:04:29.290 --> 00:04:34.124
+= 1 *  因为 1 要

00:04:34.124 --> 00:04:39.870
进入数据中 layer_1

00:04:39.870 --> 00:04:40.160
嘭

00:04:40.160 --> 00:04:42.570
完全一样的值 看到了吧

00:04:42.569 --> 00:04:46.930
而且最棒的是 我们实际上只调用了这个矩阵的一部分

00:04:46.930 --> 00:04:51.480
也就是 70,000 多个值中的两个

00:04:53.420 --> 00:04:55.980
那么我们刚刚节约了

00:04:55.980 --> 00:05:02.150
对剩下的 69,000 多个值进行这一操作

00:05:02.149 --> 00:05:03.810
这会省下很大一部分工作量

00:05:03.810 --> 00:05:07.399
现在 我们来看看它实际上运行效果如何

00:05:07.399 --> 00:05:09.099
一定是非常好的

00:05:09.100 --> 00:05:11.970
我很想知道它的结果

00:05:13.209 --> 00:05:17.769
我们再回到这个神经网络 看看有什么其他可以提高效率之处

00:05:17.769 --> 00:05:22.609
另一个效率的地方就是： 1 乘以任何值 都是它自己

00:05:22.610 --> 00:05:25.280
所以用 1 乘也是浪费时间

00:05:25.279 --> 00:05:30.149
那么 不如我们直接改成加法

00:05:30.149 --> 00:05:33.079
也就是直接删掉 1 * 对吧？

00:05:33.079 --> 00:05:35.019
所以 索引4 和 9 是 1 和之前一样

00:05:35.019 --> 00:05:35.430
好极了

00:05:35.430 --> 00:05:40.040
所以我们省掉了做乘法 省掉了所有这些

00:05:40.040 --> 00:05:43.090
而依然获得了同样的隐藏层

00:05:43.089 --> 00:05:48.009
和我们做整个向量矩阵的点乘时

00:05:48.009 --> 00:05:49.899
得到的值一样

00:05:49.899 --> 00:05:51.310
我太喜欢这个了

00:05:51.310 --> 00:05:52.910
我觉得这里有很多构建要做

00:05:52.910 --> 00:05:54.110
大部分权重都在这里 对吧？

00:05:54.110 --> 00:05:56.300
从隐藏层到输出层只有四个权重

00:05:56.300 --> 00:05:57.610
在我们的例子中 即隐藏层的大小

00:05:58.720 --> 00:06:04.900
我觉得我们的层应该更大 但大部分计算都耗在这里

00:06:04.899 --> 00:06:09.209
74,000 乘以不管多大的隐藏层 都构成了神经网络的

00:06:09.209 --> 00:06:10.487
训练和运行的一大块工作量

00:06:10.487 --> 00:06:14.077
说到这 就要提到我们的下一个项目了

00:06:14.077 --> 00:06:21.489
项目 5 是要将它安装到之前的神经网络中

00:06:21.490 --> 00:06:27.160
下面 我想让大家自己试一试

00:06:27.160 --> 00:06:31.048
稍后回来 我们聊聊如何在之前的神经网络中实现这部分

00:06:31.048 --> 00:06:32.589
祝你好运

