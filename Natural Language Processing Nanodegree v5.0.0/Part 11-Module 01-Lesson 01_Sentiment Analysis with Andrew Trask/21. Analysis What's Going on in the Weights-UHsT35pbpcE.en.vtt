WEBVTT
Kind: captions
Language: en

00:00:00.340 --> 00:00:00.930
Welcome back.

00:00:00.930 --> 00:00:04.170
So, in this section we're going
to be talking a little more

00:00:04.170 --> 00:00:08.189
about what's going on, a little more
theory, a little less project based.

00:00:08.189 --> 00:00:11.710
And it's really just about trying to
understand what are these weights doing?

00:00:11.710 --> 00:00:15.840
What can I attach my mind to when I'm
thinking about a neural net training

00:00:15.840 --> 00:00:19.080
that's going to be able to help me debug
it, help me be able to find better

00:00:19.080 --> 00:00:23.660
signal and cut out better noise and just
have a better framework to work from.

00:00:23.660 --> 00:00:25.110
To think about things like that.

00:00:25.110 --> 00:00:30.150
So things that we know, so we're taking
this 1 that represents X1, right?

00:00:30.150 --> 00:00:32.310
And we're summing it into layer_1 and
we're making a prediction.

00:00:32.310 --> 00:00:36.020
Now there's an interesting
phenomenon that happens when,

00:00:36.020 --> 00:00:38.520
especially when you have
linear layers right here.

00:00:38.520 --> 00:00:39.920
And I talked about this before

00:00:41.140 --> 00:00:44.110
very briefly when I was saying that we
wanted to have a linear layer here and

00:00:44.110 --> 00:00:46.800
I said I would talk about it later,
and now's the time.

00:00:46.800 --> 00:00:50.290
When we have a linear layer like this,
it's making a prediction.

00:00:51.430 --> 00:00:55.870
What's really happening is these four,
in this case or

00:00:55.870 --> 00:00:59.970
more, however many you decide it to be,
weights are feature detectors.

00:01:01.070 --> 00:01:05.319
What they're doing is, they're trying to
detect a certain state in this neuron,

00:01:05.319 --> 00:01:09.790
or this neuron, or this neuron.

00:01:09.790 --> 00:01:11.710
Right, and so
if this is like a big negative number,

00:01:12.750 --> 00:01:16.370
this is looking for
this to be a big negative number.

00:01:16.370 --> 00:01:19.510
But if this is zero or
if this is a positive number,

00:01:19.510 --> 00:01:23.570
it's going to have the opposite effect,
right?

00:01:23.570 --> 00:01:26.480
So these are looking for a certain set
of states, it's looking for 0.5, 0.9,

00:01:26.480 --> 00:01:28.430
1 and negative 2, right?

00:01:28.430 --> 00:01:33.040
So there are certain hidden layers that
cause this to output a high number and

00:01:33.040 --> 00:01:36.540
a certain hidden layers that cause
this to output a low number.

00:01:36.540 --> 00:01:40.310
And it's just weighted sum,
it's actually pretty simple, right?

00:01:40.310 --> 00:01:42.510
If these are positive in
all the same times or

00:01:42.510 --> 00:01:46.060
all the same ways that these weights
are positive, this puts a lot.

00:01:46.060 --> 00:01:49.345
But if this is positive and
this is negative and this is positive.

00:01:49.345 --> 00:01:50.155
Positive, and this is negative.

00:01:50.155 --> 00:01:51.605
And this is positive,
and this is negative.

00:01:51.605 --> 00:01:52.815
And this is positive,
and this is negative.

00:01:52.815 --> 00:01:56.445
Then this ends up being a really
really low number, right?

00:01:56.445 --> 00:02:00.455
So, when these have the same polarity,
and

00:02:00.455 --> 00:02:04.085
have high values in all the same places,
this makes a high prediction.

00:02:04.085 --> 00:02:06.240
So what does that mean for
all this stuff back here?

00:02:06.240 --> 00:02:08.658
Well, horrible and terrible,

00:02:08.658 --> 00:02:13.058
when we think about what
commonly happens in our data set.

00:02:13.058 --> 00:02:17.280
They're associated with negativity,
right?

00:02:17.280 --> 00:02:20.370
Excellent, and fantastic,
if that was another word up here,

00:02:20.370 --> 00:02:22.670
are constantly trying
to predict this one.

00:02:22.670 --> 00:02:28.170
They're trying to relate to this
vector in a certain way, right?

00:02:28.170 --> 00:02:32.750
So what's interesting is that if
these words are being trained,

00:02:32.750 --> 00:02:34.680
which they're just rows in the matrix,
right?

00:02:34.680 --> 00:02:40.780
They're being trained to
manipulate this in the same way,

00:02:40.780 --> 00:02:41.690
to create a high value or a low value.

00:02:41.690 --> 00:02:45.060
What does it mean when
we've learned here.

00:02:45.060 --> 00:02:46.510
We backup the gradients and

00:02:46.510 --> 00:02:50.770
we update these to both
exhibit the same phenomenon.

00:02:50.770 --> 00:02:55.210
Horrible and terrible are both supposed
to affect this node in the same way.

00:02:55.210 --> 00:02:58.810
And as we're updating these weights to
cause them to do that, what happens?

00:02:58.810 --> 00:03:01.370
Their weights become similar, why?

00:03:01.370 --> 00:03:04.330
Because both of them have the same goal,
right?

00:03:04.330 --> 00:03:09.270
So, if this one predicts positively,
this thing's going to go no, no, no.

00:03:09.270 --> 00:03:13.490
Update your weights,
mister horrible vector, and

00:03:13.490 --> 00:03:18.140
be a little more like this, because this
is what causes me to predict negative.

00:03:18.140 --> 00:03:21.480
That's what that propagation is all
about, it's about this node being able

00:03:21.480 --> 00:03:26.020
to tell these weights how to be better,
how to be more accurate, right?

00:03:26.020 --> 00:03:29.890
And because it's sending the same
signal, horrible and terrible, right?

00:03:29.890 --> 00:03:31.670
Be negative, right?

00:03:31.670 --> 00:03:36.640
They end up congregating together,
and it's really quite cool.

00:03:36.640 --> 00:03:37.380
So check this out.

00:03:37.380 --> 00:03:39.380
So we've got kind of a heuristic for

00:03:39.380 --> 00:03:44.190
seeing how similar the different
vectors are for each word, and

00:03:44.190 --> 00:03:48.300
now we can go through and we can say,
okay, show me the vectors.

00:03:48.300 --> 00:03:51.530
The words that have the most
similar vector to excellent.

00:03:51.530 --> 00:03:53.470
I'm just using a simple .product.

00:03:53.470 --> 00:03:56.820
Excellent, perfect, amazing, today,
wonderful, fun, great, best, and

00:03:56.820 --> 00:03:59.130
this is on a trained neural net.

00:03:59.130 --> 00:04:00.610
Look at that!

00:04:00.610 --> 00:04:05.690
After we trained, because these words,
excellent, perfect, amazing, all were

00:04:05.690 --> 00:04:11.260
supposed to create the same effect or
very similar effect on the output.

00:04:11.260 --> 00:04:12.760
They were given similar weights.

00:04:14.910 --> 00:04:21.886
Inversely, if we say the opposite,
if we say terrible.

00:04:21.886 --> 00:04:24.083
[BLANK_AUDIO]

00:04:24.083 --> 00:04:28.901
Worst, awful, waste, poor, terrible,
dull, poorly, disappointment, fail,

00:04:28.901 --> 00:04:31.280
disappointing, boring, unfortunate.

00:04:31.280 --> 00:04:32.480
I mean, it just goes on, and on, and on.

00:04:32.480 --> 00:04:35.540
This is actually an even better
filter than out original counts.

00:04:35.540 --> 00:04:37.650
I mean, our counts had
these weird names in them,

00:04:37.650 --> 00:04:41.002
and they just had a noise,
but this is awesome.

00:04:41.002 --> 00:04:45.980
It's clear, it's evident that
the network has figured out

00:04:45.980 --> 00:04:48.000
that these words are related, right?

00:04:48.000 --> 00:04:52.020
These words are both trying to create
the same effect on the output data,

00:04:52.020 --> 00:04:52.490
right?

00:04:52.490 --> 00:04:55.380
Now to be clear,
we can't over state this.

00:04:55.380 --> 00:04:57.960
It doesn't know that these have
the same meaning generally.

00:04:57.960 --> 00:05:01.590
All it knows is that they have
the same meaning in the context

00:05:01.590 --> 00:05:05.020
of this one output neuron, because
it's the only output neuron there.

00:05:05.020 --> 00:05:07.330
They exist to create the same
effect as this neuron.

00:05:07.330 --> 00:05:12.650
Now we had a bunch of different neurons
that were pushing it in one way or

00:05:12.650 --> 00:05:14.730
another to be the same or

00:05:14.730 --> 00:05:18.210
different in a bunch of different ways,
this could get a lot more complicated.

00:05:18.210 --> 00:05:21.790
But basically it grouped
words by sentiment, right?

00:05:23.580 --> 00:05:24.300
It said hey,

00:05:24.300 --> 00:05:27.070
all your negative words have a really
similar vector coming out of you and

00:05:27.070 --> 00:05:30.620
all your positive words have a really
similar vector coming out of you.

00:05:30.620 --> 00:05:32.440
And that's a really powerful intuition,

00:05:32.440 --> 00:05:35.660
because that means you can look at
a neural net like this and you can say,

00:05:35.660 --> 00:05:36.970
hm, okay,
we're going to trade it for this.

00:05:36.970 --> 00:05:41.600
So it's probably going to group these
vectors like this and these vectors like

00:05:41.600 --> 00:05:46.600
this, and you can kind of start to think
about what´s going on under the hood and

00:05:46.600 --> 00:05:51.660
how you can, once again,
identify signal and noise.

00:05:51.660 --> 00:05:55.700
Because that´s what framing
the problem is really all about.

00:05:55.700 --> 00:06:00.398
Now to kind of add a bit of
extra kind of jazz to this.

00:06:00.398 --> 00:06:05.527
What I like to do is something
that's called tf–idf.

00:06:05.527 --> 00:06:10.030
So tf–idf, what it does is it takes
a high dimensional vector, and

00:06:10.030 --> 00:06:14.270
in our case, high dimensions
might just be four, right?

00:06:14.270 --> 00:06:17.219
And it clusters them
into two dimensions, or

00:06:17.219 --> 00:06:21.871
some other, with how many you pick, so
we can plot them on an X Y graph and

00:06:21.871 --> 00:06:25.925
see how they're naturally
clustered in a higher dimension.

00:06:25.925 --> 00:06:30.940
And the cool thing is
that we can do that here.

00:06:30.940 --> 00:06:33.870
So what I'm going to do is
I'm going to cluster them,

00:06:33.870 --> 00:06:37.830
but then I'm also going to say hey,
the words that are our

00:06:37.830 --> 00:06:40.500
ratio instead of really positive
I'm going to make them green, and

00:06:40.500 --> 00:06:42.690
the ones that were really negative
I'm going to to make them black.

00:06:42.690 --> 00:06:45.560
And so, in theory because all
these vectors are really similar,

00:06:45.560 --> 00:06:49.175
it should cluster, or it should
show how they're clustered already.

00:06:49.175 --> 00:06:52.445
And look at that.

00:06:52.445 --> 00:06:56.457
So it's, I mean there's a couple
that are scattered in between but,

00:06:56.457 --> 00:07:00.080
the neural net that has
clearly separated those.

00:07:00.080 --> 00:07:02.520
There's this big long negative cluster,
big long negative cluster.

00:07:02.520 --> 00:07:05.540
And then these really
nice positive clusters.

00:07:05.540 --> 00:07:06.070
Let's take a look.

00:07:06.070 --> 00:07:07.480
So I think we can add labels here.

00:07:07.480 --> 00:07:10.850
I left them in, that was kind of messy.

00:07:10.850 --> 00:07:13.530
Check this out.

00:07:13.530 --> 00:07:18.530
So we click that real zoom,
you can kind of look in.

00:07:18.530 --> 00:07:19.020
Wow, interesting.

00:07:20.530 --> 00:07:23.730
So unconvincing, dimensional, lousy.

00:07:25.350 --> 00:07:27.920
Look at this.

00:07:27.920 --> 00:07:34.740
Extraordinary, lovable,
provoking, award, touched.

00:07:35.960 --> 00:07:37.490
It's beautiful, it's awesome.

00:07:37.490 --> 00:07:38.400
So we can kind of look around.

00:07:38.400 --> 00:07:42.776
So this,
maybe we'll call this a vector space.

00:07:42.776 --> 00:07:45.550
And actually you see the ones that

00:07:45.550 --> 00:07:48.060
are kind of agnostic are like
the ones with those names.

00:07:48.060 --> 00:07:49.780
So this is kind of a little
bit of a noisier one.

00:07:49.780 --> 00:07:52.740
It's like Dan right there.

00:07:52.740 --> 00:07:57.190
[LAUGH] But
still it did a really good job of

00:07:57.190 --> 00:08:00.170
clustering vectors by sentiment
automatically, right?

00:08:00.170 --> 00:08:03.835
I guess I told it to do this because I
gave it training data to allow it to do

00:08:03.835 --> 00:08:04.233
this.

00:08:04.233 --> 00:08:07.939
But all it was trying to do is predict
accurately, but implicitly while

00:08:07.939 --> 00:08:11.644
it was trying to predict accurately,
it had to group these words to have

00:08:11.644 --> 00:08:15.909
similar different vectors so that it can
classify them in aggregate when they're

00:08:15.909 --> 00:08:19.485
all summed together as being
more positive or more negative.

00:08:19.485 --> 00:08:21.225
And that's what the neural network did.

00:08:21.225 --> 00:08:25.580
And it's really cool to kind of
see it kind of behave this way.

00:08:25.580 --> 00:08:31.020
And so I guess to close,
this is what neural nets are all about.

00:08:31.020 --> 00:08:32.500
This is what framing
the problem's all about.

00:08:32.500 --> 00:08:34.620
It's about understanding what's
going on under the hood so

00:08:34.620 --> 00:08:37.929
you can reduce the noise
as much as possible.

00:08:37.929 --> 00:08:39.539
Increase the signal as
much as possible so

00:08:39.539 --> 00:08:42.380
that it can find the structure that
you're interested in it finding.

00:08:42.380 --> 00:08:47.090
We didn't tell it that lovable and

00:08:47.090 --> 00:08:51.290
extraordinary were similar
terms in this context.

00:08:51.290 --> 00:08:54.150
All we said was, hey,
all these reviews are positive,

00:08:54.150 --> 00:08:55.970
all these reviews are negative,
figure it out.

00:08:55.970 --> 00:09:00.005
And it figured out which terms were
similar, which terms were different.

00:09:00.005 --> 00:09:01.665
It was able to do that to
make accurate predictions.

00:09:01.665 --> 00:09:05.625
So I hope you've enjoyed this segment.

00:09:05.625 --> 00:09:09.485
So we'll see you soon and
continue to enjoy your course.

