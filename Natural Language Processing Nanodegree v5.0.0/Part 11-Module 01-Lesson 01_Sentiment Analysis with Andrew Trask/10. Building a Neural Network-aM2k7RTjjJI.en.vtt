WEBVTT
Kind: captions
Language: en

00:00:00.100 --> 00:00:02.100
So in this section we're going to
take everything we've learned and

00:00:02.100 --> 00:00:04.720
we're going to build our first neural
network to train over the datasets that

00:00:04.720 --> 00:00:06.130
we just created.

00:00:06.130 --> 00:00:07.689
Now what I'd like for you to do for

00:00:07.689 --> 00:00:10.887
this project is to start with your
neural net form the last chapter.

00:00:10.887 --> 00:00:14.568
I guess the last module that you did
where you built a basic neural network

00:00:14.568 --> 00:00:16.930
for predicting on
a structured data dataset.

00:00:16.930 --> 00:00:19.367
Then I would like to take this
three layer neural network and

00:00:19.367 --> 00:00:21.375
remove the non-linearity
in the hidden layer.

00:00:21.375 --> 00:00:22.870
I'll show you why later.

00:00:22.870 --> 00:00:26.240
Then I would like for you to use
the functions that we created above

00:00:26.240 --> 00:00:29.200
to generate the trained data on the fly.

00:00:29.200 --> 00:00:32.390
So a review and a label goes in.

00:00:32.390 --> 00:00:36.940
It's converted into the two vectors that
we need for the input and output data.

00:00:36.940 --> 00:00:39.886
And then a forward pass and
a back prop pass happen, so

00:00:39.886 --> 00:00:42.186
that the data is being
trained on the fly.

00:00:42.186 --> 00:00:44.363
Next thing I would like for
you to do is create a function for

00:00:44.363 --> 00:00:45.570
pre-processing the data.

00:00:45.570 --> 00:00:49.040
So that all of these kind of
vocabulary variables, and

00:00:49.040 --> 00:00:53.140
word2index variables
are variables of this class, so

00:00:53.140 --> 00:00:55.580
everything is self-contained
in that class.

00:00:55.580 --> 00:00:59.530
And then modify the train variable to
actually train over the entire corpus,

00:00:59.530 --> 00:01:03.520
instead of just on one inputs and
targets list.

00:01:03.520 --> 00:01:06.148
So, that's kind of what I would like for
you to do.

00:01:06.148 --> 00:01:07.867
You can start with
either with this shell,

00:01:07.867 --> 00:01:10.543
that was presented at the beginning
of your last week's chapter or

00:01:10.543 --> 00:01:13.050
with the complete neural net
that you started with last time.

00:01:14.460 --> 00:01:17.914
Now if you do need help, obviously
the first thing to do is to go re-watch

00:01:17.914 --> 00:01:21.431
the previous week's Udacity lectures,
make you're familiar with that

00:01:21.431 --> 00:01:24.435
propagation and it's gradient ascent,
and the error measure

00:01:24.435 --> 00:01:28.280
that we're using and also how to modify
back prop to get rid of non-linearity.

00:01:28.280 --> 00:01:30.370
And if you still need more help,
go ahead and

00:01:30.370 --> 00:01:32.940
check out chapters three through
five of Grokking Deep Learning.

00:01:32.940 --> 00:01:35.690
I've included the 40% off code here.

00:01:35.690 --> 00:01:39.801
It does a comprehensive review of fore
prop, back prop and error gradients and

00:01:39.801 --> 00:01:41.495
stochastic gradient descent.

00:01:41.495 --> 00:01:43.903
In a moment I'll show you how I
put this network together, and

00:01:43.903 --> 00:01:47.210
then we'll kind of talk about
the different changes that we made.

00:01:47.210 --> 00:01:48.540
All right, I'll see you then.

