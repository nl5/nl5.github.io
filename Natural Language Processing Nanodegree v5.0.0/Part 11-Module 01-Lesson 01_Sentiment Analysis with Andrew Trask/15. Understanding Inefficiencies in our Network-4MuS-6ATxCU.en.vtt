WEBVTT
Kind: captions
Language: en

00:00:01.690 --> 00:00:02.120
All right.

00:00:02.120 --> 00:00:05.570
So in the last section, we optimized
our neural network to better find

00:00:05.570 --> 00:00:11.060
the correlation of our data set by
removing some distracting noise.

00:00:11.060 --> 00:00:14.661
And the neural network attended
the signal so much better.

00:00:14.661 --> 00:00:16.451
It trained 83% accuracy
in the training data, and

00:00:16.451 --> 00:00:17.780
the testing accuracy was up to 85%.

00:00:17.780 --> 00:00:20.920
And this was just after one iteration.

00:00:20.920 --> 00:00:24.020
So we probably could have kept training
it, and squeeze out a little more,

00:00:24.020 --> 00:00:26.860
but we're going to keep looking
at this one iteration benchmark

00:00:26.860 --> 00:00:30.150
to see how fast we can get
the neural net to train.

00:00:30.150 --> 00:00:33.390
I mean, this is up from 60% before,
so this is a huge gain.

00:00:33.390 --> 00:00:37.610
In accuracy and the speed of
training for our neural network, and

00:00:37.610 --> 00:00:39.300
that was a lot of progress.

00:00:39.300 --> 00:00:42.070
However, the actual raw
computational speed,

00:00:42.070 --> 00:00:47.920
the number of seconds that it takes to
do a full pass is still pretty slow.

00:00:47.920 --> 00:00:53.490
What I want to be able to do in here
is attack this network and say okay.

00:00:53.490 --> 00:00:56.600
What is this thing doing that is
wasteful on the computation site.

00:00:56.600 --> 00:00:58.580
So before we had kind
of a wasteful data and

00:00:58.580 --> 00:01:00.930
now I want to say what is
wasteful inside this neural net.

00:01:00.930 --> 00:01:02.680
Coz you know it's funny.

00:01:02.680 --> 00:01:05.019
You could do a lot of things
on this theory side and

00:01:05.019 --> 00:01:07.130
try to say okay how can it learn faster.

00:01:07.130 --> 00:01:10.200
But truth is,
the other one before was the learning.

00:01:10.200 --> 00:01:11.970
It was just taking a really long time.

00:01:11.970 --> 00:01:15.258
So we also could of tied a optimize
the computational side so

00:01:15.258 --> 00:01:16.573
that it's just train so

00:01:16.573 --> 00:01:21.610
much more faster that it's still able to
learn what we want the other to learn.

00:01:21.610 --> 00:01:25.610
The faster you can get your neural
to train then, to be honest.

00:01:25.610 --> 00:01:27.510
The longer you'll let it
train before you get bored.

00:01:27.510 --> 00:01:29.740
And you'll find more interesting stuff.

00:01:29.740 --> 00:01:31.930
And people who train neural nets.

00:01:31.930 --> 00:01:33.050
You can kind of just keep training.

00:01:33.050 --> 00:01:34.490
There's no natural finish.

00:01:34.490 --> 00:01:36.280
It's unlike probabilistic
graphical models.

00:01:36.280 --> 00:01:36.940
Or many of them,

00:01:36.940 --> 00:01:40.970
anyways, where you do a discrete
count of lots of different things.

00:01:40.970 --> 00:01:42.261
And then when it's done, it's done.

00:01:42.261 --> 00:01:45.171
In accuracy, neural nets can
kind of just keep training, right?

00:01:45.171 --> 00:01:48.414
But the faster you can get it to train,
the more data that you can put into it,

00:01:48.414 --> 00:01:49.440
the stronger it can be.

00:01:49.440 --> 00:01:52.310
So what we're going to do here
is we're going to analyze

00:01:52.310 --> 00:01:55.360
what's going on in our network and
look for things that we can

00:01:55.360 --> 00:01:58.070
shave out that are going to allow
our neural net to go faster.

00:01:59.200 --> 00:02:03.630
And now there's one first one
that kind of stands out to me.

00:02:03.630 --> 00:02:05.960
We're creating a really big vector for
layer 0.

00:02:05.960 --> 00:02:08.600
It's 74,000 and
70 something values, right?

00:02:08.600 --> 00:02:15.250
And only a handful of them
are being turned onto 1.

00:02:15.250 --> 00:02:16.990
Now why does this matter?

00:02:16.990 --> 00:02:20.030
Well, this four propagation
step is a weighted sum.

00:02:20.030 --> 00:02:24.760
We take this 1, we multiply it by
these weights, we add it into layer 1.

00:02:24.760 --> 00:02:27.230
Then, we take the next one, 0,

00:02:27.230 --> 00:02:31.570
we multiply it by these weights and
we add that into layer 1.

00:02:31.570 --> 00:02:32.880
Woh, wait a minute.

00:02:32.880 --> 00:02:35.890
We take a zero and
we multiple it by these weights, and

00:02:35.890 --> 00:02:37.420
then we add the result of that.

00:02:37.420 --> 00:02:41.480
That means every time there's a zero,
when we take this vector and

00:02:41.480 --> 00:02:46.620
do a big matrix multiplication to
create our layer one, all these zeros

00:02:46.620 --> 00:02:51.250
aren't doing anything, because zero
times anything is still just zero.

00:02:51.250 --> 00:02:54.360
So, zero times this
vector added into layer

00:02:54.360 --> 00:02:58.920
one doesn't change layer one
from what it was before.

00:02:58.920 --> 00:03:03.350
So that, to me, is like the biggest
source of inefficiency in this network.

00:03:03.350 --> 00:03:06.880
To kind of show you,
computationally, and

00:03:06.880 --> 00:03:10.370
sort of prove to you that this
is the case, check this out.

00:03:10.370 --> 00:03:13.030
So we have kind of a fake layer
0 that only has 10 values,

00:03:13.030 --> 00:03:16.240
we're going to picture it here.

00:03:16.240 --> 00:03:20.272
And then we're going to say,
okay, layer zero.

00:03:20.272 --> 00:03:27.585
Layer 0, 4=1, layer, kind of pretend
that we put a few words in here.

00:03:27.585 --> 00:03:29.278
[BLANK_AUDIO]

00:03:29.278 --> 00:03:35.220
Now we're looking at layer zero again,
it looks like that, right?

00:03:35.220 --> 00:03:39.792
So, weights 01, we're going to say
this is just a random write matrix.

00:03:39.792 --> 00:03:42.840
[BLANK_AUDIO]

00:03:42.840 --> 00:03:47.454
And then we're going to say,
okay, layer0.weights01.

00:03:47.454 --> 00:03:50.120
Okay, so that's the output.

00:03:50.120 --> 00:03:55.640
Now, what if instead we only summed
these vectors in here, right?

00:03:55.640 --> 00:03:58.450
So we just said, okay,
1 times this goes in here.

00:03:58.450 --> 00:04:01.952
So if we have these two indices,

00:04:01.952 --> 00:04:06.896
4, 9, we have to have a new layer,
right?

00:04:06.896 --> 00:04:10.530
So layer1 equals np.zeros, so
it's empty, and it's got 5 values.

00:04:10.530 --> 00:04:15.234
And we're going to say,

00:04:15.234 --> 00:04:19.490
for index in indices,

00:04:19.490 --> 00:04:25.994
weights0 1, index, layer 1.

00:04:25.994 --> 00:04:29.291
[BLANK_AUDIO]

00:04:29.291 --> 00:04:34.124
+= 1 *, because one is

00:04:34.124 --> 00:04:39.870
going in the data, layer one.

00:04:39.870 --> 00:04:40.160
Boom.

00:04:40.160 --> 00:04:42.570
Exactly the same values,
look at that and

00:04:42.570 --> 00:04:46.930
the cool thing here is we only actually
worked with part of this matrix.

00:04:46.930 --> 00:04:51.480
So if this, you know two words
out of 70,000 words then we

00:04:53.420 --> 00:04:55.980
just saved not having to,

00:04:55.980 --> 00:05:02.150
you know perform this operation in
this sum with the 69,000 other words.

00:05:02.150 --> 00:05:03.810
That should be a pretty great savings.

00:05:03.810 --> 00:05:07.400
Now, we'll see how much it actually
works out to in the end, but

00:05:07.400 --> 00:05:09.100
that should be really positive.

00:05:09.100 --> 00:05:11.970
I'll be curious to see how
that kind of works out.

00:05:13.210 --> 00:05:17.770
Now let's take a look at the neural net
again, look for some more efficiency.

00:05:17.770 --> 00:05:22.610
Now the other thing that's inefficient
is one times anything is just itself so

00:05:22.610 --> 00:05:25.280
this whole one times
thing is kind of a waste.

00:05:25.280 --> 00:05:30.150
So what if instead we change
this to just be a sum?

00:05:30.150 --> 00:05:33.080
One times,
we can just eliminate that, right?

00:05:33.080 --> 00:05:35.020
So [INAUDIBLE], pair one,
still the same thing.

00:05:35.020 --> 00:05:35.430
Awesome.

00:05:35.430 --> 00:05:40.040
So we got rid of this multiplication, we
got rid of doing these all together, and

00:05:40.040 --> 00:05:43.090
we're still getting
the same hidden state.

00:05:43.090 --> 00:05:48.010
That we were getting when we did this
full dot practice, this full matrix or

00:05:48.010 --> 00:05:49.900
vector matrix multiplication.

00:05:49.900 --> 00:05:51.310
I'm really liking this.

00:05:51.310 --> 00:05:52.910
I think this is a ton to build upon.

00:05:52.910 --> 00:05:54.110
And most of the weights are over here,
right?

00:05:54.110 --> 00:05:56.300
There's only four weights that go
from the hidden to the output.

00:05:56.300 --> 00:05:57.610
Well in our case it's hidden layer size.

00:05:58.720 --> 00:06:04.900
I think we have a bigger layer, but
most of the computation is here.

00:06:04.900 --> 00:06:09.210
74,000 by whatever hidden layer size,
this is the beefy part of training and

00:06:09.210 --> 00:06:10.488
writing our neural net.

00:06:10.488 --> 00:06:14.078
So that brings us to
kind of the next project.

00:06:14.078 --> 00:06:21.490
So project five is about installing this
into the neural network from before.

00:06:21.490 --> 00:06:27.160
So I'm going to go ahead and let you
take a stab at that and then we're

00:06:27.160 --> 00:06:31.048
going to come back and talk about how
we can do that in a neural net before.

00:06:31.048 --> 00:06:32.590
All right, best of luck.

