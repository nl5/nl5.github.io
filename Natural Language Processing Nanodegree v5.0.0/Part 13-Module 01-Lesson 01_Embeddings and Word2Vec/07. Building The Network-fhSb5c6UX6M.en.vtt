WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.500
Hello everyone. Welcome back.

00:00:02.500 --> 00:00:07.040
So now we're going to actually start building the network.

00:00:07.040 --> 00:00:11.500
So here we can see the general structure of the network that we're going to build.

00:00:11.500 --> 00:00:13.820
So we have our inputs, and as I said before this,

00:00:13.820 --> 00:00:16.844
we're not actually going to be doing this you know one hot encoding,

00:00:16.844 --> 00:00:18.675
we're going to be passing in integers,

00:00:18.675 --> 00:00:22.510
but so we're going to pass them to this hidden layer which is our embedding layer.

00:00:22.510 --> 00:00:25.519
And this is just linear neurons so we're not,

00:00:25.518 --> 00:00:27.799
you know putting the activation on them at all.

00:00:27.800 --> 00:00:29.600
They're just linear. And then,

00:00:29.600 --> 00:00:33.478
this is going to an output softmax layer.

00:00:33.478 --> 00:00:35.345
So again remember that we're,

00:00:35.345 --> 00:00:38.314
we're passing in like some current linear text and then,

00:00:38.314 --> 00:00:39.899
and then target words,

00:00:39.899 --> 00:00:47.170
and so we're going to be trying to predict our target words using this softmax layer.

00:00:47.170 --> 00:00:49.929
Then when we train everything up,

00:00:49.929 --> 00:00:52.280
what's going to happen is that our,

00:00:52.280 --> 00:00:55.195
our hidden layer is going to form these you know

00:00:55.195 --> 00:00:59.265
vector representations of the words that contain some semantic meaning.

00:00:59.265 --> 00:01:00.880
And that's, that's what we're really interested in.

00:01:00.880 --> 00:01:04.540
We're only, we only really care about these these representations,

00:01:04.540 --> 00:01:05.829
that's what we're doing, it's like we're,

00:01:05.828 --> 00:01:07.929
we're changing these words into vectors.

00:01:07.930 --> 00:01:09.969
So what that means is that when we're done training we can

00:01:09.968 --> 00:01:12.180
just get rid of this softmax layer,

00:01:12.180 --> 00:01:14.108
that we don't really like care about it anymore.

00:01:14.108 --> 00:01:15.313
It's just there to,

00:01:15.313 --> 00:01:17.033
to train up the,

00:01:17.034 --> 00:01:20.709
basically like the weight between the inputs and the hidden layer.

00:01:20.709 --> 00:01:25.810
Okay, so the first thing you're going to do here is build the inputs.

00:01:25.810 --> 00:01:31.594
So here, you're going to create the inputs using tensor flow placeholders.

00:01:31.593 --> 00:01:34.899
So we're going to be passing in integers and

00:01:34.900 --> 00:01:39.149
so then you need to set the data type to tf.int32.

00:01:39.149 --> 00:01:43.739
And then the batches that we're passing in are going to have random sizes so

00:01:43.739 --> 00:01:48.450
you'll need to leave the batch size arbitrary,

00:01:48.450 --> 00:01:50.599
so you can just set that to none.

00:01:50.599 --> 00:01:54.500
And then tensor flow will figure it out when you pass in the batches.

00:01:54.500 --> 00:01:59.655
And then to make things work later on with some of the code, the labels,

00:01:59.655 --> 00:02:02.254
the targets, this placeholder needs to have

00:02:02.254 --> 00:02:05.799
a second dimension that you either set to none or one.

00:02:05.799 --> 00:02:09.780
Okay, so that is the input nodes.

00:02:09.780 --> 00:02:14.400
So down here we're going to look at our embedding layer.

00:02:14.400 --> 00:02:17.754
So again here you're going to create your embedding layer.

00:02:17.753 --> 00:02:21.823
And so the first thing you do is create the embedding matrix.

00:02:21.824 --> 00:02:23.835
So this is going to have,

00:02:23.835 --> 00:02:26.955
you know the size of the number of words in your vocabulary,

00:02:26.955 --> 00:02:30.909
by your desired embedding dimensions.

00:02:30.908 --> 00:02:33.495
So for example if you have 10000 words and you have

00:02:33.495 --> 00:02:39.908
300 hidden units which is your embedding dimension then the matrix will be 10000 by 300.

00:02:39.908 --> 00:02:43.989
So then, here you're going to be setting the number of

00:02:43.990 --> 00:02:47.080
embedding features and the embedding dimension and then

00:02:47.080 --> 00:02:50.395
create the embedding weight matrix here.

00:02:50.395 --> 00:02:52.925
So remember that you do this with tf.variable.

00:02:52.925 --> 00:02:58.485
Okay, now to do the actual look up, we use tf.nn.embedding_lookup.

00:02:58.485 --> 00:03:04.599
So you can read the documentation here but basically you can pass

00:03:04.598 --> 00:03:12.110
in the the weight matrix and then you pass in some integer and then it does the look up,

00:03:12.110 --> 00:03:15.818
or you can pass in a tensor integers and it's going to do the look up on

00:03:15.818 --> 00:03:21.318
all those tensors and then you get your hidden layer values.

00:03:21.318 --> 00:03:23.818
So I'll let you work on this.

00:03:23.818 --> 00:03:29.649
And so here you'll be doing like creating the inputs and creating the embedding layer.

00:03:29.650 --> 00:03:32.408
So if you need help with this check out my solution,

00:03:32.408 --> 00:03:35.188
and I'll have a video with my solution next.

