WEBVTT
Kind: captions
Language: zh-CN

00:00:00.430 --> 00:00:01.080
好的 欢迎回来

00:00:01.080 --> 00:00:03.640
现在 我们来到了项目 6 我们通过对词汇表的策略性调整

00:00:03.640 --> 00:00:05.450
来减少噪音

00:00:05.450 --> 00:00:09.320
我们已经完成的是 利用之前使用的这些指标

00:00:09.320 --> 00:00:13.000
判断我们的想法是否有用

00:00:13.000 --> 00:00:17.940
接下来 我们将用它剔除一些噪音

00:00:17.940 --> 00:00:20.859
以使神经网络更好地识别信号

00:00:20.859 --> 00:00:24.649
在这里 大部分操作都在预处理环节完成

00:00:24.649 --> 00:00:24.879
对吧？

00:00:24.879 --> 00:00:29.179
我们之前已经更新了训练

00:00:29.179 --> 00:00:32.460
这里 我们将简化词汇表 将它变成这个 vocab 对象

00:00:32.460 --> 00:00:35.820
它之后将变成这个 word2index 对象 用于创建索引

00:00:35.820 --> 00:00:41.189
我们想以特定的方式来简化词汇表 根据不同的界限

00:00:41.189 --> 00:00:42.210
和阈值

00:00:42.210 --> 00:00:46.677
所以 我创建了一个 polarity_cutoff 分界值和一个 min_count 分界值

00:00:46.677 --> 00:00:53.299
min_count 分界值的作用是：它规定 某个单词如要进入

00:00:53.299 --> 00:00:58.094
review_vocab 词汇表 也就是新创建的这个词汇表

00:00:58.094 --> 00:01:01.344
它必须超过最低出现次数

00:01:01.344 --> 00:01:05.355
这样 根据预处理步骤 如果该词没能进入 vocab

00:01:05.355 --> 00:01:08.995
它就不会被训练

00:01:08.995 --> 00:01:12.750
所以 如果一个词不在 vocab 里 它也不在 words2index.keys 中

00:01:12.750 --> 00:01:15.870
就不会被添加到这个索引列表中 也不会被训练

00:01:15.870 --> 00:01:17.189
就直接被忽略了

00:01:17.189 --> 00:01:18.390
这是第一点

00:01:18.390 --> 00:01:20.500
第二点 就是极性分界值

00:01:20.500 --> 00:01:24.150
它规定这个 pos_neg_ratio 必须大于或等于

00:01:24.150 --> 00:01:28.950
极性分界值 或者小于或等于负的极性分界值

00:01:28.950 --> 00:01:31.780
因为如果你记得的话 这个值以 1 为中心

00:01:31.780 --> 00:01:36.900
所以我们说一个词必须小于某个负比率或

00:01:36.900 --> 00:01:39.430
大于某个正比率 对吧？

00:01:39.430 --> 00:01:45.080
所以它排除了中间这些非常高但不相干的部分

00:01:46.709 --> 00:01:47.819
这就是我们的想法

00:01:47.819 --> 00:01:52.669
另一件事是 我要求这个 pos_neg_ratio 必须达到

00:01:52.670 --> 00:01:54.859
一定的最小值 才能起作用

00:01:55.900 --> 00:02:00.740
我这样做的原因是 有时候 你会遇到频率非常低的词

00:02:00.739 --> 00:02:08.400
它们可能具有很小或很大的相关性

00:02:08.400 --> 00:02:10.150
但它们只出现一次或两次

00:02:10.150 --> 00:02:14.030
所以 这个指标规则非常简单 是个简单的分界值
39
00:02:14,030 --&gt; 00:02:15,509
不可能因为它出现了一次

00:02:15.509 --> 00:02:18.310
就判定它是完全正相关的 对吧？

00:02:18.310 --> 00:02:21.310
在这种情况下 我发现这是一个恰当的数字

00:02:22.400 --> 00:02:25.064
所以这两个是阈值

00:02:25.064 --> 00:02:26.889
方式没有对错之分

00:02:26.889 --> 00:02:29.229
再次说明 我们的目标是减少噪音

00:02:29.229 --> 00:02:31.239
这对于每个数据集来说都相不同 对吧？

00:02:31.240 --> 00:02:32.657
当你遇到一个新的数据集

00:02:32.657 --> 00:02:35.287
要创建神经网络时 要设计神经网络

00:02:35.287 --> 00:02:37.300
的架构 来成功反映数据集

00:02:37.300 --> 00:02:40.150
这只是你在解决某种特定问题时

00:02:40.150 --> 00:02:40.828
所采用的一种方法

00:02:40.828 --> 00:02:45.798
也就是说 这些并非一成不变的硬性规定 而只是一种技巧

00:02:45.798 --> 00:02:50.686
让你的神经网络能更好地捕捉到新的

00:02:50.686 --> 00:02:52.840
和有趣的模式

00:02:52.840 --> 00:02:56.280
那么 所有这些都是一样的 没有任何变化

00:02:56.280 --> 00:02:57.747
一样的测试 一样的运行

00:02:57.747 --> 00:02:59.570
只是词汇表变小了

00:02:59.569 --> 00:03:02.908
那么 我们来试试看 将 min_count 设为 20

00:03:02.908 --> 00:03:06.688
polarity_cutoff 为 0.05 这是个非常小的极性分界值

00:03:06.688 --> 00:03:10.120
不会剔除太多词汇

00:03:10.120 --> 00:03:13.491
后面 我们可以不断提高它

00:03:13.491 --> 00:03:18.092
我们马上就能看到结果

00:03:18.092 --> 00:03:22.273
现在 它就在计算所有的统计数据

00:03:22.272 --> 00:03:26.769
它计算完 就会开始训练了

00:03:26.770 --> 00:03:29.645
那么 当你调用构造函数时

00:03:29.645 --> 00:03:34.467
调用 pre-process_data 然后预处理的数据就会通过所有这些步骤

00:03:34.467 --> 00:03:39.020
来帮助识别出哪些是我们要训练的信号

00:03:39.020 --> 00:03:39.466
好的

00:03:39.466 --> 00:03:40.564
非常好

00:03:42.735 --> 00:03:46.650
和之前相比 这个速度有所增加

00:03:46.650 --> 00:03:50.849
不是很多 只有一点点 因为我们减少了词汇量

00:03:50.849 --> 00:03:54.840
看看我们的测试数据 85.9

00:03:54.840 --> 00:03:58.979
提高得不是很多 但是一个小小的、重要的进步

00:03:58.979 --> 00:04:03.019
你可以自己尝试 我花的时间不是很长

00:04:03.020 --> 00:04:06.550
你可以继续调整这个 min_count 和 polarity_cutoff

00:04:06.550 --> 00:04:09.090
找到你的最佳值

00:04:09.090 --> 00:04:14.115
我想在这里展示的另一点 就是将它调大

00:04:14.115 --> 00:04:16.300
会怎样？

00:04:16.300 --> 00:04:20.939
比如说 让我想想

00:04:20.939 --> 00:04:22.439
我们将 min_count 保持不变

00:04:22.439 --> 00:04:23.779
将这个设为 0.8

00:04:23.779 --> 00:04:29.919
我们将中间的大部分剔除掉

00:04:29.920 --> 00:04:32.000
将它卸掉一大块

00:04:32.000 --> 00:04:36.350
0.8 基本上是从这到这 然后

00:04:36.350 --> 00:04:37.330
你知道吗

00:04:37.329 --> 00:04:39.669
这些东西有一点点相关性

00:04:39.670 --> 00:04:44.569
但我觉得有点模棱两可 那么将它们完全剔除掉吧

00:04:44.569 --> 00:04:47.779
然后 我们要训练的是这些

00:04:47.779 --> 00:04:53.159
这样 我们可以大幅提高速度

00:04:53.160 --> 00:04:54.900
也就是训练的速度

00:04:56.100 --> 00:05:00.570
但对测试准确率稍稍有点影响

00:05:00.569 --> 00:05:05.319
有时你会发现这样做很有益 原因有二

00:05:05.319 --> 00:05:09.050
对于一些问题 你想训练更多的数据

00:05:09.050 --> 00:05:10.750
你想运行得非常快

00:05:10.750 --> 00:05:13.375
来看看它能有多快

00:05:13.375 --> 00:05:17.199
训练中 我的天 快看

00:05:17.199 --> 00:05:19.589
每秒 7,000 条评论

00:05:19.589 --> 00:05:23.929
只用了几秒钟就训练完了所有的数据

00:05:23.930 --> 00:05:28.014
来看看测试集中的结果

00:05:28.014 --> 00:05:28.819
82.2 准确率降低了 3%

00:05:28.819 --> 00:05:35.112
不算糟 我们的速度提高了 7 倍

00:05:35.112 --> 00:05:40.560
大约 7 倍 相对于准确率只下降了 3%

00:05:40.560 --> 00:05:43.540
你会发现在神经网络中 速度与质量

00:05:43.540 --> 00:05:44.150
一直是一种博弈权衡

00:05:44.149 --> 00:05:47.029
我唯一知道的两者能兼得的场景 就是当你

00:05:47.029 --> 00:05:48.389
降低噪音的时候

00:05:48.389 --> 00:05:51.250
因为降低噪音就会减少要处理的数据量

00:05:51.250 --> 00:05:53.569
从而速度和准确度都会提高

00:05:53.569 --> 00:05:55.980
这就是我喜欢这种方式的原因

00:05:55.980 --> 00:05:58.220
这种方式会有很大的好处

00:05:58.220 --> 00:06:00.090
首先 对于生产系统来说

00:06:00.089 --> 00:06:03.489
当你需要以非常快的速度处理大量数据

00:06:03.490 --> 00:06:06.420
要解决现实生活中实际存在的问题

00:06:06.420 --> 00:06:09.379
实验室里无干扰情况下的

00:06:11.980 --> 00:06:16.430
可能的最高分不见得是最佳答案

00:06:16.430 --> 00:06:19.410
有些时候 我们需要在一天内飞速处理

00:06:19.410 --> 00:06:22.730
上百个医学报告 以拯救尽可能多的生命

00:06:22.730 --> 00:06:25.028
我们需要更快的速度

00:06:25.028 --> 00:06:29.589
在这种情况下 这是你可以去做的

00:06:29.589 --> 00:06:33.079
我们达到了接近 10 倍的速度 或许我们还能

00:06:33.079 --> 00:06:34.680
让它更快

00:06:34.680 --> 00:06:38.329
所以 这样做的好处在于 你剔除的部分越多 比如你说：

00:06:38.329 --> 00:06:39.669
我要让速度更快

00:06:39.670 --> 00:06:43.500
但同时在加快速度的情况下尽可能使质量的损失最小

00:06:43.500 --> 00:06:47.947
所以 我在预测中去除了大多数意义模糊的词

00:06:47.947 --> 00:06:51.069
帮我节省了大量时间

00:06:51.069 --> 00:06:52.310
那它为什么能帮我节省时间呢？

00:06:52.310 --> 00:06:54.780
这是因为我们做的算术更少了

00:06:54.779 --> 00:06:58.251
词越少时 我们在矩阵中做的算术更少

00:06:58.252 --> 00:07:01.096
反向传播也更少

00:07:01.096 --> 00:07:04.800
因为我们选出的评论中的

00:07:04.800 --> 00:07:08.290
单词量更少 我们要训练的词汇也更少

00:07:08.290 --> 00:07:10.230
这就是我们所做的

00:07:10.230 --> 00:07:13.550
幸运的是 速度的提高非常大

00:07:14.810 --> 00:07:20.079
另一个原因 人们想像这样或以其他方式提高速度

00:07:20.079 --> 00:07:25.109
是因为 如果你有很多的

00:07:25.110 --> 00:07:28.830
训练数据 花再多时间都无法训练完时

00:07:28.829 --> 00:07:33.599
你只需让它运行得更快 训练更多数据

00:07:33.600 --> 00:07:36.040
即使你选择更朴素的算法或

00:07:36.040 --> 00:07:39.830
限制很多东西 同时也使准确性很高

00:07:39.829 --> 00:07:43.919
最著名的一个例子 很多人可能都不知道

00:07:45.240 --> 00:07:46.944
是在 Word2Vec 中的运用

00:07:46.944 --> 00:07:51.379
Word2Vec 是对一种人们一直在训练的

00:07:51.379 --> 00:07:54.319
其他语言模型的近似模拟

00:07:54.319 --> 00:07:57.377
但问题是 这些其他语言模型需要花费一月的

00:07:57.377 --> 00:08:00.920
时间来训练几十亿标记 即使如此 权重更新进步也不大

00:08:00.920 --> 00:08:04.632
Word2Vec 显示的是 如果你清除所有其他东西

00:08:04.632 --> 00:08:09.180
跳过一些步骤 就会获得近似的结果 但是速度会快很多

00:08:09.180 --> 00:08:11.120
你可以

00:08:11.120 --> 00:08:15.372
好像是在 32 个核心机上 在 6 小时内训练 80 亿个标记

00:08:15.372 --> 00:08:18.360
它的速度非常快 训练的数据也非常多

00:08:18.360 --> 00:08:22.410
你可以收集非常多的信息 即使这台挖掘机可能会

00:08:22.410 --> 00:08:27.640
挖到土 但它挖的速度如此快

00:08:27.639 --> 00:08:30.250
迭代次数如此多 这些都显得不重要了

00:08:30.250 --> 00:08:34.490
所以很多情况下 我们都可以让它训练得更快

00:08:34.490 --> 00:08:37.690
这样能训练 7 倍的数据量

00:08:37.690 --> 00:08:40.970
使我们得到更高的得分

00:08:40.970 --> 00:08:44.899
我可以向你保证 如果我们有 7 倍的数据

00:08:44.899 --> 00:08:49.731
我们用快 7 倍的速度来训练它

00:08:49.731 --> 00:08:51.475
一定会得到高得多的得分

00:08:51.475 --> 00:08:55.179
因为你覆盖的范围更广了

00:08:55.179 --> 00:08:58.689
而我们损失的准确性很少

00:08:58.690 --> 00:09:02.460
所以如果你需要训练很多数据

00:09:02.460 --> 00:09:07.460
这真的是非常好的解决方案

00:09:07.460 --> 00:09:10.810
所以 希望你这部分有所收获 在下个部分 我们将详细聊聊

00:09:10.809 --> 00:09:14.269
在权重的背后到底发生了什么

00:09:14.269 --> 00:09:17.829
它们如何相互调整 词汇到底发生了什么

00:09:17.830 --> 00:09:20.690
还有一个非常酷的可视化将帮助我们

00:09:20.690 --> 00:09:22.930
更直观地了解神经网络

00:09:22.929 --> 00:09:23.669
到时候见

