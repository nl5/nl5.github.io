WEBVTT
Kind: captions
Language: zh-CN

00:00:00.100 --> 00:00:02.359
在此项目中 我们将创建输入和输出数据

00:00:02.359 --> 00:00:05.903
对于输入数据 我们将统计评论中每个单词的出现次数

00:00:05.902 --> 00:00:08.320
然后 将它们放在一个固定长度的向量里

00:00:08.320 --> 00:00:12.169
向量中的每个位置都对应词汇表中的一个词

00:00:12.169 --> 00:00:14.375
所以 我们要做的第一件事 就是对词汇计数

00:00:14.375 --> 00:00:17.890
大概一共有 74,000 多个单词

00:00:17.890 --> 00:00:19.960
好 现在我们要创建空的向量

00:00:19.960 --> 00:00:23.650
通常 较好的做法是 预分配一个向量

00:00:23.649 --> 00:00:26.289
使其内容全为空 之后使用时再改变它的值

00:00:26.289 --> 00:00:28.989
因为 分配新内存

00:00:28.989 --> 00:00:30.659
对于计算机而言是一件代价高昂的事情

00:00:30.660 --> 00:00:33.487
我们不希望每次需要使用这个向量时

00:00:33.487 --> 00:00:33.886
都要创建一个全新的它

00:00:33.886 --> 00:00:36.987
因此 我们先创建一个空的向量 再新建一个函数

00:00:36.987 --> 00:00:39.019
用于以计值来更新向量

00:00:40.140 --> 00:00:43.859
所以 我们要做的第一件事 就是确定向量中每个单词的位置

00:00:43.859 --> 00:00:46.695
然后创建一个用于研究它的变量

00:00:46.695 --> 00:00:49.759
事实上 单词放在哪个位置并不重要

00:00:49.759 --> 00:00:51.767
horrible 可以放在下面这里 也可以在上面

00:00:51.767 --> 00:00:55.320
只要我们始终选择同样的位置

00:00:55.320 --> 00:00:59.390
因此 我会创建一个词典 可以让我们

00:00:59.390 --> 00:01:03.039
根据单词在词汇表中的位置

00:01:03.039 --> 00:01:03.350
查找到每一个词

00:01:03.350 --> 00:01:05.200
然后 我们创建我们的函数

00:01:05.200 --> 00:01:06.670
这里的 layer 是一个全局变量

00:01:06.670 --> 00:01:08.129
我们要清除掉所有旧的变量值

00:01:08.129 --> 00:01:12.849
然后对评论中的每个单词进行迭代

00:01:12.849 --> 00:01:17.875
对于向量中每个位置所对应的单词

00:01:17.876 --> 00:01:20.125
单词每出现一次 次数加一

00:01:20.125 --> 00:01:24.722
然后试一下第一条评论 就是 review[0] 对吧？

00:01:24.722 --> 00:01:25.640
好像奏效了

00:01:25.640 --> 00:01:29.814
实际上 其中一个单词 可能是我标记的那个空的

00:01:29.813 --> 00:01:31.102
出现了 18 次

00:01:31.102 --> 00:01:32.495
怎么样？

00:01:32.495 --> 00:01:37.096
所以 get_target_for_label 看起来可以用

00:01:37.096 --> 00:01:43.657
label(0) 为正 label(1) 我想是负的

00:01:43.658 --> 00:01:45.114
所以 看起来非常不错

00:01:45.114 --> 00:01:46.469
这个方法会很管用

00:01:46.469 --> 00:01:48.498
这是我们的输入和输出数据集

00:01:48.498 --> 00:01:52.076
希望你的网络也创建了类似的变量

00:01:52.076 --> 00:01:54.935
我们这里做得较好的一点 我认为大家能学到的

00:01:54.935 --> 00:01:56.689
最主要的一点就是高效率 对吧？

00:01:56.689 --> 00:01:59.049
所以 当你在创建这些向量时

00:01:59.049 --> 00:02:01.789
尽量不要为数据分配全新的向量

00:02:01.790 --> 00:02:05.590
第二件我们没有去做的事 就是预生成

00:02:05.590 --> 00:02:06.576
完整的数据集 对吧？

00:02:06.576 --> 00:02:12.439
因为整个矩阵的大小是 74,000 乘以 多少个训练示例来着？

00:02:12.439 --> 00:02:19.335
25,000  那就是 74,000 vocab_size x 25,000

00:02:19.336 --> 00:02:25.220
结果是 [笑] 大约 20 亿整数

00:02:25.219 --> 00:02:28.992
如果都存在你的机器上 数量太大了 但实际上

00:02:28.992 --> 00:02:30.722
我们可以很容易地填充它

00:02:30.722 --> 00:02:33.650
它们大多数的值都是 0 很快就能生成

00:02:33.650 --> 00:02:35.620
因此 这是一个很好的操作方法

00:02:35.620 --> 00:02:39.310
又节省你的笔记本电脑内存 又能创建数据集

00:02:39.310 --> 00:02:41.349
所以 这就是我们的输入和输出数据集

00:02:41.349 --> 00:02:42.819
这些就是你们需要注意的点

00:02:42.819 --> 00:02:44.789
不要一次性分配太多新内存

00:02:44.789 --> 00:02:46.996
也不要每次都创建新的变量

00:02:46.997 --> 00:02:49.604
这些就是我们将要在神经网络中用到的形式

00:02:49.604 --> 00:02:51.764
下一部分 我们将讨论如何将学到的这些内容

00:02:51.764 --> 00:02:52.990
整合到神经网络中

00:02:52.990 --> 00:02:53.480
到时候见

