WEBVTT
Kind: captions
Language: pt-BR

00:00:00.000 --> 00:00:04.234
Olá, sou Andrew Trask,
doutorando na Univ. de Oxford.

00:00:04.267 --> 00:00:07.367
Estudo aprendizagem profunda para
processamento de linguagem natural.

00:00:07.400 --> 00:00:10.567
Aprendizagem profunda
é um conjunto de ferramentas

00:00:10.601 --> 00:00:13.767
que permite que peguem o que sabem
e prevejam o que querem saber,

00:00:13.801 --> 00:00:15.067
usando redes neurais.

00:00:15.100 --> 00:00:18.033
Processamento de linguagem natural
é o estudo da linguagem humana,

00:00:18.067 --> 00:00:19.601
usando ferramentas
como aprendizagem de máquina

00:00:19.634 --> 00:00:21.701
e, neste caso,
aprendizagem profunda.

00:00:21.734 --> 00:00:24.467
Nesta aula, falaremos
sobre classificação de sentimento,

00:00:24.501 --> 00:00:27.834
ou a classificação de
se um texto gerado por humanos

00:00:27.868 --> 00:00:29.567
é positiva ou negativa.

00:00:29.601 --> 00:00:33.100
O que sabemos é um texto
gerado por humanos,

00:00:33.133 --> 00:00:37.501
e queremos saber um desses rótulos,
positivo ou negativo.

00:00:37.534 --> 00:00:41.701
Esta aula será, na verdade,
sobre enquadrar um problema,

00:00:41.734 --> 00:00:44.534
para a rede obter sucesso
descobrindo correlações

00:00:44.567 --> 00:00:46.901
entre seus dados
de entrada e de saída.

00:00:46.934 --> 00:00:50.100
Classificação de sentimento
é muito bom para isso.

00:00:50.133 --> 00:00:53.667
Redes neurais não aceitam
naturalmente entrada textuais,

00:00:53.701 --> 00:00:55.067
elas aceitam números.

00:00:55.100 --> 00:00:59.767
Devemos, então, transformar
nossos dados de entrada textuais

00:00:59.801 --> 00:01:04.801
em forma numérica, para a rede
descobrir correlações.

00:01:04.834 --> 00:01:08.667
Nossa meta será podermos ver
como mudar a forma como fazemos

00:01:08.701 --> 00:01:12.567
e colocar o problema de forma
que a rede descubra correlações

00:01:12.601 --> 00:01:14.767
com rapidez e facilidade.

00:01:14.801 --> 00:01:19.701
Suponho que já saibam
o que viram no curso do Udacity.

00:01:19.734 --> 00:01:21.834
Redes neurais, propagação
progressiva e retropropagação,

00:01:21.868 --> 00:01:24.100
descida do gradiente,
erro médio quadrado,

00:01:24.133 --> 00:01:27.667
divisão entre treinamento e testes,
tudo isso deve ser familiar.

00:01:27.701 --> 00:01:30.934
Se não
se sentirem preparados,

00:01:30.968 --> 00:01:35.300
voltem e vejam
aulas anteriores.

00:01:35.334 --> 00:01:39.367
Se preferirem de outra forma,
com textos,

00:01:39.400 --> 00:01:43.834
busquem este guia, o meu livro
"Grokking Deep Learning".

00:01:43.868 --> 00:01:47.968
Meu editor é generoso
e oferece 40% de desconto

00:01:48.000 --> 00:01:49.901
aos alunos do Udacity.

00:01:49.934 --> 00:01:54.567
Eu também estou disponível
para suas perguntas.

00:01:55.367 --> 00:01:59.400
A aula será dividida
em 6 projetos.

00:01:59.434 --> 00:02:02.467
O primeiro começará com
a curadoria dos dados,

00:02:02.501 --> 00:02:07.767
e com uma teoria que prevê onde
achamos que a correlação existe.

00:02:07.801 --> 00:02:10.634
Depois,
validaremos esta teoria,

00:02:10.667 --> 00:02:15.501
e a transformaremos
nos seus dados de entrada.

00:02:15.534 --> 00:02:18.667
Vamos iterar várias vezes,
e tentaremos aumentar

00:02:18.701 --> 00:02:21.200
a correlação que
nossa rede encontra

00:02:21.234 --> 00:02:23.400
num período de tempo menor.

00:02:23.434 --> 00:02:27.934
No fim, vamos quebrar as redes
neurais e ver como elas entendem

00:02:27.968 --> 00:02:30.534
o que acontece
na minha retropropagação

00:02:30.567 --> 00:02:33.901
e tentar resolver o sentimento
dentro da rede neural.

00:02:33.934 --> 00:02:36.300
Sem mais, vamos começar.

