WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.730
大家好 我是 Andrew Trask

00:00:02.730 --> 00:00:04.049
我是牛津大学的博士生

00:00:04.049 --> 00:00:05.910
在研究深度学习中的

00:00:05.910 --> 00:00:08.010
自然语言处理

00:00:08.010 --> 00:00:10.019
大家所要学的深度学习

00:00:10.019 --> 00:00:11.580
就是一套工具 让我们根据已知的数据

00:00:11.580 --> 00:00:13.469
通过使用神经网络

00:00:13.469 --> 00:00:15.809
预测我们想要知道的事物

00:00:15.809 --> 00:00:17.430
自然语言处理就是对人类语言的研究

00:00:17.430 --> 00:00:19.320
使用一系列工具 如机器学习

00:00:19.320 --> 00:00:20.789
在我们的课程中 也就是深度学习

00:00:20.789 --> 00:00:22.800
本课程中 我们将学习

00:00:22.800 --> 00:00:24.359
情感分类

00:00:24.359 --> 00:00:26.099
这种分类能够

00:00:26.099 --> 00:00:28.019
判断人类生成的一段文字

00:00:28.019 --> 00:00:30.179
是正面或是负面的 所以 在此情况下

00:00:30.179 --> 00:00:32.098
我们已知一段人类生成的语言文本

00:00:32.098 --> 00:00:34.559
想要预测的是

00:00:34.559 --> 00:00:36.149
这些正面或负面标签之一

00:00:36.149 --> 00:00:39.690
本课程真正要做的

00:00:39.690 --> 00:00:41.790
就是解构问题

00:00:41.790 --> 00:00:43.620
这样 我们的神经网络才能

00:00:43.620 --> 00:00:45.059
成功发现

00:00:45.059 --> 00:00:48.000
输入和输出数据之间的相关性

00:00:48.000 --> 00:00:49.649
情感分类非常适合做这件事

00:00:49.649 --> 00:00:52.829
因为神经网络不能直接接受

00:00:52.829 --> 00:00:55.530
文本输入 它只接受数字输入

00:00:55.530 --> 00:00:58.410
所以 我们要做的

00:00:58.410 --> 00:01:01.230
就是将文本输入数据转换成数字形式

00:01:01.230 --> 00:01:02.789
以一种神经网络可以

00:01:02.789 --> 00:01:05.099
轻易找到相关性的方式来呈现

00:01:05.099 --> 00:01:06.930
我们的目标就是

00:01:06.930 --> 00:01:08.880
看看我们如何改变方式来完成

00:01:08.880 --> 00:01:10.439
如何构造我们的问题

00:01:10.439 --> 00:01:11.580
可以让神经元能够

00:01:11.580 --> 00:01:13.380
快速轻易地找到相关性

00:01:13.380 --> 00:01:14.430
快速轻易地找到相关性

00:01:14.430 --> 00:01:16.320
现在 我假设大家已经

00:01:16.320 --> 00:01:18.360
具备了一些基础知识

00:01:18.360 --> 00:01:20.939
上过优达学城的课程 包括神经网络

00:01:20.939 --> 00:01:23.220
正向和反向传播 梯度下降

00:01:23.220 --> 00:01:24.780
平均方差 和训练集/测试集划分

00:01:24.780 --> 00:01:26.130
这些你都应该相当熟悉了

00:01:26.130 --> 00:01:27.840
如果你觉得有些不熟

00:01:27.840 --> 00:01:30.090
觉得准备不充分

00:01:30.090 --> 00:01:31.380
我鼓励大家

00:01:31.380 --> 00:01:32.670
回去看看之前几周的课程

00:01:32.670 --> 00:01:34.650
回去看看之前几周的课程

00:01:34.650 --> 00:01:37.409
如果你想换一种方式学

00:01:37.409 --> 00:01:39.509
可能是书面的方式

00:01:39.509 --> 00:01:40.920
你可以看看我的书

00:01:40.920 --> 00:01:42.720
Grokking Deep Learning

00:01:42.720 --> 00:01:45.180
这本书是我写的 我的出版商万宁公司

00:01:45.180 --> 00:01:46.740
非常慷慨

00:01:46.740 --> 00:01:48.570
为所有优达学城的学员

00:01:48.570 --> 00:01:50.610
提供了六折的优惠码

00:01:50.610 --> 00:01:52.380
我也会在 Slack 频道上与你们互动

00:01:52.380 --> 00:01:55.110
回答问题

00:01:55.110 --> 00:01:57.630
本教程将由

00:01:57.630 --> 00:01:59.550
六个项目组成

00:01:59.550 --> 00:02:00.900
第一个项目

00:02:00.900 --> 00:02:03.690
你将整理数据

00:02:03.690 --> 00:02:05.160
并构想出一个预测理论

00:02:05.160 --> 00:02:06.840
考虑数据中的相关性究竟是如何的

00:02:06.840 --> 00:02:08.669
接下来

00:02:08.669 --> 00:02:10.649
我们将验证这一理论

00:02:10.649 --> 00:02:13.080
当验证完理论

00:02:13.080 --> 00:02:14.700
我们将理论运用到输入和输出数据中

00:02:14.700 --> 00:02:16.110
我们将理论运用到输入和输出数据中

00:02:16.110 --> 00:02:18.090
然后重复迭代几次

00:02:18.090 --> 00:02:19.530
训练神经网络 使它能够尽可能

00:02:19.530 --> 00:02:21.480
在更短的时间内找到更高的相关性

00:02:21.480 --> 00:02:24.090
在更短的时间内找到更高的相关性

00:02:24.090 --> 00:02:25.230
最后 我们会把神经网络解构拆分开来

00:02:25.230 --> 00:02:26.700
研究其中的权重系数矩阵

00:02:26.700 --> 00:02:29.160
真正理解我们的反向传播过程

00:02:29.160 --> 00:02:31.530
尝试完成我们布置的有关神经网络中权重系数的作业

00:02:31.530 --> 00:02:34.530
尝试完成我们布置的有关神经网络中权重系数的作业

00:02:34.530 --> 00:02:40.880
废话少说 就让我们开始吧

