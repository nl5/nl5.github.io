WEBVTT
Kind: captions
Language: zh-CN

00:00:00.340 --> 00:00:00.929
欢迎回来

00:00:00.930 --> 00:00:04.169
在这部分 我们将更深入讲解一下权重

00:00:04.169 --> 00:00:08.189
更注重理论 而不太针对某个具体项目

00:00:08.189 --> 00:00:11.710
主要是要理解 这些权重起了什么作用

00:00:11.710 --> 00:00:15.839
在我们思考神经网络训练时 应该考虑哪些方面

00:00:15.839 --> 00:00:19.079
帮助我们来进行调试 找到更有效的信号

00:00:19.079 --> 00:00:23.659
更好地消除噪音 搭建更好的网络框架

00:00:23.660 --> 00:00:25.109
来思考这些问题

00:00:25.109 --> 00:00:30.149
已知 我们用这个 1 代表 excellent

00:00:30.149 --> 00:00:32.310
将这些值相加 得到 layer_1 来进行预测

00:00:32.310 --> 00:00:36.020
这里 有一个有趣的现象

00:00:36.020 --> 00:00:38.520
特别是当存在线性层时

00:00:38.520 --> 00:00:39.920
我之前的课程很简短地提到过

00:00:41.140 --> 00:00:44.109
当时我说 这里需要一个线性层

00:00:44.109 --> 00:00:46.799
后面课程会再详谈 现在就要详细讲讲这一点了

00:00:46.799 --> 00:00:50.289
当我们有这样的线性层时 它在进行预测

00:00:51.429 --> 00:00:55.869
真正发生的是 这 4 个权重（如这个例子中）

00:00:55.869 --> 00:00:59.969
或者你自己定有多少个权重 作为特征检测器

00:01:01.070 --> 00:01:05.319
它们所做的就是 检测神经元的一个特定状态

00:01:05.319 --> 00:01:09.790
或者这个神经元 这个神经元

00:01:09.790 --> 00:01:11.710
如果是一个很大的负数

00:01:12.750 --> 00:01:16.370
这个应该是大的负数

00:01:16.370 --> 00:01:19.510
但如果这是零 或正数

00:01:19.510 --> 00:01:23.570
就会产生相反的效果 对吧？

00:01:23.569 --> 00:01:26.479
所以 这里在寻找一组特定的值 例如 0.5、0.9、

00:01:26.480 --> 00:01:28.430
1 和 -2 对吧？

00:01:28.430 --> 00:01:33.040
所以 某些隐藏层会使其输出一个较高的预测值

00:01:33.040 --> 00:01:36.540
某些隐藏层会输出一个较低的预测值

00:01:36.540 --> 00:01:40.310
这只是一个加权求和 很简单 对吧？

00:01:40.310 --> 00:01:42.510
如果所有值都为正

00:01:42.510 --> 00:01:46.060
这些权重也都为正 结果就很大

00:01:46.060 --> 00:01:49.344
但如果这个为正 这个为负 这个为正

00:01:49.344 --> 00:01:50.155
这个为负

00:01:50.155 --> 00:01:51.605
这个为正 这个为负

00:01:51.605 --> 00:01:52.814
这个为正 这个为负

00:01:52.814 --> 00:01:56.444
就会得到一个很小的数字 对吧？

00:01:56.444 --> 00:02:00.454
所以 当它们具有相同的正负性

00:02:00.454 --> 00:02:04.084
并且都有很大的值时 最终的预测值就会很大

00:02:04.084 --> 00:02:06.239
那么这些究竟意味着什么呢？

00:02:06.239 --> 00:02:08.657
horrible 和 terrible

00:02:08.657 --> 00:02:13.057
当我们思考数据集里的常见现象时

00:02:13.057 --> 00:02:17.280
这些词都与“负面”相关联的 对吧？

00:02:17.280 --> 00:02:20.370
而 excellent 和 fantastic（这是另一个词）

00:02:20.370 --> 00:02:22.668
这些词往往会预测出这一结果（positive）

00:02:22.669 --> 00:02:28.169
它们都试着以某种方式与这个向量关联

00:02:28.169 --> 00:02:32.750
有趣的是 如果这些词被训练了

00:02:32.750 --> 00:02:34.680
它们只是矩阵中的行 对吧？

00:02:34.680 --> 00:02:40.780
同样的 它们被训练成能够通过操纵这些值

00:02:40.780 --> 00:02:41.689
得到一个较高或较低的预测值

00:02:41.689 --> 00:02:45.060
那么 这一训练过程中 究竟学到了什么呢？

00:02:45.060 --> 00:02:46.509
我们对梯度做反向传播

00:02:46.509 --> 00:02:50.769
更新这些值 使它们都能体现这一相同的预测结果

00:02:50.770 --> 00:02:55.210
horrible 和 terrible 都会以同样的方式影响这个值

00:02:55.210 --> 00:02:58.810
那么当我们更新权重 使它们这么做时 发生了什么？

00:02:58.810 --> 00:03:01.370
它们的权重变得越来越接近 为什么？

00:03:01.370 --> 00:03:04.330
因为它们都有相同的目标 对吧？

00:03:04.330 --> 00:03:09.270
比如 如果 horrible 给出正面的预测结果 这里会说不不不

00:03:09.270 --> 00:03:13.490
horrible 向量先生 请更新您的权重

00:03:13.490 --> 00:03:18.140
请更像这个一些 因为这才会让我给出负面的结果预测

00:03:18.139 --> 00:03:21.479
这就是传播的本质所在 它就是要

00:03:21.479 --> 00:03:26.019
让节点告诉各权重值 如何给出更好、更准确的预测

00:03:26.020 --> 00:03:29.890
因为 horrible 和 terrible 传递的是同样的信号

00:03:29.889 --> 00:03:31.669
也就是负面 对吧？

00:03:31.669 --> 00:03:36.639
这两个值最终会集合成一块 这还挺酷的

00:03:36.639 --> 00:03:37.379
那么 看看这个

00:03:37.379 --> 00:03:39.379
我们在探索

00:03:39.379 --> 00:03:44.189
看看这些单词的向量有多相似

00:03:44.189 --> 00:03:48.300
现在我们可以搜索说：展示与 excellent

00:03:48.300 --> 00:03:51.530
最相似的向量词

00:03:51.530 --> 00:03:53.469
我只是用了一个简单的点乘法

00:03:53.469 --> 00:03:56.819
Excellent、perfect、amazing、today、wonderful、fun、great、best

00:03:56.819 --> 00:03:59.129
这些都是训练过的神经网络上的词

00:03:59.129 --> 00:04:00.609
看一看

00:04:00.610 --> 00:04:05.690
经过训练之后 由于这些词 excellent、perfect、amazing 

00:04:05.689 --> 00:04:11.259
都会对最终的输出产生相似的影响

00:04:11.259 --> 00:04:12.759
它们的权重都很相似

00:04:14.909 --> 00:04:21.886
相反 如果我们搜索表示负面的词 比如 terrible

00:04:24.083 --> 00:04:28.901
会出现 Worst、awful、waste、poor、terrible、dull、poorly、disappointment、fail

00:04:28.901 --> 00:04:31.280
disappointing、boring、unfortunate

00:04:31.279 --> 00:04:32.479
非常非常多

00:04:32.480 --> 00:04:35.540
这实际上是比原来的计数法更好用的一个过滤器

00:04:35.540 --> 00:04:37.650
之前的计数法中会出现一些奇怪的名字

00:04:37.649 --> 00:04:41.001
还有一些噪音 但这个效果非常棒

00:04:41.002 --> 00:04:45.980
很显然 网络已经明白了

00:04:45.980 --> 00:04:48.000
这些词是相互关联的 对吧？

00:04:48.000 --> 00:04:52.019
这些词都会对输出数据产生相同的影响

00:04:52.019 --> 00:04:52.490
对吧？

00:04:52.490 --> 00:04:55.379
不过要明确一点 我们也不能夸大它

00:04:55.379 --> 00:04:57.959
网络并不知道 这些词普遍具有相同的含义

00:04:57.959 --> 00:05:01.589
它只知道 在这个输出神经元的语境下 这些词具有

00:05:01.589 --> 00:05:05.019
相同的意义 因为这里只有一个输出神经元

00:05:05.019 --> 00:05:07.329
它们的存在只是能产生与这个神经元相同的影响

00:05:07.329 --> 00:05:12.649
我们可能有一堆不同的神经元 将词汇推向一边或

00:05:12.649 --> 00:05:14.729
另一边

00:05:14.730 --> 00:05:18.210
或以很多不同的方式产生不同 问题可以变得非常复杂

00:05:18.209 --> 00:05:21.789
但基本来说 它是按照情感属性将单词进行分组 对吧？

00:05:23.579 --> 00:05:24.300
它说 

00:05:24.300 --> 00:05:27.069
所有表示负面的词汇都有非常相似的向量

00:05:27.069 --> 00:05:30.620
所有表示正面的词汇也有非常相似的向量

00:05:30.620 --> 00:05:32.439
这是非常强大的直觉

00:05:32.439 --> 00:05:35.660
因为这意味着 当你看到这样一个神经网络 你可以说

00:05:35.660 --> 00:05:36.970
好的 我们要训练它

00:05:36.970 --> 00:05:41.600
你可以将这些词分为一组

00:05:41.600 --> 00:05:46.600
将这些词分为一组 然后看看 背后究竟发生了什么

00:05:46.600 --> 00:05:51.660
而且 你如何可以识别信号与噪音

00:05:51.660 --> 00:05:55.700
因为这就是问题设计的核心目标

00:05:55.699 --> 00:06:00.397
现在 我再给大家来点有意思的

00:06:00.398 --> 00:06:05.527
我喜欢用一个叫作 tf–idf 的工具

00:06:05.526 --> 00:06:10.029
tf–idf 的作用是取一个高维向量

00:06:10.029 --> 00:06:14.269
在我们的例子中 高维只有 4 维

00:06:14.269 --> 00:06:17.218
它将它们分为两个维度

00:06:17.218 --> 00:06:21.870
或你指定的其他维度 然后可以将它们绘制在 X Y 图上

00:06:21.870 --> 00:06:25.924
看它们如何在更高维度图中自然聚集

00:06:25.925 --> 00:06:30.939
很酷的一点就是 我们现在就可以做

00:06:30.939 --> 00:06:33.870
我要做的就是 将它们分为集群

00:06:33.870 --> 00:06:37.829
然后说 那些正负比为正的词

00:06:37.829 --> 00:06:40.500
我要将它们显示为绿色

00:06:40.500 --> 00:06:42.689
那些正负比为负的词 我要将它们显示为黑色

00:06:42.689 --> 00:06:45.560
那么 理论上来说 因为这些向量都非常类似

00:06:45.560 --> 00:06:49.175
它们应该会聚集在一起 图上会显示它们已经聚在一起了

00:06:49.175 --> 00:06:52.444
看看

00:06:52.444 --> 00:06:56.456
虽然中间有一些掺杂在一起

00:06:56.456 --> 00:07:00.079
但神经网络已经清楚地将这些分隔开了

00:07:00.079 --> 00:07:02.519
这里有一条长长的负面词集群

00:07:02.519 --> 00:07:05.539
然后这些都是非常不错的正面词集群

00:07:05.540 --> 00:07:06.069
来看看

00:07:06.069 --> 00:07:07.480
我来试着让它显示标签

00:07:07.480 --> 00:07:10.850
我让标签显示出来了 看起来有点凌乱

00:07:10.850 --> 00:07:13.530
我们来看看

00:07:13.529 --> 00:07:18.529
我放大一点 你可以看得更清楚一些

00:07:18.529 --> 00:07:19.019
哇塞 有意思

00:07:20.529 --> 00:07:23.729
我看到了 unconvincing、dimensional、lousy

00:07:25.350 --> 00:07:27.920
看看这个

00:07:27.920 --> 00:07:34.740
Extraordinary、lovable、provoking、award、touched

00:07:35.959 --> 00:07:37.489
很美观 很棒

00:07:37.490 --> 00:07:38.400
我们可以到处看看

00:07:38.399 --> 00:07:42.775
这个 我们可以称这个为向量空间

00:07:42.776 --> 00:07:45.550
事实上

00:07:45.550 --> 00:07:48.060
这里面还有一些无关的东西 像这些名称

00:07:48.060 --> 00:07:49.780
所以这是一条噪音

00:07:49.779 --> 00:07:52.739
像这里的 Dan

00:07:52.740 --> 00:07:57.189
[笑] 但总体而言 它已经做得很棒了

00:07:57.189 --> 00:08:00.170
按情感属性自动对向量进行聚类 对吧？

00:08:00.170 --> 00:08:03.835
我刚刚说了 它之所以能做这一分类 是因为我用训练数据训练它

00:08:03.834 --> 00:08:04.233
让它这么做

00:08:04.233 --> 00:08:07.939
但它所做的 只是尝试做出准确的预测 

00:08:07.939 --> 00:08:11.644
但是隐含其中的是 为了做出准确的预测 它必须要先将这些词分类

00:08:11.644 --> 00:08:15.908
将类似的向量分到一起 从而当求和时

00:08:15.908 --> 00:08:19.485
所有正面或负面的值相加就会更大

00:08:19.485 --> 00:08:21.225
这就是神经网络所做的

00:08:21.225 --> 00:08:25.580
它表现效果非常炫酷

00:08:25.579 --> 00:08:31.019
总结来说 这就是神经网络的核心所在

00:08:31.019 --> 00:08:32.500
也是问题构建的核心所在

00:08:32.500 --> 00:08:34.620
就是要理解它背后发生的活动

00:08:34.620 --> 00:08:37.928
尽可能减少噪音

00:08:37.928 --> 00:08:39.538
尽可能放大信号

00:08:39.538 --> 00:08:42.379
以找到你想要找的结构

00:08:42.379 --> 00:08:47.090
我们没告诉它 lovable 和

00:08:47.090 --> 00:08:51.290
extraordinary 这两个词在这一语境下是非常相似的

00:08:51.289 --> 00:08:54.149
我们告诉它的信息只有：所有这些评论都是正面的

00:08:54.149 --> 00:08:55.970
所有这些都是负面的 找出它们的规律吧

00:08:55.970 --> 00:09:00.004
然后它就搞清楚了 哪些词是相似的 哪些是不同的

00:09:00.004 --> 00:09:01.664
它通过这种方式做出了准确了预测

00:09:01.664 --> 00:09:05.625
那么 希望你在这部分有所收获

00:09:05.625 --> 00:09:09.485
待会见 享受学习吧

