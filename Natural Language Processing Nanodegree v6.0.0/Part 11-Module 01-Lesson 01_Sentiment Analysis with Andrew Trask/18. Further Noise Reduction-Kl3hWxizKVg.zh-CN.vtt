WEBVTT
Kind: captions
Language: zh-CN

00:00:00.520 --> 00:00:05.490
上一节课 我们大幅提高了

00:00:05.490 --> 00:00:06.150
神经网络的训练速度

00:00:06.150 --> 00:00:07.360
甚至在上一节课

00:00:07.360 --> 00:00:11.669
我们看到了测试数据每秒处理 1,500 个左右的评论

00:00:11.669 --> 00:00:13.789
简直高得吓人

00:00:13.789 --> 00:00:17.140
这一节课 我们将倒回一个章节

00:00:17.140 --> 00:00:20.530
继续尝试减少网络训练中的噪音量

00:00:20.530 --> 00:00:23.920
并且增加信号量

00:00:23.920 --> 00:00:27.620
也就是信噪比 你越能帮助 

00:00:27.620 --> 00:00:31.820
神经网络快速完成简单明了的步骤

00:00:31.820 --> 00:00:34.480
它就越能聚焦在处理那些真正有难度的步骤上

00:00:34.479 --> 00:00:36.789
你的神经网络也就能训练得越好

00:00:36.789 --> 00:00:39.350
这就是我们真正想要继续迭代的

00:00:39.350 --> 00:00:41.649
因此 当我们开始构建解决问题的框架

00:00:41.649 --> 00:00:44.199
使神经网络变得尽可能成功

00:00:44.200 --> 00:00:49.109
我们已经看到 一些细小的改动

00:00:49.109 --> 00:00:52.229
会大幅影响网络的训练速度

00:00:52.229 --> 00:00:55.119
以及网络能在多大程度上识别你真正关心的底层模式

00:00:55.119 --> 00:00:57.809
我们将继续迭代

00:00:57.810 --> 00:00:59.289
所以在这一小节 我们将回过头来

00:00:59.289 --> 00:01:02.759
问问“我们的神经网络到底在做什么？”

00:01:02.759 --> 00:01:04.849
这是我们一直在问的一个问题

00:01:04.849 --> 00:01:06.930
它的背后究竟在发生什么？

00:01:06.930 --> 00:01:11.460
现在 我们将这些向量加到一块 

00:01:11.459 --> 00:01:13.229
来进行预测

00:01:13.230 --> 00:01:20.648
最后 这就是每个单词在评论中出现次数的总和

00:01:20.647 --> 00:01:22.709
有一点很有意思

00:01:22.709 --> 00:01:27.641
之前 在对我们的想法进行一个小小的

00:01:27.641 --> 00:01:29.399
验证时

00:01:29.400 --> 00:01:30.780
我们做了什么呢

00:01:30.780 --> 00:01:33.370
我们创建了这个比率

00:01:34.730 --> 00:01:36.910
来发现那些真正重要的单词

00:01:36.909 --> 00:01:39.939
相关性最高的词 比如 flawless、superbly

00:01:39.939 --> 00:01:44.001
perfection 或 unwatchable、pointless、atrocious、redeeming、laughable

00:01:44.001 --> 00:01:45.949
这不禁使我在想

00:01:45.950 --> 00:01:50.079
如果我们用这些信息

00:01:50.079 --> 00:01:53.787
有倾向性地赋予一些词权重 是否更好？

00:01:53.787 --> 00:01:55.481
就像对神经网络说 看这里

00:01:55.481 --> 00:01:58.640
你要在其他地方找也没关系

00:01:58.640 --> 00:02:02.320
但最好先从这些词开始

00:02:02.319 --> 00:02:06.149
或者我们直接说“金子就在这里”

00:02:06.150 --> 00:02:07.150
开始挖吧

00:02:07.150 --> 00:02:11.951
不是每个词都那么重要 比如像 Boll uwe Seagal

00:02:11.951 --> 00:02:15.475
'Seagal' 这个词太糟了

00:02:15.475 --> 00:02:18.656
'Seagal' 是最糟的词之一

00:02:18.656 --> 00:02:21.685
最负面相关的词之一 不要告诉他哦（注：Seagal是一位美国好莱坞动作明星）

00:02:21.685 --> 00:02:22.479
像这里的 Gandhi

00:02:23.960 --> 00:02:25.170
这可能只是一部关于甘地的电影

00:02:25.169 --> 00:02:27.359
我觉得这好像跟情绪没多大关系

00:02:27.360 --> 00:02:29.060
而 flawless、superbly、perfection

00:02:29.060 --> 00:02:31.479
这些才是我们希望神经网络找到的词

00:02:31.479 --> 00:02:35.009
你可以想象成你在挖金子

00:02:35.009 --> 00:02:37.079
这些词就在金子的富矿区

00:02:37.080 --> 00:02:40.469
虽然也有一些石头和铁和其他东西

00:02:40.469 --> 00:02:42.379
但是 这些词就是我们的目标

00:02:42.379 --> 00:02:46.079
是我们希望神经网络能自然找到的词

00:02:46.080 --> 00:02:49.740
那么 我们如何利用这些统计数据来帮助我们做预测呢？

00:02:49.740 --> 00:02:52.760
它能帮你做些什么？

00:02:52.759 --> 00:02:54.609
比如 我们先将一些词去掉

00:02:54.610 --> 00:02:58.540
它会起作用吗？ 我们将限制词汇量的规模

00:02:58.539 --> 00:03:00.069
首先 我们来研究一下

00:03:00.069 --> 00:03:04.150
看看这些单词比率值的分布情况是怎么样的

00:03:04.150 --> 00:03:07.210
为此 可视化库能派上大用场

00:03:08.349 --> 00:03:09.344
看这里

00:03:09.344 --> 00:03:10.539
这是我们的比率

00:03:10.539 --> 00:03:13.816
0 处在正中间的位置

00:03:13.816 --> 00:03:18.356
频率的分布是这样的

00:03:18.356 --> 00:03:20.478
因此 这个分布是归一化的

00:03:20.479 --> 00:03:26.510
但是 有一大堆词是模棱两可的

00:03:26.509 --> 00:03:28.109
它们既不是非常正面的 也不是非常负面的

00:03:28.110 --> 00:03:29.570
而是大概处于中间位置

00:03:29.569 --> 00:03:33.716
而在两边 这些实际上非常少量的词

00:03:33.717 --> 00:03:37.088
才是真正的重点 这些词非常非常重要

00:03:37.087 --> 00:03:42.350
对于我来说这样很好 因为如果这些词不重要

00:03:42.350 --> 00:03:46.431
如果它们只是 a、b、句号、逗号之类的

00:03:46.431 --> 00:03:50.366
出现的频率非常高 那么我可以剔除它们

00:03:50.366 --> 00:03:53.630
这会为我节省大量的计算时间 因为

00:03:53.629 --> 00:03:57.564
它们确实出现得很频繁 而且剔除它们并不会影响最终的表现质量

00:03:57.564 --> 00:04:00.914
事实上 反而会有积极的影响 因为

00:04:00.914 --> 00:04:02.495
这些常见词并不具有很强的预测力

00:04:02.495 --> 00:04:07.034
我们的语料库中有大量不具有预测力的单词

00:04:07.034 --> 00:04:09.794
那么 我们就剔除掉它们吧

00:04:10.995 --> 00:04:14.230
这只会让我们的神经网络更加强大

00:04:14.229 --> 00:04:17.699
在我们开始动手之前 再来看一个分布

00:04:20.720 --> 00:04:24.000
这是不同单词的相对出现频率

00:04:26.100 --> 00:04:28.720
通常称为 Zitvian 分布

00:04:28.720 --> 00:04:33.260
我们的语料库非常极端化 有几个词明显处于主导地位

00:04:33.259 --> 00:04:39.620
也就是说 有几个词的出现频率比其他的词要高得多

00:04:39.620 --> 00:04:40.819
看看这里

00:04:42.019 --> 00:04:44.659
对我来说 这非常有意思

00:04:44.660 --> 00:04:48.505
实际上在自然语言处理中 一个普遍的趋势就是

00:04:48.504 --> 00:04:51.875
剔除最高频和最低频的词

00:04:51.875 --> 00:04:54.485
像这里这些几乎永远不会出现的词

00:04:54.485 --> 00:04:54.574
为什么？

00:04:54.574 --> 00:04:56.889
频率非常高的那些词 像 the、a、and

00:04:56.889 --> 00:05:00.189
它们出现得太频繁了 无法给你什么信息

00:05:00.189 --> 00:05:03.759
而那些最不频繁的呢 它只发生一次 没形成一种模式

00:05:03.759 --> 00:05:05.079
如果只发生一次的话

00:05:05.079 --> 00:05:07.979
你怎么能说它一定存在某种相关性呢？

00:05:07.980 --> 00:05:12.200
所以在自然语言处理中 当人们做一个有意思的分类器时

00:05:12.199 --> 00:05:14.680
人们都倾向于将出现频率过多和过少的词都剔除掉 再进行分类

00:05:16.100 --> 00:05:20.320
所以 我们真正在做的 是观察这些概括性的

00:05:20.319 --> 00:05:23.670
可视化图像 找出信号是什么 噪音是什么

00:05:23.670 --> 00:05:29.100
看看我们能否使用这些不同的指标 消除噪音 增强信号

00:05:29.100 --> 00:05:32.800
所以在这里 我看到有一大片噪音

00:05:32.800 --> 00:05:35.340
它们非常频繁 但又无用

00:05:35.339 --> 00:05:36.689
我要将它们去除掉

00:05:36.689 --> 00:05:39.529
然后看看这里

00:05:39.529 --> 00:05:40.879
这里也有很多东西要处理

00:05:40.879 --> 00:05:44.810
我觉得这些 这一大块对应的是这里

00:05:44.810 --> 00:05:48.269
这个过滤器能更好地将高频词清除出去

00:05:48.269 --> 00:05:54.370
而这里的这些词 你甚至看不到它们 因为它们的出现频率实在太低了

00:05:54.370 --> 00:05:56.759
我们也要将这部分剔除掉

00:05:56.759 --> 00:05:57.795
这就到项目 6 了

00:05:57.795 --> 00:06:00.349
我真的很激动

00:06:00.350 --> 00:06:00.930
项目 6 是一个很棒的项目

00:06:03.089 --> 00:06:06.769
项目 6 的目标是通过使用这些统计数据 这些思路降低噪音

00:06:06.769 --> 00:06:09.109
从而提高学习速度

00:06:09.110 --> 00:06:12.860
并没有什么通用的神经网络规则规定你要这么做

00:06:12.860 --> 00:06:15.629
我们要做的是搭建处理这个问题的框架

00:06:15.629 --> 00:06:19.019
使相关性对于神经网络来说尽可能明显

00:06:19.019 --> 00:06:24.149
让它更容易忽略噪音并找到信号

00:06:24.149 --> 00:06:27.169
这就是解决问题的核心所在

00:06:27.170 --> 00:06:30.780
所以在下一个项目中 我们要做的就是

00:06:30.779 --> 00:06:32.809
将这些指标添加到我们的神经网络中

00:06:32.810 --> 00:06:34.230
我想让你来试一试
127
00:06:34,230 --&gt; 00:06:38,161
看看能不能构建这个神经网络 或修改这个网络

00:06:38.161 --> 00:06:42.343
在训练方法或 SentimentNetwork 方法中

00:06:42.343 --> 00:06:46.796
放入一个参数 表示你将剔除多少比例的词汇

00:06:46.797 --> 00:06:48.927
放入一些个参数 表示

00:06:48.927 --> 00:06:52.230
要清除频率最高和频率最低的那些词

00:06:52.230 --> 00:06:55.478
你会设置最低计数

00:06:55.478 --> 00:06:58.985
比如说 每个词至少要出现 10 次 或者 5 次

00:06:58.985 --> 00:07:02.254
这些词才能进入到神经网络中

00:07:02.254 --> 00:07:03.719
然后看看效果如何

00:07:03.720 --> 00:07:05.060
那么 让我们来试试吧

00:07:05.060 --> 00:07:07.449
一会儿 我将挑出一个可以成功运作的网络来聊一聊

00:07:07.449 --> 00:07:08.529
到时候见

