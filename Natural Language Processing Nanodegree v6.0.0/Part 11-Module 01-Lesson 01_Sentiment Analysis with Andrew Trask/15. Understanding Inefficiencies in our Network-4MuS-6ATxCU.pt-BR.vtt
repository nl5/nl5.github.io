WEBVTT
Kind: captions
Language: pt-BR

00:00:01.300 --> 00:00:05.033
Na última seção,
otimizamos nossa rede neural

00:00:05.067 --> 00:00:07.834
para melhor encontrar
a correlação nos dados

00:00:07.868 --> 00:00:11.234
removendo a distração
de alguns ruídos.

00:00:11.267 --> 00:00:14.434
A rede neural interpretou
muito melhor o sinal.

00:00:14.467 --> 00:00:17.100
Ficou 83% correto
nos dados de treinamento

00:00:17.133 --> 00:00:20.367
e o teste ficou 85% correto.

00:00:20.400 --> 00:00:21.701
Só com uma iteração.

00:00:21.734 --> 00:00:24.701
Podíamos treinar mais
e melhorar mais um pouco.

00:00:24.734 --> 00:00:29.033
Vamos comparar com esse resultado
para ver quão rápido pode treinar.

00:00:29.067 --> 00:00:31.334
Ficou melhor do que
os 60% de antes.

00:00:31.367 --> 00:00:36.067
Foi um grande ganho
em precisão e velocidade.

00:00:36.100 --> 00:00:38.968
Um grande progresso.

00:00:39.000 --> 00:00:42.067
Entretanto, a velocidade
de computação,

00:00:42.100 --> 00:00:46.467
quantos segundos leva
para fazer um passo completo,

00:00:46.501 --> 00:00:48.200
ainda está bem lento.

00:00:48.234 --> 00:00:52.033
O que eu quero fazer agora
é atacar esta rede

00:00:52.067 --> 00:00:56.534
e ver o que ela está fazendo
que gasta tempo de computação.

00:00:56.567 --> 00:01:01.234
Antes estávamos gastando dados,
e agora, onde estamos gastando?

00:01:01.267 --> 00:01:03.000
É engraçado.

00:01:03.033 --> 00:01:07.133
Pode fazer muitas coisas teóricas
para aprender mais rápido,

00:01:07.167 --> 00:01:10.300
mas na verdade,
a anterior estava aprendendo,

00:01:10.334 --> 00:01:11.968
só que demorava muito.

00:01:12.000 --> 00:01:14.767
Também podemos otimizar
o lado computacional

00:01:14.801 --> 00:01:17.167
para treinar
muito mais rápido

00:01:17.200 --> 00:01:21.167
e ainda consegue
aprender o que deve.

00:01:21.200 --> 00:01:24.367
Quanto mais rápido
puder treinar...

00:01:24.400 --> 00:01:25.701
Para ser sincero,

00:01:25.734 --> 00:01:29.567
quanto mais tempo deixar treinar
antes de ficar entediado.

00:01:29.601 --> 00:01:33.000
Quando se treina redes neurais,
pode deixar treinando,

00:01:33.033 --> 00:01:34.534
não tem um fim natural.

00:01:34.567 --> 00:01:38.100
Ao contrário de certos modelos
de grafos probabilísticos

00:01:38.133 --> 00:01:43.100
onde você conta um monte de coisas,
e quando termina, termina.

00:01:43.133 --> 00:01:45.834
As redes neurais
podem continuar treinando.

00:01:45.868 --> 00:01:48.200
Quanto mais rápido
conseguir treinar,

00:01:48.234 --> 00:01:51.400
quanto mais dados colocar,
a rede será mais forte.

00:01:51.434 --> 00:01:53.934
O que vamos fazer aqui
é analisar a rede

00:01:53.968 --> 00:01:56.434
e procurar por coisas
que podemos tirar

00:01:56.467 --> 00:01:58.868
para que a rede
possa ir mais rápido.

00:01:58.901 --> 00:02:02.634
A primeira
que chama minha atenção

00:02:02.667 --> 00:02:05.367
é que estamos criando
um vetor muito grande

00:02:05.400 --> 00:02:06.634
para a camada zero.

00:02:06.667 --> 00:02:10.868
74 mil e alguma coisa
valores.

00:02:10.901 --> 00:02:15.367
Só alguns deles
estão com valor 1.

00:02:15.400 --> 00:02:17.400
Por que isso importa?

00:02:17.434 --> 00:02:20.367
O passo de propagação
é a soma ponderada.

00:02:20.400 --> 00:02:23.100
Este 1 multiplicado
por estes pesos,

00:02:23.133 --> 00:02:24.667
somados na camada 1.

00:02:24.701 --> 00:02:29.167
Depois o zero seria
multiplicado pelos pesos,

00:02:29.200 --> 00:02:31.067
e somado na camada 1.

00:02:31.100 --> 00:02:32.601
Espere...

00:02:32.634 --> 00:02:35.601
Pegamos um zero
e multiplicamos pelo peso

00:02:35.634 --> 00:02:37.767
e somamos o resultado?

00:02:37.801 --> 00:02:39.601
Então sempre
que tem um zero,

00:02:39.634 --> 00:02:43.567
pegamos este vetor
e multiplicamos pela matriz,

00:02:43.601 --> 00:02:45.067
para criar a camada 1,

00:02:45.100 --> 00:02:47.667
todos esses zeros
não estão fazendo nada,

00:02:47.701 --> 00:02:51.367
porque zero vezes
qualquer coisa dá zero.

00:02:51.400 --> 00:02:55.133
Zero vezes este vetor
somado na camada 1

00:02:55.167 --> 00:02:58.601
não muda a camada 1
do que ela era antes.

00:02:58.634 --> 00:03:04.100
Essa é a maior fonte
de ineficiência nesta rede.

00:03:04.133 --> 00:03:09.367
Para mostrar computacionalmente
e provar que é isso mesmo,

00:03:09.400 --> 00:03:10.901
vejam isso.

00:03:10.934 --> 00:03:13.834
Temos uma camada 0 falsa,
com 10 valores.

00:03:13.868 --> 00:03:15.701
Vou colocar ela aqui.

00:03:15.734 --> 00:03:18.934
Vamos dizer
que a camada zero...

00:03:20.167 --> 00:03:24.267
A camada zero,
o quarto valor é 1,

00:03:24.300 --> 00:03:28.300
finja que tem
umas palavras aqui.

00:03:29.434 --> 00:03:32.267
Vamos ver a camada 0 de novo.
Assim.

00:03:34.200 --> 00:03:40.300
Os pesos podem ser
uma matriz aleatória.

00:03:42.901 --> 00:03:47.434
Agora vamos ver a camada zero
multiplicada pelos pesos.

00:03:48.367 --> 00:03:49.801
Pronto.

00:03:49.834 --> 00:03:52.234
E se, em vez disso,

00:03:52.267 --> 00:03:55.367
somássemos só esses vetores?

00:03:55.400 --> 00:03:59.934
Vamos fazer 1 vezes isso
e colocar aqui.

00:03:59.968 --> 00:04:04.400
Tem dois índices,
o 4 e o 9.

00:04:04.434 --> 00:04:07.234
Precisamos
de uma camada nova.

00:04:07.267 --> 00:04:09.300
Camada 1.

00:04:11.634 --> 00:04:14.767
Uma camada vazia,
com 5 valores.

00:04:14.801 --> 00:04:18.734
Para cada um
daqueles índices

00:04:18.767 --> 00:04:20.467
os pesos

00:04:21.634 --> 00:04:24.200
e os índices...

00:04:24.767 --> 00:04:26.334
Camada 1.

00:04:29.067 --> 00:04:33.367
Multiplicado por 1,
porque tinha nos dados.

00:04:33.400 --> 00:04:35.067
Camada 1.

00:04:39.501 --> 00:04:42.033
São os mesmos valores.

00:04:42.067 --> 00:04:47.501
O que é legal é que trabalhamos
só com parte da matriz.

00:04:47.534 --> 00:04:52.767
Se forem duas
entre 70 mil palavras,

00:04:52.801 --> 00:05:01.567
acabamos de economizar
esta operação 69 mil vezes.

00:05:01.601 --> 00:05:04.133
Deve ser um bom ganho.

00:05:04.167 --> 00:05:06.501
Vamos ver quanto
vai ser no final,

00:05:06.534 --> 00:05:09.834
mas deve ser bem legal.

00:05:09.868 --> 00:05:12.467
Quero ver como fica.

00:05:12.501 --> 00:05:15.267
Vamos dar uma olhada
na rede neural de novo.

00:05:15.300 --> 00:05:17.434
Procurar mais ineficiências.

00:05:17.467 --> 00:05:19.868
Outra coisa ineficiente

00:05:19.901 --> 00:05:22.467
é que 1 vezes qualquer coisa
é ele mesmo.

00:05:22.501 --> 00:05:25.067
Multiplicar por 1
é perda de tempo.

00:05:25.100 --> 00:05:29.834
E se, em vez disso,
fizéssemos só uma soma.

00:05:29.868 --> 00:05:32.968
Estamos multiplicando
por 1, mesmo.

00:05:34.100 --> 00:05:35.834
Deu a mesma coisa.

00:05:35.868 --> 00:05:39.667
Nos livramos da multiplicação
e de todas essas.

00:05:39.701 --> 00:05:42.801
Ainda obtemos
o mesmo estado oculto

00:05:42.834 --> 00:05:47.067
que quando calculávamos
todo o produto interno,

00:05:47.100 --> 00:05:49.734
a multiplicação
da matriz pelo vetor.

00:05:49.767 --> 00:05:50.767
Estou gostando.

00:05:50.801 --> 00:05:53.968
É uma boa melhora.
A maior parte dos pesos é aqui.

00:05:54.000 --> 00:05:56.501
Só tem 4 que vão
da oculta para a saída.

00:05:56.534 --> 00:06:00.601
Bem, o tamanho da camada oculta.
Acho que é maior do que isso.

00:06:00.634 --> 00:06:03.701
A maior parte da computação
está aqui.

00:06:03.734 --> 00:06:06.467
74 mil vezes
o tamanho da camada oculta.

00:06:06.501 --> 00:06:10.367
É a maior parte
do treinamento.

00:06:10.400 --> 00:06:14.100
Isso nos leva
ao próximo projeto.

00:06:14.133 --> 00:06:16.133
O projeto 6 é...

00:06:16.167 --> 00:06:18.067
O projeto 5, na verdade.

00:06:18.100 --> 00:06:22.868
Instalar isso
na rede neural de antes.

00:06:22.901 --> 00:06:26.501
Vou deixar você tentar.

00:06:26.534 --> 00:06:31.267
Quando voltarmos, vamos falar
sobre como fazer isso.

00:06:31.300 --> 00:06:32.267
Boa sorte!

