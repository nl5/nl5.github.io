WEBVTT
Kind: captions
Language: en

00:00:00.430 --> 00:00:01.080
All right, welcome back.

00:00:01.080 --> 00:00:03.640
So we're in project six where we're
going to be reducing noise by

00:00:03.640 --> 00:00:05.450
strategically reducing the vocabulary.

00:00:05.450 --> 00:00:09.320
So what we've done is we've taken
these metrics that we've kind of used

00:00:09.320 --> 00:00:13.000
earlier as a juristic to see
whether an idea was a good idea.

00:00:13.000 --> 00:00:17.940
And we're going to use it to carve
out a little bit of the noise so

00:00:17.940 --> 00:00:20.860
that the neural net can
better see the signal.

00:00:20.860 --> 00:00:24.650
Now in this case most of the action
happens in this pre-processing step,

00:00:24.650 --> 00:00:24.880
right?

00:00:24.880 --> 00:00:29.180
So instead of,
we kind of updated the training before.

00:00:29.180 --> 00:00:32.460
Here we're going to try to reduce
the vocabulary to this vocab object,

00:00:32.460 --> 00:00:35.820
which gets turned into the word index
object and used to create our indices.

00:00:35.820 --> 00:00:41.190
We want to reduce this in specific ways
according to different cut-offs and

00:00:41.190 --> 00:00:42.210
thresholds.

00:00:42.210 --> 00:00:46.678
So, in this case, I created a
polarity_cutoff and a min_count cutoff.

00:00:46.678 --> 00:00:53.300
Now what the min_count cutoff does,
it says okay, so in order for a word

00:00:53.300 --> 00:00:58.095
to be included in the review_vocab,
Vocab, one of those created vocab.

00:00:58.095 --> 00:01:01.345
It has to exceed a minimum count,
all right?

00:01:01.345 --> 00:01:05.355
So, if it doesn't get into vocab,
it doesn't actually get trained

00:01:05.355 --> 00:01:08.995
on according to the pre-processing
step down here.

00:01:08.995 --> 00:01:12.750
So if the word Is not in the vocab,
it's not in words2index.keys.

00:01:12.750 --> 00:01:15.870
It doesn't get added to this indices
list, it doesn't get trained on, so

00:01:15.870 --> 00:01:17.190
it gets ignored.

00:01:17.190 --> 00:01:18.390
So that's the first thing.

00:01:18.390 --> 00:01:20.500
And the second thing is
this polarity cut off.

00:01:20.500 --> 00:01:24.150
This is saying that this pos neg ratio
has to be greater than or equal to

00:01:24.150 --> 00:01:28.950
the polarity cut off or less than or
equal to negative the polarity cut off.

00:01:28.950 --> 00:01:31.780
because if you remember,
this thing centers around one.

00:01:31.780 --> 00:01:36.900
So we're saying a word has to either
be less than the negative ratio or

00:01:36.900 --> 00:01:39.430
more than the positive ratio, right?

00:01:39.430 --> 00:01:45.080
So it's excluding this really tall, and
yet irrelevant, section in the middle.

00:01:46.710 --> 00:01:47.820
That's the idea.

00:01:47.820 --> 00:01:52.670
One other thing I did was I required
that it be a minimum count for

00:01:52.670 --> 00:01:54.860
this pos_neg_ratio to
really take effect.

00:01:55.900 --> 00:02:00.740
And a reason I did that is that
occasionally you have really infrequent

00:02:00.740 --> 00:02:08.400
terms that have very little correlative
power or a lot of correlative power.

00:02:08.400 --> 00:02:10.150
But they only happen once or twice.

00:02:10.150 --> 00:02:14.030
So the metric, it's a simple rule,
it's a simple cutoff.

00:02:14.030 --> 00:02:15.510
I can't have it be,

00:02:15.510 --> 00:02:18.310
it's perfectly positively correlated
would only happen once, right?

00:02:18.310 --> 00:02:21.310
So in this case I found
this was a good number.

00:02:22.400 --> 00:02:25.065
So those are the two thresholds.

00:02:25.065 --> 00:02:26.890
There's not really a right way or
a wrong way to do this.

00:02:26.890 --> 00:02:29.230
Again, we're trying to
cut out the noise and

00:02:29.230 --> 00:02:31.240
this is different for
every data set, right?

00:02:31.240 --> 00:02:32.657
So you're in kind of
a new data setting and

00:02:32.657 --> 00:02:35.287
you're trying to take a neural net and
you're going to try to frame the neural

00:02:35.287 --> 00:02:37.300
net a problem where it
can succeed a data set.

00:02:37.300 --> 00:02:40.150
This is just the method you go
about by framing the problem

00:02:40.150 --> 00:02:40.828
when you're in a domain.

00:02:40.828 --> 00:02:45.798
Like these are not hard and fast rules,
these are techniques to be able

00:02:45.798 --> 00:02:50.686
to make sure that your neural net has
the best chance to capture new and

00:02:50.686 --> 00:02:52.840
interesting patterns.

00:02:52.840 --> 00:02:56.280
So, all of this is the same,
none of these has changed at all,

00:02:56.280 --> 00:02:57.747
testing is not different,
running is not different,

00:02:57.747 --> 00:02:59.570
other than the real
vocabulary is smaller.

00:02:59.570 --> 00:03:02.909
So let's give it a shot,
I'm going to set the min count at 20 and

00:03:02.909 --> 00:03:06.689
the polarity cut off at .05 which
is a really small polarity cutoff,

00:03:06.689 --> 00:03:10.120
like this doesn't cut out
too much of the vocabulary.

00:03:10.120 --> 00:03:13.491
We can get more aggressive
about it later,

00:03:13.491 --> 00:03:18.092
and yeah, we'll see how this
thing goes here in a second.

00:03:18.092 --> 00:03:22.273
So what it's doing right now is it's
calculating all the statistics as it

00:03:22.273 --> 00:03:26.770
started, so we had do it then, and
then it's going to start training.

00:03:26.770 --> 00:03:29.645
So yeah, so
when you call the constructor,

00:03:29.645 --> 00:03:34.467
calls pre-processed data and
pre-processed data runs all these stakes

00:03:34.467 --> 00:03:39.020
from before to help kind of identify
signal we're going to train.

00:03:39.020 --> 00:03:39.466
All right, very.

00:03:39.466 --> 00:03:40.564
Very good.

00:03:40.564 --> 00:03:42.735
[BLANK_AUDIO]

00:03:42.735 --> 00:03:46.650
So this is actually a little bit
of a speed lift from before.

00:03:46.650 --> 00:03:50.850
Not a lot, just a little bit, because
we've reduced our vocabulary size.

00:03:50.850 --> 00:03:54.840
And then we look at our testing data,
85.9.

00:03:54.840 --> 00:03:58.980
Not a huge lift, but
a small and significant lift.

00:03:58.980 --> 00:04:03.020
Like, and you can pick your battles and
I didn't do this too long.

00:04:03.020 --> 00:04:06.550
So you kind of keep playing with this
min count and polarity cut off and

00:04:06.550 --> 00:04:09.090
find which one seems to
work the best for you.

00:04:09.090 --> 00:04:14.115
Now the other thing that I wanted to
show here is that if we take this and

00:04:14.115 --> 00:04:16.300
just crank it up, right?

00:04:16.300 --> 00:04:20.940
So, if we set this to be more like,
I don't know.

00:04:20.940 --> 00:04:22.440
We'll keep min count the same.

00:04:22.440 --> 00:04:23.780
We'll set this to be .8.

00:04:23.780 --> 00:04:29.920
We really carve out
the middle of this guy.

00:04:29.920 --> 00:04:32.000
We're just going to take a big chunk.

00:04:32.000 --> 00:04:36.350
.8, that's like right here to right
here, and we're going to say,

00:04:36.350 --> 00:04:37.330
you know what?

00:04:37.330 --> 00:04:39.670
This thing stuff has little correlation,

00:04:39.670 --> 00:04:44.570
but I just think it's ambiguous,
so just carve it all out.

00:04:44.570 --> 00:04:47.780
And we say, okay,
we're going to train on that.

00:04:47.780 --> 00:04:53.160
Then we can really crank
up the speed of this thing.

00:04:53.160 --> 00:04:54.900
The speed by which it trains,

00:04:56.100 --> 00:05:00.570
with a relatively small effect
on our testing accuracy.

00:05:00.570 --> 00:05:05.320
So sometimes you'll find that this
can be beneficial for two reasons.

00:05:05.320 --> 00:05:09.050
For some problems you want to be
able to train on so much more data,

00:05:09.050 --> 00:05:10.750
you want to be able to run a lot faster.

00:05:10.750 --> 00:05:13.375
Let's see how much faster it goes.

00:05:13.375 --> 00:05:17.200
Straining, my gosh, look at that.

00:05:17.200 --> 00:05:19.590
So 7,000 reviews per second.

00:05:19.590 --> 00:05:23.930
I mean, it trained whole
thing in a couple seconds.

00:05:23.930 --> 00:05:28.015
And let's see how,
let's see the damages in the test.

00:05:28.015 --> 00:05:28.820
82.2, so we lost 3%.

00:05:28.820 --> 00:05:35.113
Not bad, we got a 7x increase
in speed for a, well,

00:05:35.113 --> 00:05:40.560
almost 7x, for a 3% decrease in quality.

00:05:40.560 --> 00:05:43.540
Now, you're going to find in neural
nets there's usually a speed

00:05:43.540 --> 00:05:44.150
trade out quality.

00:05:44.150 --> 00:05:47.030
The only time that I know of when
there consistently isn't is when

00:05:47.030 --> 00:05:48.390
you can reduce noise.

00:05:48.390 --> 00:05:51.250
because reducing noise tends to reduce
the amount of data that you have to

00:05:51.250 --> 00:05:53.570
process, which increases both speed and
accuracy.

00:05:53.570 --> 00:05:55.980
It's why I like it so much.

00:05:55.980 --> 00:05:58.220
But this can actually
be really beneficial.

00:05:58.220 --> 00:06:00.090
One, for production systems.

00:06:00.090 --> 00:06:03.490
When you just need something to fly,
to get through a ton of data.

00:06:03.490 --> 00:06:06.420
To really solve a real world
problem that exists out there.

00:06:06.420 --> 00:06:09.380
Like the most laboratory

00:06:11.980 --> 00:06:16.430
kind of sterile, highest possible
score isn't always the best answer.

00:06:16.430 --> 00:06:19.410
Sometimes the best answer is hey,
we want to fly through

00:06:19.410 --> 00:06:22.730
a million medical reports a day so
that we can save the most lives.

00:06:22.730 --> 00:06:25.029
Give me something that's faster.

00:06:25.029 --> 00:06:29.590
And this is the something that
you can do to try to do that.

00:06:29.590 --> 00:06:33.080
And once again, we got almost 10X and
if we cranked it up even further,

00:06:33.080 --> 00:06:34.680
we probably could.

00:06:34.680 --> 00:06:38.330
And so the cool thing about this is that
the more you carve out you're saying,

00:06:38.330 --> 00:06:39.670
hey, I need to go faster and

00:06:39.670 --> 00:06:43.500
I'm trying to do the minimum amount
of damage that I can to go faster.

00:06:43.500 --> 00:06:47.947
So I'm getting rid of the terms that
are most ambiguous in my prediction and

00:06:47.947 --> 00:06:51.070
it's saving me lots of time.

00:06:51.070 --> 00:06:52.310
So now why is it saving me time?

00:06:52.310 --> 00:06:54.780
Well, we do fewer sums.

00:06:54.780 --> 00:06:58.252
When we have fewer terms we do fewer
sums in the display matrix and

00:06:58.252 --> 00:07:01.096
we do fewer back propagation
in the display matrix.

00:07:01.096 --> 00:07:04.800
Because there are fewer words in
the review that we've allowed to pass

00:07:04.800 --> 00:07:08.290
through that were candidate
words that we could train on.

00:07:08.290 --> 00:07:10.230
That's just all we did, and

00:07:10.230 --> 00:07:13.550
fortunately it increases
the speed very significantly.

00:07:14.810 --> 00:07:20.080
Now, the other reason that sometimes
people will increase the speed

00:07:20.080 --> 00:07:25.110
like this or in a variety of
other ways is if you have so

00:07:25.110 --> 00:07:28.830
much training data that you could
never train over all of it.

00:07:28.830 --> 00:07:33.600
It turns out that just being able
to run faster over more data,

00:07:33.600 --> 00:07:36.040
even if you choose a more
naive algorithm or

00:07:36.040 --> 00:07:39.830
your limiting a lot of stuff,
makes the accuracy really, really high.

00:07:39.830 --> 00:07:43.920
The most famous example of this
that maybe people don't know is

00:07:45.240 --> 00:07:46.945
taking its effect is Word2Vec.

00:07:46.945 --> 00:07:51.380
So Word2Vec is a close approximation of

00:07:51.380 --> 00:07:54.320
other language models that people
have been training forever.

00:07:54.320 --> 00:07:57.377
But the problem was other language
models took like a month to train on

00:07:57.377 --> 00:08:00.920
a billion tokens and even then it wasn't
making as aggressive weight updates.

00:08:00.920 --> 00:08:04.632
What Word2Vec showed was that if
you striped out everything and

00:08:04.632 --> 00:08:09.180
you skip some steps, yeah it's an
approximation, but It's so much faster.

00:08:09.180 --> 00:08:11.120
You can train it on,

00:08:11.120 --> 00:08:15.372
was it eight billion tokens in like
six hours on a 32 core machine?

00:08:15.372 --> 00:08:18.360
I mean it's so much faster that
you can train so much data.

00:08:18.360 --> 00:08:22.410
You can gather so much more information
that even though the backhoe is dropping

00:08:22.410 --> 00:08:27.640
dirt while it's driving back to
the dumpster, it can do it so

00:08:27.640 --> 00:08:30.250
much faster, in so many more iterations,
it just doesnâ€™t matter.

00:08:30.250 --> 00:08:34.490
So, there's a lot of cases where
we might make it way faster and

00:08:34.490 --> 00:08:37.690
then we'll train on seven
times more data, and

00:08:37.690 --> 00:08:40.970
then that'll actually get
us to a higher score.

00:08:40.970 --> 00:08:44.900
Actually I can almost guarantee you
that if we had seven times more data and

00:08:44.900 --> 00:08:49.732
we trained this with something that's
seven times faster relative to this,

00:08:49.732 --> 00:08:51.476
it's going to get a way higher score.

00:08:51.476 --> 00:08:55.180
Because once again,
you're covering more ground.

00:08:55.180 --> 00:08:58.690
We lost a marginal
amount of accuracy here.

00:08:58.690 --> 00:09:02.460
So if training over more
data was the problem,

00:09:02.460 --> 00:09:07.460
then this can often be a really,
really good solution.

00:09:07.460 --> 00:09:10.810
So, I hope you've enjoyed and
in the next section we're going to talk

00:09:10.810 --> 00:09:14.270
a little bit more about what's actually
happening under the hood in the weights.

00:09:14.270 --> 00:09:17.830
How are they adjusting to each other,
what's happening on the terms, and

00:09:17.830 --> 00:09:20.690
really see some kind of cool
visualizations that help us

00:09:20.690 --> 00:09:22.930
get a more intuitive feeling for
what's going on in the neural net.

00:09:22.930 --> 00:09:23.670
See you there.

