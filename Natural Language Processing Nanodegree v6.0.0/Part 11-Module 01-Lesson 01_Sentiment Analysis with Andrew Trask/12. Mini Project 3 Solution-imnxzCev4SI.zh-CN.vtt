WEBVTT
Kind: captions
Language: zh-CN

00:00:00.340 --> 00:00:03.520
好 在项目3中 我们将构建神经网络

00:00:03.520 --> 00:00:06.220
来预测一篇影评表达的是正面情感还是负面情感

00:00:06.219 --> 00:00:11.149
方法就是对影评中出现的单词进行计数

00:00:11.150 --> 00:00:15.120
我做的第一步 就是创建一个数据预处理函数

00:00:15.119 --> 00:00:18.320
将我们之前构造并测试过的各个小块串在一起

00:00:18.320 --> 00:00:23.329
包括 word2index 不同的词汇 以及词汇表大小等

00:00:23.329 --> 00:00:26.139
那些我们在生成训练数据集逻辑中使用的变量

00:00:26.140 --> 00:00:28.750
我希望将它们都放在一个数据预处理函数中

00:00:28.750 --> 00:00:33.280
这样 这些变量都存在于这个相对独立的类中

00:00:33.280 --> 00:00:35.960
接下来我做的 是将 int_network 函数拆分出来

00:00:35.960 --> 00:00:38.350
让这部分代码看起来更简洁明了

00:00:38.350 --> 00:00:43.340
而且 这个函数需要知道输入节点数 和输出节点数

00:00:43.340 --> 00:00:47.640
它们取决于共有多少条评论 或者有多少不重复词汇

00:00:47.640 --> 00:00:49.490
以及我们一共有多少个标签

00:00:49.490 --> 00:00:52.900
所以 将函数分别独立出来挺好的

00:00:52.899 --> 00:00:55.969
这样你通过查看函数名称就可以了解相关步骤

00:00:55.969 --> 00:00:58.600
清晰明了 我挺喜欢的

00:00:58.600 --> 00:01:01.213
接下来 我做的是 更新输入层 为标签设定目标值

00:01:01.213 --> 00:01:03.695
之前我们创建过它们的函数

00:01:03.695 --> 00:01:05.974
然后将它们移到类中

00:01:05.974 --> 00:01:08.306
使这一类的功能齐全

00:01:08.307 --> 00:01:11.490
方便调用 我可以在其他地方使用它

00:01:11.489 --> 00:01:11.959
好

00:01:11.959 --> 00:01:14.439
现在来看 train 函数 这就是步骤最多的地方

00:01:14.439 --> 00:01:16.899
所以第一步 我要检查的是

00:01:16.900 --> 00:01:20.050
待训练评论的数量与标签数量是否一致

00:01:20.049 --> 00:01:22.769
以防人为输入了前后不一致的值

00:01:22.769 --> 00:01:25.519
这样可以提醒用户

00:01:25.519 --> 00:01:29.140
出现了异常输入

00:01:29.140 --> 00:01:31.548
接下来 我们要做的就是 初始化一个 correct_so_far 值

00:01:31.548 --> 00:01:33.638
用于统计到目前为止预测正确的数目

00:01:33.638 --> 00:01:34.679
在训练过程中密切关注训练进展

00:01:34.680 --> 00:01:37.860
这是一个很有用的指标 我喜欢用它来直观了解

00:01:37.859 --> 00:01:41.099
神经网络在训练过程中究竟表现如何

00:01:41.099 --> 00:01:45.349
是越来越好 毫无进展 还是越来越糟？

00:01:45.349 --> 00:01:49.250
这些都是最基本的帮你了解训练效果的方法

00:01:49.250 --> 00:01:51.620
让你可以据此优化网络

00:01:51.620 --> 00:01:53.500
现在 我们从训练评论集中

00:01:53.500 --> 00:01:56.370
选择一条评论和对应的标签 对输入层进行更新

00:01:56.370 --> 00:02:00.280
这类似于我们之前从第 0 层到第 1 层传播

00:02:00.280 --> 00:02:02.840
或者说 从输入层向隐藏层传播

00:02:02.840 --> 00:02:07.100
但不同的是 这里 我们要先调整和生成输入数据集

00:02:07.099 --> 00:02:10.948
然后再进行传播运算

00:02:10.949 --> 00:02:14.330
所以现在 我们与之前一样生成隐藏层 只是它是线性的

00:02:14.330 --> 00:02:17.100
最后一层 是非线性的

00:02:17.099 --> 00:02:19.939
这些就是正向传播的步骤

00:02:19.939 --> 00:02:22.689
接下来反向传播 第一步

00:02:22.689 --> 00:02:24.564
就是确定误差

00:02:24.564 --> 00:02:26.490
在这里放入之前创建的函数

00:02:26.490 --> 00:02:31.519
用预测值 减去函数所调用的实际标签值

00:02:31.519 --> 00:02:36.935
由于该层具有非线性 所以要得到 layer_2_delta 

00:02:36.935 --> 00:02:41.883
乘数之一应为 sigmoid 的导数函数 即 sigmoid * (1 - sigmoid)

00:02:41.883 --> 00:02:42.640
然后 继续以这种方式做反向传播

00:02:42.639 --> 00:02:46.489
你们可能注意到了 我未对第 1 层做反向传播 

00:02:46.490 --> 00:02:51.469
是因为第 1 层是线性的 我不用做类似的导数乘法

00:02:51.469 --> 00:02:53.060
因为这是一个线性层

00:02:53.060 --> 00:02:58.134
我们不必像非线性一般调整斜率

00:02:58.133 --> 00:03:00.430
有了 layer_2_delta 和 layer_1_delta 后

00:03:00.431 --> 00:03:02.219
我们就可以更新权值了

00:03:02.219 --> 00:03:04.849
这和我们之前神经网络中的做法

00:03:04.849 --> 00:03:05.900
完全一样

00:03:05.900 --> 00:03:09.879
然后 稍微添加一些逻辑 来用日志记录训练过程

00:03:09.879 --> 00:03:14.449
看看训练速度多快 正确预测的数量有多少

00:03:14.449 --> 00:03:18.229
那么 我们如何判定是否预测对了呢？

00:03:18.229 --> 00:03:22.099
我这里看到的 是预测值的绝对值

00:03:22.099 --> 00:03:24.280
抱歉 是我们预测误差的绝对值

00:03:24.280 --> 00:03:27.784
所以在上面这里 我们计算两者差异 即预测结果

00:03:27.784 --> 00:03:29.169
和实际值之间的差异

00:03:29.169 --> 00:03:32.526
所以 我说 如果预测值正好是 0.5（虽然并没有） 

00:03:32.526 --> 00:03:34.209
这就完全是模棱两可

00:03:34.210 --> 00:03:38.378
刚好处在正负的正中间 没有选择任何一项

00:03:38.377 --> 00:03:42.595
但是 如果它更接近正确预测值

00:03:42.596 --> 00:03:47.710
那么 这一误差测量值将小于 0.5

00:03:47.710 --> 00:03:52.252
所以我能看到 究竟预测正确了多少个

00:03:52.252 --> 00:03:56.426
而不是只知道误差值是多少

00:03:56.425 --> 00:03:57.829
因为电脑会一直记录日志

00:03:59.379 --> 00:04:02.519
这里 我想做的另一件事 就是测试它

00:04:02.520 --> 00:04:06.140
实际上就是 先对它做一个正向传播

00:04:06.139 --> 00:04:09.869
再用一个评估逻辑 把它们作为一整个函数 就是我这里所做的

00:04:09.870 --> 00:04:14.300
然后 我还定义了另一个运行函数：输入一段评论文本

00:04:14.300 --> 00:04:17.300
它会将其转换成输入数据 经过正向传播

00:04:17.300 --> 00:04:18.519
得出 正标签 或 负标签

00:04:18.519 --> 00:04:20.588
所以 我们可以测试整个数据集

00:04:20.588 --> 00:04:24.689
也可以对某些单独的样例进行测试

00:04:24.689 --> 00:04:28.930
完成了这一步 现在我们继续

00:04:28.930 --> 00:04:33.509
我们先来创建一个验证对象

00:04:34.649 --> 00:04:37.727
这里 我选择了前 24,000 条评论来进行训练

00:04:37.728 --> 00:04:39.300
然后

00:04:39.300 --> 00:04:46.600
将最后的 1,000 个评论作为我们的测试集

00:04:46.600 --> 00:04:48.129
我就继续这么用了

00:04:48.129 --> 00:04:50.230
当然你可以划分不同的训练集和测试集

00:04:50.230 --> 00:04:53.939
事实上 IMDB 数据集中还能再找到 25,000 条评论供你使用

00:04:53.939 --> 00:04:55.399
但为了简便起见

00:04:55.399 --> 00:04:58.250
我们就用这些数据了

00:04:58.250 --> 00:05:00.009
首先 我们这样初始化数据

00:05:00.009 --> 00:05:02.159
这是我们的默认学习率

00:05:02.160 --> 00:05:07.140
来自这里 我把它放进来 让大家看得见

00:05:07.139 --> 00:05:10.399
开始之前我还想做的一件事 就是先测试它

00:05:10.399 --> 00:05:13.099
我们的权值是随机初始化的

00:05:13.100 --> 00:05:16.879
它的预测表现应该并不好

00:05:16.879 --> 00:05:20.939
在这个例子中 测试准确度正好是 50%

00:05:20.939 --> 00:05:25.389
如果你在正和负之间随机猜一个值

00:05:25.389 --> 00:05:28.159
就会有 50% 的准确率 这也是我们这里所看到的

00:05:28.160 --> 00:05:31.970
这是一个不错的起点 特别是当你的神经网络

00:05:31.970 --> 00:05:33.800
只有两个预测值时

00:05:33.800 --> 00:05:37.470
我很希望看到它在刚开始时不偏向于任何一边

00:05:37.470 --> 00:05:41.889
如果在初始化权值时 它的预测总是偏向于一边

00:05:41.889 --> 00:05:45.829
或另一边 或者哪一边都预测不对

00:05:45.829 --> 00:05:47.620
那我就得停下来了 找找看

00:05:47.620 --> 00:05:51.689
是不是哪里出错了 我得先研究一下情况

00:05:51.689 --> 00:05:55.389
但是 可以看到 它现在的比例是随机的

00:05:55.389 --> 00:05:59.819
目前似乎还没有任何的预测能力

00:05:59.819 --> 00:06:02.099
那么 我们就要开始训练我们的网络了

00:06:04.290 --> 00:06:07.280
我后面加进来的一些代码规定

00:06:07.279 --> 00:06:10.509
每预测 2,500 次 另起一新行 所以我们看不到

00:06:10.509 --> 00:06:15.199
当前的进程 但我们可以看到它如何随时间变化

00:06:16.560 --> 00:06:18.910
所以 当我观察它训练时 我会注意以下几个方面

00:06:18.910 --> 00:06:23.520
首先是速度 要试着衡量 我要坐在这里等多久

00:06:23.519 --> 00:06:26.123
然后是训练准确度

00:06:26.124 --> 00:06:31.081
如果你注意观察 会发现它目前预测得并不太好

00:06:31.081 --> 00:06:33.910
甚至比随机情况下还糟

00:06:33.910 --> 00:06:37.140
比之前的还糟

00:06:37.139 --> 00:06:40.990
到这个点 进度已经达到训练数据集的 14% 了

00:06:40.990 --> 00:06:45.329
但它什么都还没学到 看起来比之前还糟

00:06:45.329 --> 00:06:48.019
我开始有点怀疑是不是哪里出问题了

00:06:48.019 --> 00:06:52.379
的确有几种神经网络 会在这个点上

00:06:52.379 --> 00:06:55.469
给出如随机结果般的预测 特别是在强化学习中

00:06:55.470 --> 00:06:58.630
但在这个数据集上 我们寻找的是直接相关性

00:06:58.629 --> 00:07:03.517
现在我应该能看到一些变化才对 

00:07:03.517 --> 00:07:08.485
所以我打算直接退出它 我们可以继续等 但我觉得

00:07:08.485 --> 00:07:13.224
这不是一个好主意 所以我选择直接点停止

00:07:13.225 --> 00:07:15.612
在这里 我自然会想

00:07:15.612 --> 00:07:17.430
会不会是学习率太高了 对吧？

00:07:17.430 --> 00:07:21.655
当出现这种情况的时候 可能它未能收敛 谁知道呢 

00:07:21.654 --> 00:07:24.137
那么 让我们将学习率调低一些

00:07:24.137 --> 00:07:30.351
设置新值的一个好办法就是按数量级来调整

00:07:30.351 --> 00:07:35.087
我将它除以十 并重新初始化网络

00:07:35.088 --> 00:07:40.218
训练网络然后进行比较 让数据跑一会儿

00:07:40.218 --> 00:07:44.090
糟糕 好像又和之前的一样

00:07:44.089 --> 00:07:45.609
并没有任何改善

00:07:45.610 --> 00:07:47.060
现在我们让它训练一会儿

00:07:47.060 --> 00:07:50.009
同时聊聊我为什么要降低学习率

00:07:50.009 --> 00:07:54.159
如果你还记得 我们之前讲过 学习率也叫步长

00:07:54.160 --> 00:07:57.520
即每次采取多大的一步 去试着减小误差

00:07:58.720 --> 00:08:02.810
发生这种情况的一个可能的合理解释就是

00:08:02.810 --> 00:08:06.790
过伸 (overshoot) 因为步长过大

00:08:07.990 --> 00:08:12.170
所以它并没有比一开始更接近正确答案

00:08:12.170 --> 00:08:15.954
另一种：未及 (undershoot) 是指网络训练的速度非常非常慢

00:08:15.954 --> 00:08:20.403
它确实是在一点点改进 但它改得实在太缓慢了

00:08:20.403 --> 00:08:23.690
看起来就像压根没有变化一样

00:08:23.690 --> 00:08:27.609
它一直徘徊在 50% 左右 真的让人担忧

00:08:27.610 --> 00:08:31.182
我们已经训练了 20% 了 这时应该能看出一些变化了

00:08:31.182 --> 00:08:34.524
那么我们取消进程 再来一次

00:08:34.524 --> 00:08:36.873
看看这个

00:08:36.873 --> 00:08:37.820
再重复一遍

00:08:40.283 --> 00:08:41.932
好的 在 40% 左右徘徊

00:08:43.826 --> 00:08:46.533
很好

00:08:46.533 --> 00:08:47.481
加油

00:08:47.481 --> 00:08:48.449
有意思

00:08:48.450 --> 00:08:54.280
最终 这些类型的指标看起来非常有娱乐性

00:08:54.279 --> 00:08:57.709
我真的有点惊讶 居然一点效果都没有

00:08:59.120 --> 00:09:02.509
哦 哦 开始了 它开始学到一点了

00:09:02.509 --> 00:09:03.809
这是个好兆头 对吧？

00:09:03.809 --> 00:09:07.849
它开始找到相关性了 但速度仍然很慢 不仅如此

00:09:07.850 --> 00:09:10.870
每秒处理评论量的速度也很慢

00:09:10.870 --> 00:09:13.610
每秒只能处理 100 个评论

00:09:13.610 --> 00:09:16.840
并没有快速收敛 对吧？

00:09:16.840 --> 00:09:18.980
我可以继续调低学习率

00:09:18.980 --> 00:09:21.730
事实上 学习率越低

00:09:21.730 --> 00:09:23.409
就学得越慢了 对吧？

00:09:23.409 --> 00:09:26.959
如果是之前的过伸 我可以继续调低学习率

00:09:26.960 --> 00:09:30.110
所以 我在这里可以做的一件事是 继续调整学习率

00:09:30.110 --> 00:09:31.899
我可以一整天都做这个

00:09:31.899 --> 00:09:34.059
获得增量改进

00:09:34.059 --> 00:09:39.169
但有点过早了 我们都没做优化 

00:09:39.169 --> 00:09:43.659
只要对我们神经网络的再评估提出一些框架性的问题

00:09:43.659 --> 00:09:45.029
比如 我们能否重新解构问题

00:09:45.029 --> 00:09:47.809
让相关性显示得更清楚明显？

00:09:47.809 --> 00:09:51.149
现在 我返回到上面这里

00:09:51.149 --> 00:09:55.079
这是我们进行设置的地方 我们对单词计数 将它们放在这里

00:09:55.080 --> 00:09:57.450
然后用它们进行预测

00:09:57.450 --> 00:10:01.990
是不是它目前的设置对网络来说太难了

00:10:01.990 --> 00:10:06.350
它的确在收敛 但速度不是很快

00:10:06.350 --> 00:10:12.040
有没有什么办法 可以让网络更轻易地

00:10:12.039 --> 00:10:15.959
识别那些经过验证的单词  在我们的……

00:10:15.960 --> 00:10:18.790
不是在这些列表中 这些是原始计数

00:10:18.789 --> 00:10:22.799
在上面这里（pos_neg_ratios） 对 使网络能更容易找到它们

00:10:22.799 --> 00:10:25.419
这里 我通常会做两件事

00:10:25.419 --> 00:10:29.599
第一件是：试着修改一些东西 看看是否有用 

00:10:29.600 --> 00:10:34.290
另一件是：进一步搞清楚到底发生了什么？ 研究几个训练样例

00:10:34.289 --> 00:10:38.179
看一看 是否我设想的模式真的在起作用

00:10:38.179 --> 00:10:41.569
还是我本身的逻辑就有问题

00:10:41.570 --> 00:10:44.500
十有八九 当训练不正确的时候 往往意味着

00:10:44.500 --> 00:10:47.039
有一些简单的东西被我弄反了

00:10:47.039 --> 00:10:48.799
而不是什么大的复杂变化

00:10:48.799 --> 00:10:51.729
但有时候 也需要大动干戈

00:10:51.730 --> 00:10:55.750
它仍然在非常快地训练

00:10:55.750 --> 00:10:59.029
如果我们来分析它 有时候它刚开始

00:10:59.029 --> 00:11:01.769
会训练得很快 然后势头减退

00:11:01.769 --> 00:11:05.225
我不怎么看到它超过 61% 62%

00:11:05.225 --> 00:11:11.180
大概就在这左右 不知道它是不是还在继续训练

00:11:11.179 --> 00:11:13.118
好的

00:11:13.119 --> 00:11:14.149
那么 现在我来提出我的问题

00:11:14.149 --> 00:11:14.236
好

00:11:14.236 --> 00:11:15.399
我该如何简化它？

00:11:16.600 --> 00:11:19.050
哪些是训练数据中的信号？

00:11:19.049 --> 00:11:21.149
哪些是训练数据中的噪音？

00:11:22.360 --> 00:11:27.320
这将作为我们下一节的话题 

00:11:27.320 --> 00:11:30.410
我们将分析它 尝试能不能加快训练速度

00:11:30.409 --> 00:11:32.169
你也可以让这个训练继续下去

00:11:32.169 --> 00:11:34.759
不过我觉得 它正确率不会再提高多少了

00:11:34.759 --> 00:11:37.539
我真的认为 我们肯定会构建一个更棒的分类器

00:11:37.539 --> 00:11:37.849
马上就好

