WEBVTT
Kind: captions
Language: zh-CN

00:00:00.550 --> 00:00:05.232
好的 在这部分 我们将讨论信号与噪音

00:00:05.232 --> 00:00:09.839
神经网络的任务 就是寻找相关性 

00:00:09.839 --> 00:00:10.580
而神经网络可以做得非常好

00:00:10.580 --> 00:00:14.449
不过现在 我们又得谈到如何设计问题

00:00:14.449 --> 00:00:17.289
让神经网络能发挥最大优势 可以训练

00:00:17.289 --> 00:00:20.890
并理解最复杂的模式

00:00:20.890 --> 00:00:24.810
上一节 我们看到的是

00:00:24.809 --> 00:00:27.109
它的确训练速度不怎么快

00:00:27.109 --> 00:00:31.600
似乎 有些地方信号更多 比如 excellent 与 terrible 相对

00:00:31.600 --> 00:00:33.929
moving 对其他什么词

00:00:33.929 --> 00:00:38.445
可以看到 有些词是非常正面或负面的

00:00:38.445 --> 00:00:42.409
似乎作为人类 我只看到这些文本 

00:00:42.409 --> 00:00:46.429
只用这些词汇 预测准确率就能达到 60% 以上

00:00:46.429 --> 00:00:49.659
我们现在要做的是回到……

00:00:49.659 --> 00:00:52.487
先不做一些花里胡哨的、高级的正则化

00:00:52.487 --> 00:00:55.335
而是先回到数据本身

00:00:55.335 --> 00:00:58.120
数据是金子所在的地方

00:00:58.119 --> 00:01:01.539
而神经网络就像是一台挖掘机 我们用它

00:01:01.539 --> 00:01:02.820
来将所有的金子挖出来

00:01:02.820 --> 00:01:06.629
但如果我们挖不到太多金子 尤其是在一开始

00:01:06.629 --> 00:01:08.349
问题很可能不在于挖掘机

00:01:08.349 --> 00:01:13.259
而在于我们挖掘的位置 选择的地块

00:01:13.260 --> 00:01:14.540
以及操纵它的方式

00:01:14.540 --> 00:01:18.935
那么 我想在这里谈论的是噪音与信号

00:01:18.935 --> 00:01:19.926
因为这就是关键所在

00:01:19.926 --> 00:01:23.656
这里指的是 我们有海量的数据 我们知道存在某种模式

00:01:23.656 --> 00:01:26.200
我们希望神经网络可以找到它

00:01:26.200 --> 00:01:30.740
之前 当我们创建这个 update_input_layer 时 完全靠运气

00:01:30.739 --> 00:01:32.039
我在这里看到了 18

00:01:32.040 --> 00:01:34.710
当时 我有点惊讶

00:01:34.709 --> 00:01:37.069
哇 真的很高

00:01:37.069 --> 00:01:41.289
从我的角度想想 如果这里出现一个 18 会是什么情况？

00:01:41.290 --> 00:01:46.040
那么 对于这个正向传播 这是一个加权求和

00:01:46.040 --> 00:01:49.215
来自输入层的四个权重的加权和 对吧？

00:01:49.215 --> 00:01:54.719
值 1 分别乘以 4 个权值   这个 1 同样乘以 4 个权值

00:01:54.719 --> 00:01:57.019
然后这两个向量在这里相加

00:01:57.019 --> 00:01:58.554
它是一个加权和

00:01:58.555 --> 00:02:02.877
它实际上是加权和的加权和 但不管了

00:02:02.876 --> 00:02:05.719
这个向量就是 horrible 的特征

00:02:05.719 --> 00:02:09.270
我这里说的向量 是指权值组成的列表 也就是这个权值

00:02:09.270 --> 00:02:14.895
这个权值 这个还有这个 它们对这四个节点施加一定的影响

00:02:14.895 --> 00:02:18.164
[笑] 有意思的是 它们两者有点互相影响

00:02:18.163 --> 00:02:23.061
这个值有多大 就会影响这些权重

00:02:23.062 --> 00:02:27.370
对这个隐藏层的控制力有多强

00:02:27.370 --> 00:02:31.659
而这些权重决定了这个输入值对隐藏层的影响力有多大

00:02:31.659 --> 00:02:34.530
两者互乘 互相关联

00:02:34.530 --> 00:02:37.370
以这种方式相互影响

00:02:37.370 --> 00:02:39.930
但是 如果这个乘以 18

00:02:39.930 --> 00:02:44.860
而这个乘以 1 那么 这个将占主导地位

00:02:44.860 --> 00:02:45.190
因为

00:02:45.189 --> 00:02:49.800
这个向量一定会等于这四个权值

00:02:49.800 --> 00:02:54.710
给 horrible 乘以 18 就像是这些节点的能量占比

00:02:54.710 --> 00:02:58.530
主要将被这个词占据

00:02:58.530 --> 00:03:01.669
我当时看到这个 就想 哪个词被 18 加权了？

00:03:01.669 --> 00:03:02.913
如果我们看一下word2index

00:03:06.668 --> 00:03:09.950
抱歉 应该是 vocab[0] 因为这是第零个位置

00:03:09.950 --> 00:03:16.947
或者是 review.vocab 是 reviews.vocab 吗？

00:03:16.948 --> 00:03:19.424
我可能把它删了

00:03:19.424 --> 00:03:20.181
回到这里

00:03:25.131 --> 00:03:28.580
哦 是list(vocab)

00:03:28.580 --> 00:03:29.193
好的 对了

00:03:29.193 --> 00:03:30.762
这个词就是 “空格”

00:03:30.762 --> 00:03:34.709
[笑] 当我将词汇标记化以后 其中一个词就是一个“空格”而已

00:03:34.709 --> 00:03:37.750
再看这儿 居然有 18 个“空格”

00:03:37.750 --> 00:03:39.900
如果只有一个 那还好 神经网络还能够自我分辨

00:03:39.900 --> 00:03:43.719
但是 这里 向量表示最多的就是毫无信息的“空格”

00:03:43.719 --> 00:03:47.300
但是这些词 这些词 权重都被弱化了 对吧？

00:03:47.300 --> 00:03:49.675
我再看看其他的

00:03:49.675 --> 00:03:57.872
比如说 review.split(“ “)

00:03:57.872 --> 00:04:02.082
应该是 reviews[0].split(“ “)

00:04:02.081 --> 00:04:03.043
好

00:04:04.795 --> 00:04:07.001
空格 空格

00:04:07.002 --> 00:04:09.455
看看 都是空格

00:04:09.455 --> 00:04:12.215
第一点 可能有一些标记化的错误 但是

00:04:12.215 --> 00:04:15.391
这里有一大堆句号 有很多句号

00:04:15.391 --> 00:04:17.115
我想知道它的分布是什么

00:04:17.115 --> 00:04:21.562
那么单个 review_counter = Counter() 对吧？

00:04:21.562 --> 00:04:27.038
所以 for word in reviews[0].split(“ “):

00:04:27.038 --> 00:04:31.877
review_counter[word] += 1

00:04:31.877 --> 00:04:33.120
所以 再用一次计数器

00:04:33.120 --> 00:04:34.210
我喜欢用计数器

00:04:34.211 --> 00:04:37.949
review_counter.most_common() 显示一下

00:04:37.949 --> 00:04:40.365
哇塞 看看

00:04:40.365 --> 00:04:42.792
快看看

00:04:42.791 --> 00:04:46.379
使用频率最高的这些词 都与情绪无关

00:04:47.980 --> 00:04:51.480
下面应该有一些情绪词

00:04:51.480 --> 00:04:55.230
insightful 在这里

00:04:55.230 --> 00:04:59.240
但如果你看看这个

00:05:00.639 --> 00:05:06.719
评论里大部分词都是一些毫无价值的凑数词 像 the、to、I、is、of、a

00:05:06.720 --> 00:05:12.472
而这种加权 使它对隐藏层产生了主导影响

00:05:12.471 --> 00:05:15.129
而隐藏层正是输出层赖以预测的

00:05:15.129 --> 00:05:16.290
数据来源

00:05:16.290 --> 00:05:18.360
如果隐藏层不能提供丰富的信息

00:05:18.360 --> 00:05:20.550
输出层就难为无米之炊了

00:05:20.550 --> 00:05:24.175
现在我想 我们之前决定一开始就进行计数

00:05:24.175 --> 00:05:29.939
或许这不是个好主意 因为计值没有突显信号

00:05:29.939 --> 00:05:33.180
看看这些计数 它们似乎突显的是噪音

00:05:33.180 --> 00:05:36.209
我这里说的突显 是指赋予最多的权重

00:05:36.209 --> 00:05:38.419
神经网络只是关于一堆权重和函数

00:05:38.420 --> 00:05:42.585
例如 你取这些值 赋予新的权重

00:05:42.584 --> 00:05:45.603
到这四个节点 然后运行一个函数

00:05:45.603 --> 00:05:48.719
在这个例子中 我们没有运行函数 但它是线性的

00:05:48.720 --> 00:05:50.870
然后再重新加权 运行一个函数

00:05:50.870 --> 00:05:52.360
就得到了预测值 对吧？

00:05:52.360 --> 00:05:55.810
但是如果加权没有用 通过我们创建的输入数据

00:05:55.810 --> 00:05:58.310
会难以找到真正的信号

00:05:58.310 --> 00:05:58.769
这些都是噪音

00:05:58.769 --> 00:06:01.505
这意味着 我们对问题的设计本身

00:06:01.505 --> 00:06:03.393
添加了大量噪音

00:06:03.394 --> 00:06:08.076
因为在这些权重中 神经网络要学习的是

00:06:08.076 --> 00:06:09.571
嘿 句号 安静点

00:06:09.571 --> 00:06:14.150
嘿 空格 安静一下 the、to、i、high、is、of、a

00:06:14.149 --> 00:06:15.039
都别吱声

00:06:15.040 --> 00:06:17.073
我想听到的是 insightful

00:06:17.072 --> 00:06:19.091
我要听 welcome

00:06:19.091 --> 00:06:22.930
我想听到其他积极的词汇 因为这是一条正面评论

00:06:22.930 --> 00:06:26.504
我们也可以看看负面评论

00:06:26.504 --> 00:06:30.975
但是 这个神经网络试图让所有不相关的单词都安静下来

00:06:30.975 --> 00:06:34.300
然后用心倾听那些真正相关的单词

00:06:34.300 --> 00:06:38.520
而我们并没有在帮助它 因为让最大的权重赋予了

00:06:38.519 --> 00:06:39.129
最高频的内容

00:06:39.129 --> 00:06:41.961
所以 我觉得我们应该试着消除这一点

00:06:41.961 --> 00:06:47.790
在项目 4 中 我们会尝试这么做

00:06:47.790 --> 00:06:49.890
我先来描述一下我们要做的事情

00:06:49.889 --> 00:06:54.644
首先 我们将利用

00:06:54.644 --> 00:06:59.958
项目 3 中完成的

00:06:59.958 --> 00:07:03.040
代码

00:07:03.040 --> 00:07:05.578
然后来到项目 4 也就是

00:07:05.577 --> 00:07:08.119
项目 4：减少输入数据中的噪音

00:07:08.120 --> 00:07:09.009
对吧？

00:07:09.009 --> 00:07:11.779
我们将使用这个网络 然后思考：

00:07:11.779 --> 00:07:16.409
如何修改这个网络 让它不再按出现次数加权呢？

00:07:18.029 --> 00:07:19.459
如果我们不再按出现次数加权

00:07:19.459 --> 00:07:22.039
这意味着 它的值将总是 1 或 0

00:07:22.040 --> 00:07:23.730
这样

00:07:23.730 --> 00:07:26.410
它只是表示是否出现了该词汇

00:07:27.600 --> 00:07:30.692
如果这样做 应该会有用

00:07:30.692 --> 00:07:33.554
这些句号和 the、and、to、I 还会在这里

00:07:33.553 --> 00:07:36.563
神经网络需要抉择哪些词最重要

00:07:36.564 --> 00:07:40.205
但它无需再给句号乘以 27

00:07:40.204 --> 00:07:44.769
即在句号进入隐藏层时 用 27 乘以权重 

00:07:44.769 --> 00:07:48.942
这会遮盖许多其他信号 而如果我们只用 1 和 0

00:07:48.942 --> 00:07:53.579
以二进制方式来表示 就会大大减少噪音

00:07:53.579 --> 00:07:55.240
使神经元更容易做出判断

00:07:55.240 --> 00:07:59.925
我觉得 在这里 改起来应该很容易

00:07:59.925 --> 00:08:02.430
我就在这个项目中操作

00:08:02.430 --> 00:08:05.894
所以 这是我们的 update_input_layer

00:08:07.160 --> 00:08:11.852
我们这里不做累加 而是去掉加号

00:08:11.851 --> 00:08:15.945
将它设为等于 1 所以对于 layer_0 中的每个值

00:08:15.946 --> 00:08:18.639
如果该词汇存在 就让它等于 1

00:08:18.639 --> 00:08:20.670
好的 我们重新构建它

00:08:20.670 --> 00:08:24.030
利用上面的训练值

00:08:25.620 --> 00:08:26.298
我们会使用原来那些

00:08:29.374 --> 00:08:32.340
我们也需要训练值

00:08:32.340 --> 00:08:36.009
复制这个 删掉那个

00:08:36.009 --> 00:08:40.029
在下面这里记录它

00:08:40.029 --> 00:08:42.120
创建我们的网络

00:08:43.320 --> 00:08:47.773
这实际上是我们新的类 然后点击“训练”

00:08:47.773 --> 00:08:50.753
快看 快看

00:08:50.753 --> 00:08:55.704
2% 的进度后已经达到 60% 准确度了

00:08:55.703 --> 00:08:58.549
真是惊人的进步 快看

00:08:58.549 --> 00:09:04.189
所以我们通过摆脱这种加权 消除了很多噪音

00:09:05.190 --> 00:09:07.830
神经网络也很快地找到了相关性

00:09:07.830 --> 00:09:08.964
快看 70%

00:09:08.964 --> 00:09:12.021
而我们的训练进度才到 9%

00:09:12.020 --> 00:09:17.134
这就是增加信号和减少噪音的精髓所在

00:09:17.134 --> 00:09:20.408
那就是让信号在你的神经网络显得更明显

00:09:20.408 --> 00:09:23.196
使神经网络可以工作起来 正确处理信号

00:09:23.196 --> 00:09:27.951
以有意思的方式来结合信号 寻找更复杂的模式

00:09:27.951 --> 00:09:30.049
而你就可以摆脱训练数据中的噪音

00:09:30.049 --> 00:09:35.149
我们可以花上几天时间在这里调整我们的

00:09:35.149 --> 00:09:38.870
阿尔法 试着调低一点 让训练速度更慢一点

00:09:38.870 --> 00:09:42.552
但在现实中 如果我们能摆脱这些讨厌的噪音

00:09:42.552 --> 00:09:47.029
就可以快速取得巨大进步 

00:09:47.029 --> 00:09:50.259
因为我们在训练神经网络做些有趣的事情

00:09:50.259 --> 00:09:52.367
有趣的事不是指要忽略句号

00:09:52.368 --> 00:09:57.129
而是指识别哪些词有相关性

00:09:57.129 --> 00:10:00.080
识别哪些词的组合具有相关性

00:10:00.080 --> 00:10:01.530
这是我们想让神经网络做的事情

00:10:01.529 --> 00:10:03.000
发现有意思的表达

00:10:03.000 --> 00:10:06.440
在隐藏层进行有趣的操作

00:10:06.440 --> 00:10:11.390
真正理解评论中所用的词汇

00:10:13.580 --> 00:10:14.700
这是我们希望看到的

00:10:14.700 --> 00:10:16.850
看 接近 80% 了

00:10:16.850 --> 00:10:18.960
那么 我会让它继续训练

00:10:18.960 --> 00:10:23.546
你自己也可以大胆地训练试试

00:10:23.546 --> 00:10:27.564
看起来还不错

00:10:27.563 --> 00:10:31.370
我注意到的另外一点…… 可能因为这是一个视频

00:10:31.370 --> 00:10:33.620
我希望这个能训练得更快一些

00:10:33.620 --> 00:10:36.649
接下来我希望做的是探寻

00:10:36.649 --> 00:10:38.309
神经网络内部 理解所发生的一切

00:10:38.309 --> 00:10:41.179
看我们能不能进一步提高速度

00:10:41.179 --> 00:10:43.409
不过现在 我们就继续讲课 让它继续训练吧

