WEBVTT
Kind: captions
Language: pt-BR

00:00:00.387 --> 00:00:04.324
Depois da 1ª onda de redes neurais
artificiais, nos anos 1980,

00:00:04.357 --> 00:00:07.901
ficou claro que as redes feedforward
são limitadas,

00:00:07.934 --> 00:00:11.343
já que elas não conseguem capturar
dependências temporais,

00:00:11.376 --> 00:00:16.186
que, como dissemos, são dependências
que mudam com o tempo.

00:00:16.885 --> 00:00:19.130
Modelar dados temporais
é crucial

00:00:19.163 --> 00:00:21.257
na maioria dos programas
do mundo real,

00:00:21.290 --> 00:00:24.603
visto que os sinais naturais,
como a fala e o vídeo,

00:00:24.636 --> 00:00:26.649
têm propriedades
de variação de tempo

00:00:26.682 --> 00:00:30.281
e se caracterizam por ter
dependências ao longo do tempo.

00:00:30.910 --> 00:00:34.716
Aliás, as redes neurais biológicas
têm conexões recorrentes,

00:00:34.749 --> 00:00:38.787
então aplicar recorrência às redes
neurais feedforward artificiais

00:00:38.820 --> 00:00:40.455
fez todo sentido.

00:00:41.012 --> 00:00:44.250
A primeira tentativa de adicionar
memória às redes neurais

00:00:44.283 --> 00:00:48.911
foram as redes neurais
com atraso de tempo, ou TDNNs.

00:00:48.944 --> 00:00:51.715
Nas TDNNs, as entradas
de timesteps passados

00:00:51.748 --> 00:00:54.307
foram introduzidas
na entrada da rede,

00:00:54.340 --> 00:00:57.200
alterando as verdadeiras
entradas externas.

00:00:57.233 --> 00:01:00.198
Isso trouxe a clara vantagem
de permitir que a rede

00:01:00.231 --> 00:01:02.895
visse além
do timestep atual,

00:01:02.928 --> 00:01:05.619
mas também trouxe
uma desvantagem inegável,

00:01:05.652 --> 00:01:08.195
já que as dependências temporais
ficaram restritas

00:01:08.228 --> 00:01:10.461
à janela do tempo escolhido.

00:01:11.023 --> 00:01:12.260
As RNNs simples,

00:01:12.293 --> 00:01:15.761
conhecidas também como
redes de Elman e redes de Jordan,

00:01:15.794 --> 00:01:17.264
surgiram em seguida.

00:01:17.297 --> 00:01:19.802
Falaremos de todas elas
mais tarde na aula.

00:01:20.116 --> 00:01:22.298
Foi constatado,
no início dos anos 1990,

00:01:22.331 --> 00:01:24.192
que todas essas redes sofrem

00:01:24.225 --> 00:01:27.070
do chamado "Problema
da Dissipação do Gradiente",

00:01:27.103 --> 00:01:29.046
no qual as contribuições
de informação

00:01:29.079 --> 00:01:31.824
deterioraram-se geometricamente
ao longo do tempo.

00:01:31.857 --> 00:01:36.831
Então capturar relações contendo
mais de 8 ou 10 passos para trás

00:01:36.864 --> 00:01:39.171
era praticamente impossível.

00:01:39.204 --> 00:01:43.418
Apesar da elegância dessas redes,
todas tinham essa grande falha.

00:01:44.084 --> 00:01:47.843
Nós discutiremos melhor o Problema
da Dissipação do Gradiente em breve.

00:01:47.876 --> 00:01:50.447
Você pode ver mais informações
sobre esse assunto

00:01:50.480 --> 00:01:52.121
logo depois deste vídeo.

00:01:52.514 --> 00:01:53.840
Em meados dos anos 1990,

00:01:53.873 --> 00:01:58.532
as redes Long Short-Term
Memory Cells, ou LSTMs,

00:01:58.565 --> 00:02:01.766
foram inventadas para resolver
exatamente este problema.

00:02:01.799 --> 00:02:06.119
A grande novidade das LSTMs
era a ideia de que alguns sinais,

00:02:06.152 --> 00:02:08.163
que chamamos
de variáveis de estado,

00:02:08.196 --> 00:02:11.083
podem ser mantidos fixos
através de portões

00:02:11.116 --> 00:02:15.732
e reintroduzidos ou não
em um momento apropriado no futuro.

00:02:15.765 --> 00:02:20.152
Deste modo, intervalos de tempo
arbitrários podem ser representados

00:02:20.185 --> 00:02:23.595
e dependências temporais
podem ser capturadas.

00:02:23.628 --> 00:02:25.697
Não se assuste
com este esboço.

00:02:25.730 --> 00:02:30.052
Veremos todos os detalhes das LSTMs
mais adiante na aula.

00:02:30.085 --> 00:02:31.875
Outras versões da LSTM,

00:02:31.908 --> 00:02:35.832
como as Gated Recurrent Networks,
ou GRUs,

00:02:35.865 --> 00:02:37.606
enriqueceram ainda mais
este tema

00:02:37.639 --> 00:02:40.717
e hoje representam
mais uma abordagem principal

00:02:40.750 --> 00:02:42.549
para entender as RNNs.

