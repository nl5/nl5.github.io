WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.940
我希望你现在更有信心

00:00:02.940 --> 00:00:06.150
更加深入地了解循环神经网络

00:00:06.150 --> 00:00:09.240
不过我们如何训练这些网络呢？

00:00:09.240 --> 00:00:13.750
我们怎样找到一组好的权重 可以最小化误差呢？

00:00:13.750 --> 00:00:18.094
你会发现我们的训练框架与之前见到的类似

00:00:18.094 --> 00:00:21.835
但其中的反向传播算法中具有轻微改变

00:00:21.835 --> 00:00:27.765
训练循环神经网络时 我们使用基于时间的反向传播算法

00:00:27.765 --> 00:00:32.475
为了简便 我们决定 从此刻起

00:00:32.475 --> 00:00:35.460
不管什么时候提到偏导数

00:00:35.460 --> 00:00:37.649
我都会简称为导数

00:00:37.649 --> 00:00:40.589
因为我会不断涉及到这些

00:00:40.590 --> 00:00:42.630
在我开始前

00:00:42.630 --> 00:00:44.140
有个小小的提醒

00:00:44.140 --> 00:00:47.035
如果你觉得记笔记很有效

00:00:47.034 --> 00:00:49.233
那就继续使用这个方法

00:00:49.234 --> 00:00:53.065
为了更好地理解基于时间的反向传播算法

00:00:53.064 --> 00:00:55.420
我们需要一些符号定义

00:00:55.420 --> 00:00:59.825
在之前的视频中我们见到过它们 但是需要再次强调一下

00:00:59.825 --> 00:01:03.380
通过应用激活函数 如双曲正切

00:01:03.380 --> 00:01:07.665
对状态向量 St 进行赋值

00:01:07.665 --> 00:01:13.580
得到输入向量 Xt 乘以权重矩阵 Wx 的乘积

00:01:13.579 --> 00:01:21.349
加上之前的状态向量 St-1 乘以权重矩阵 Ws 的乘积

00:01:21.349 --> 00:01:27.394
时间 t 的输出等于状态向量 St

00:01:27.394 --> 00:01:29.149
和权重矩阵 Wy 的乘积

00:01:29.150 --> 00:01:33.620
除非你也使用 Softmax 函数

00:01:33.620 --> 00:01:35.859
而损失函数

00:01:35.859 --> 00:01:40.609
即平方差 Et 等于时间 t 的理想输出和

00:01:40.609 --> 00:01:45.829
网络输出的方差 在之前的课程里

00:01:45.829 --> 00:01:50.429
你见过其他误差函数 如交叉熵损失函数

00:01:50.430 --> 00:01:56.030
不过为了保持一致 我们使用这节课程中相同误差

00:01:56.030 --> 00:01:58.373
在基于时间的反向传播中

00:01:58.373 --> 00:02:04.009
我们不会单独在特定时间 t 训练系统

00:02:04.010 --> 00:02:07.490
我们要在特定时间 t 训练系统

00:02:07.489 --> 00:02:11.570
也要考虑之前发生的所有内容

00:02:11.570 --> 00:02:17.004
例如假设我们位于时间步长 t 等于 3

00:02:17.004 --> 00:02:20.590
我们的平方差仍然与之前相同

00:02:20.590 --> 00:02:24.233
即理想输出和网络输出的方差

00:02:24.233 --> 00:02:27.170
在这个例子中时间 t 等于 3

00:02:27.169 --> 00:02:31.284
这是特定时间的折叠框架

00:02:31.284 --> 00:02:33.924
为了更新每个权重矩阵

00:02:33.925 --> 00:02:37.750
我们要找出时间 t 等于 3 时损失函数的导数

00:02:37.750 --> 00:02:42.159
作为整个权重矩阵的函数

00:02:42.159 --> 00:02:45.939
换句话说我们要使用梯度下降修改每个矩阵

00:02:45.939 --> 00:02:51.444
就像我们之前在前馈神经网络中一样

00:02:51.444 --> 00:02:57.829
不过另外我们还需要考虑之前的时间步长

00:02:57.830 --> 00:03:02.935
为了更好理解如何继续使用基于时间的反向传播过程

00:03:02.935 --> 00:03:05.460
我们可以展开这个模型

00:03:05.460 --> 00:03:07.409
接下来视频中将介绍所有这些内容

