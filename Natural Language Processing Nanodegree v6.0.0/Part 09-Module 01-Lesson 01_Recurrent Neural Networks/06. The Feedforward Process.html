<!-- udacimak v1.4.4 -->
<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="ie=edge" http-equiv="X-UA-Compatible"/>
  <title>
   The Feedforward Process
  </title>
  <link href="../assets/css/bootstrap.min.css" rel="stylesheet"/>
  <link href="../assets/css/plyr.css" rel="stylesheet"/>
  <link href="../assets/css/katex.min.css" rel="stylesheet"/>
  <link href="../assets/css/jquery.mCustomScrollbar.min.css" rel="stylesheet"/>
  <link href="../assets/css/styles.css" rel="stylesheet"/>
  <link href="../assets/img/udacimak.png" rel="shortcut icon" type="image/png">
  </link>
 </head>
 <body>
  <div class="wrapper">
   <nav id="sidebar">
    <div class="sidebar-header">
     <h3>
      Recurrent Neural Networks
     </h3>
    </div>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled components">
     <li class="">
      <a href="01. Introducing Ortal .html">
       01. Introducing Ortal
      </a>
     </li>
     <li class="">
      <a href="02. RNN Introduction.html">
       02. RNN Introduction
      </a>
     </li>
     <li class="">
      <a href="03. RNN History.html">
       03. RNN History
      </a>
     </li>
     <li class="">
      <a href="04. RNN Applications.html">
       04. RNN Applications
      </a>
     </li>
     <li class="">
      <a href="05. Feedforward Neural Network-Reminder.html">
       05. Feedforward Neural Network-Reminder
      </a>
     </li>
     <li class="">
      <a href="06. The Feedforward Process.html">
       06. The Feedforward Process
      </a>
     </li>
     <li class="">
      <a href="07. Feedforward Quiz.html">
       07. Feedforward Quiz
      </a>
     </li>
     <li class="">
      <a href="08. Backpropagation- Theory.html">
       08. Backpropagation- Theory
      </a>
     </li>
     <li class="">
      <a href="09. Backpropagation - Example (part a).html">
       09. Backpropagation - Example (part a)
      </a>
     </li>
     <li class="">
      <a href="10. Backpropagation- Example (part b).html">
       10. Backpropagation- Example (part b)
      </a>
     </li>
     <li class="">
      <a href="11. Backpropagation Quiz.html">
       11. Backpropagation Quiz
      </a>
     </li>
     <li class="">
      <a href="12.  RNN (part a).html">
       12.  RNN (part a)
      </a>
     </li>
     <li class="">
      <a href="13. RNN (part b).html">
       13. RNN (part b)
      </a>
     </li>
     <li class="">
      <a href="14. RNN-  Unfolded Model.html">
       14. RNN-  Unfolded Model
      </a>
     </li>
     <li class="">
      <a href="15. Unfolded Model Quiz.html">
       15. Unfolded Model Quiz
      </a>
     </li>
     <li class="">
      <a href="16. RNN- Example.html">
       16. RNN- Example
      </a>
     </li>
     <li class="">
      <a href="17. Backpropagation Through Time (part a).html">
       17. Backpropagation Through Time (part a)
      </a>
     </li>
     <li class="">
      <a href="18. Backpropagation Through Time (part b).html">
       18. Backpropagation Through Time (part b)
      </a>
     </li>
     <li class="">
      <a href="19. Backpropagation Through Time (part c).html">
       19. Backpropagation Through Time (part c)
      </a>
     </li>
     <li class="">
      <a href="20. BPTT Quiz 1.html">
       20. BPTT Quiz 1
      </a>
     </li>
     <li class="">
      <a href="21. BPTT Quiz 2.html">
       21. BPTT Quiz 2
      </a>
     </li>
     <li class="">
      <a href="22. BPTT Quiz 3.html">
       22. BPTT Quiz 3
      </a>
     </li>
     <li class="">
      <a href="23. Some more math.html">
       23. Some more math
      </a>
     </li>
     <li class="">
      <a href="24. RNN Summary.html">
       24. RNN Summary
      </a>
     </li>
     <li class="">
      <a href="25. From RNN to LSTM.html">
       25. From RNN to LSTM
      </a>
     </li>
     <li class="">
      <a href="26. Wrap Up.html">
       26. Wrap Up
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
   </nav>
   <div id="content">
    <header class="container-fluild header">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <div class="align-items-middle">
         <button class="btn btn-toggle-sidebar" id="sidebarCollapse" type="button">
          <div>
          </div>
          <div>
          </div>
          <div>
          </div>
         </button>
         <h1 style="display: inline-block">
          06. The Feedforward Process
         </h1>
        </div>
       </div>
      </div>
     </div>
    </header>
    <main class="container">
     <div class="row">
      <div class="col-12">
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h1 id="feedforward">
          Feedforward
         </h1>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          In this section we will look closely at the math behind the feedforward process. With the use of basic Linear Algebra tools, these calculations are pretty simple!
         </p>
         <p>
          If you are not feeling confident with linear combinations and matrix multiplications, you  can use the following links as a refresher:
         </p>
         <ul>
          <li>
           <a href="http://linear.ups.edu/html/section-LC.html" rel="noopener noreferrer" target="_blank">
            Linear Combination
           </a>
          </li>
          <li>
           <a href="https://en.wikipedia.org/wiki/Matrix_multiplication" rel="noopener noreferrer" target="_blank">
            Matrix Multiplication
           </a>
          </li>
         </ul>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          Assuming that we have a single hidden layer, we will need two steps in our calculations. The first will be calculating the value of the hidden states and the latter will be calculating the value of the outputs.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="" class="img img-fluid" src="img/screen-shot-2017-10-27-at-1.29.13-pm.png"/>
          <figcaption class="figure-caption">
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          Notice that both the hidden layer and the output layer are displayed as vectors, as they are both represented by more than a single neuron.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          Our first video will help you understand the first step-
          <strong>
           Calculating the value of the hidden states
          </strong>
          .
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
         <p>
          06 FeedForward A V7 Final
         </p>
        </h3>
        <video controls="">
         <source src="06. 06 FeedForward A V7 Final-4rCfnWbx8-0.mp4" type="video/mp4"/>
         <track default="false" kind="subtitles" label="zh-CN" src="06. 06 FeedForward A V7 Final-4rCfnWbx8-0.zh-CN.vtt" srclang="zh-CN"/>
         <track default="false" kind="subtitles" label="pt-BR" src="06. 06 FeedForward A V7 Final-4rCfnWbx8-0.pt-BR.vtt" srclang="pt-BR"/>
         <track default="true" kind="subtitles" label="en" src="06. 06 FeedForward A V7 Final-4rCfnWbx8-0.en.vtt" srclang="en"/>
        </video>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          As you saw in the video above, vector h' of the hidden layer will be calculated by multiplying the input vector with the weight matrix
          <span class="mathquill ud-math">
           W^{1}
          </span>
          the following way:
         </p>
         <p>
          <span class="mathquill ud-math">
           \bar{h'} = (\bar{x}  W^1 )
          </span>
         </p>
         <p>
          Using vector by matrix multiplication, we can look at this computation the following way:
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="_Equation 1_" class="img img-fluid" src="img/screen-shot-2017-12-05-at-11.55.58-am.png"/>
          <figcaption class="figure-caption">
           <p>
            <em>
             Equation 1
            </em>
           </p>
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          After finding
          <span class="mathquill ud-math">
           h'
          </span>
          , we need an activation function (
          <span class="mathquill ud-math">
           \Phi
          </span>
          ) to finalize the computation of the hidden layer's values. This activation function can be a Hyperbolic Tangent, a Sigmoid or a ReLU function. We can use the following two equations to express the final hidden vector
          <span class="mathquill ud-math">
           \bar{h}
          </span>
          :
         </p>
         <p>
          <span class="mathquill ud-math">
           \bar{h} = \Phi(\bar{x}  W^1 )
          </span>
         </p>
         <p>
          or
         </p>
         <p>
          <span class="mathquill ud-math">
           \bar{h} = \Phi(h')
          </span>
         </p>
         <p>
          Since
          <span class="mathquill ud-math">
           W_{ij}
          </span>
          <br/>
          represents the weight component in the weight matrix, connecting neuron
          <strong>
           i
          </strong>
          from the input to neuron
          <strong>
           j
          </strong>
          in the hidden layer, we can also write these calculations in the following way:
          <br/>
          (notice that in this example we have
          <em>
           n
          </em>
          inputs and only 3 hidden neurons)
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="Equation 2" class="img img-fluid" src="img/screen-shot-2017-10-27-at-6.29.49-pm.png"/>
          <figcaption class="figure-caption">
           <p>
            Equation 2
           </p>
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          More information on the activation functions and how to use them can be found
          <a href="https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions" rel="noopener noreferrer" target="_blank">
           here
          </a>
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          This next video will help you understand the second step-
          <strong>
           Calculating the values of the Outputs
          </strong>
          .
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
         <p>
          07 FeedForward B V3
         </p>
        </h3>
        <video controls="">
         <source src="06. 07 FeedForward B V3-kTYbTVh1d0k.mp4" type="video/mp4"/>
         <track default="true" kind="subtitles" label="en" src="06. 07 FeedForward B V3-kTYbTVh1d0k.en.vtt" srclang="en"/>
         <track default="false" kind="subtitles" label="pt-BR" src="06. 07 FeedForward B V3-kTYbTVh1d0k.pt-BR.vtt" srclang="pt-BR"/>
         <track default="false" kind="subtitles" label="zh-CN" src="06. 07 FeedForward B V3-kTYbTVh1d0k.zh-CN.vtt" srclang="zh-CN"/>
        </video>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          As you've seen in the video above, the process of calculating the output vector is mathematically similar to that of calculating the vector of the hidden layer. We use, again, a vector by matrix multiplication, which can be followed by an activation function. The vector is the newly calculated hidden layer and the matrix is the one connecting the hidden layer to the output.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="" class="img img-fluid" src="img/screen-shot-2017-10-30-at-10.54.50-am.png"/>
          <figcaption class="figure-caption">
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          Essentially, each new layer in an neural network is calculated by a vector by matrix multiplication, where the vector represents the inputs to the new layer and the matrix is the one connecting these new inputs to the next layer.
         </p>
         <p>
          In our example, the input vector is
          <span class="mathquill ud-math">
           \bar{h}
          </span>
          and the matrix is
          <span class="mathquill ud-math">
           W^2
          </span>
          , therefore
          <span class="mathquill ud-math">
           \bar{y}=\bar{h}W^2
          </span>
          . In some applications it can be beneficial to use a softmax function (if we want all output values to be between zero and 1, and their sum to be 1).
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="Equation 3" class="img img-fluid" src="img/screen-shot-2017-10-30-at-11.56.27-am.png"/>
          <figcaption class="figure-caption">
           <p>
            Equation 3
           </p>
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          The two error functions that are most commonly used are the
          <a href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener noreferrer" target="_blank">
           Mean Squared Error (MSE)
          </a>
          (usually used in regression problems) and the
          <a href="https://www.ics.uci.edu/~pjsadows/notes.pdf" rel="noopener noreferrer" target="_blank">
           cross entropy
          </a>
          (usually used in classification problems).
         </p>
         <p>
          In the above calculations we used a variation of the MSE.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          The next few videos will focus on the backpropagation process, or what we also call stochastic gradient decent with the use of the chain rule.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
      </div>
      <div class="col-12">
       <p class="text-right">
        <a class="btn btn-outline-primary mt-4" href="07. Feedforward Quiz.html" role="button">
         Next Concept
        </a>
       </p>
      </div>
     </div>
    </main>
    <footer class="footer">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <p class="text-center">
         udacity2.0 If you need the newest courses Plase add me wechat: udacity6
        </p>
       </div>
      </div>
     </div>
    </footer>
   </div>
  </div>
  <script src="../assets/js/jquery-3.3.1.min.js">
  </script>
  <script src="../assets/js/plyr.polyfilled.min.js">
  </script>
  <script src="../assets/js/bootstrap.min.js">
  </script>
  <script src="../assets/js/jquery.mCustomScrollbar.concat.min.js">
  </script>
  <script src="../assets/js/katex.min.js">
  </script>
  <script>
   // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });

      // scroll to first video on page loading
      if ($('video').length) {
        $('html,body').animate({ scrollTop: $('div.plyr').prev().offset().top});
      }

      // auto play first video: this may not work with chrome/safari due to autoplay policy
      if (players && players.length > 0) {
        players[0].play();
      }

      // scroll sidebar to current concept
      const currentInSideBar = $( "ul.sidebar-list.components li a:contains('06. The Feedforward Process')" )
      currentInSideBar.css( "text-decoration", "underline" );
      $("#sidebar").mCustomScrollbar('scrollTo', currentInSideBar);
    });
  </script>
 </body>
</html>
