WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:06.315
最后我们即将探讨循环神经网络 简称 RNN

00:00:06.315 --> 00:00:10.635
目前为止我们了解的知识都为这一刻做准备

00:00:10.634 --> 00:00:13.004
我们复习了前馈过程

00:00:13.005 --> 00:00:16.620
详细了解到反向传播过程

00:00:16.620 --> 00:00:20.539
这可以帮助你掌握接下来的一组视频

00:00:20.539 --> 00:00:22.445
正如我之前提到的

00:00:22.445 --> 00:00:25.295
如果你查询循环这个词的定义

00:00:25.295 --> 00:00:29.890
你会发现这个词语只是表示不断出现或重复出现

00:00:29.890 --> 00:00:34.495
那么为什么这些网络叫做循环神经网络呢？

00:00:34.494 --> 00:00:35.779
是因为利用循环神经网络

00:00:35.780 --> 00:00:41.350
我们在输入语句中为每个要素执行形同任务

00:00:41.350 --> 00:00:44.030
随后我们会多次见到这个概略图

00:00:44.030 --> 00:00:48.620
循环神经网络通过维护内部记忆单元

00:00:48.619 --> 00:00:52.534
即所谓的状态 (states)

00:00:52.534 --> 00:00:56.259
尝试解决采集信息和之前输入的需要

00:00:56.259 --> 00:01:01.832
许多应用具有时序依赖性

00:01:01.832 --> 00:01:08.754
这表示我们当前的输出不仅取决于当前输入

00:01:08.754 --> 00:01:14.515
还取决于过去输入的记忆单元

00:01:14.515 --> 00:01:16.370
例如这些情况下

00:01:16.370 --> 00:01:19.005
我们需要使用循环神经网络

00:01:19.004 --> 00:01:25.250
使用循环神经网络的典型例子是预测句子中的下一个词语

00:01:25.250 --> 00:01:31.879
通常需要观察前几个词语 而不只是当前的一个词语

00:01:31.879 --> 00:01:35.914
我们也提到过其他几类应用

00:01:35.915 --> 00:01:40.305
如情感分析 语音识别

00:01:40.305 --> 00:01:45.780
时间序列预测 自然语言处理和手势识别

00:01:45.780 --> 00:01:48.935
实际上 循环神经网络的应用不断兴起

00:01:48.935 --> 00:01:53.347
几乎每天都要应对新挑战

00:01:53.347 --> 00:01:57.295
那么我们应该如何看待这种全新神经网络呢？

00:01:57.295 --> 00:01:59.004
它的结构如何呢？

00:01:59.004 --> 00:02:00.045
训练和评估阶段

00:02:00.045 --> 00:02:02.295
如何发生变化呢？

00:02:02.295 --> 00:02:08.145
循环神经网络的原则与前馈神经网络相同

00:02:08.145 --> 00:02:12.840
所以我们花费大量时间回顾前馈神经网络的内容

00:02:12.840 --> 00:02:15.969
与前馈神经网络相同

00:02:15.969 --> 00:02:17.835
在循环神经网络中

00:02:17.835 --> 00:02:25.490
输入和输出可以是多对多 多对一和一对多

00:02:25.490 --> 00:02:29.814
不过循环神经网络和前馈神经网络

00:02:29.814 --> 00:02:32.995
存在两个根本区别

00:02:32.995 --> 00:02:38.830
第一个区别是我们定义输入和输出的方式不同

00:02:38.830 --> 00:02:42.190
除了每个时间步长中使用单一输入

00:02:42.189 --> 00:02:44.829
单一输出训练网络

00:02:44.830 --> 00:02:49.630
我们还可以使用序列进行训练 因为之前的输入也会产生影响

00:02:49.629 --> 00:02:54.879
第二个区别来自循环神经网络的记忆单元

00:02:54.879 --> 00:03:02.935
当前输入和神经元的激活作为下一个时间步长的输入

00:03:02.935 --> 00:03:06.045
在前馈神经网络中

00:03:06.044 --> 00:03:12.969
我们看到信息从输入流向输出 没有任何反馈

00:03:12.969 --> 00:03:16.014
现在前馈框架发生变化

00:03:16.014 --> 00:03:19.629
包括反馈或存储单元

00:03:19.629 --> 00:03:22.495
我们会考虑定义的存储

00:03:22.495 --> 00:03:25.134
作为隐藏层的输出

00:03:25.134 --> 00:03:30.969
也可以作为接下来训练步骤中向网络的其他输入

00:03:30.969 --> 00:03:35.560
我们不再使用 h 作为隐藏层的输出

00:03:35.560 --> 00:03:37.854
而是使用 S 表示状态

00:03:37.854 --> 00:03:40.509
代表含有存储的系统

00:03:40.509 --> 00:03:44.814
循环神经网络的基础框架称为简单循环神经网络

00:03:44.814 --> 00:03:49.180
即所谓的 Elman 网络

00:03:49.180 --> 00:03:50.980
注意在这个图解中这里

00:03:50.979 --> 00:03:53.319
我只使用两个输出

00:03:53.319 --> 00:03:56.799
你也可以有多个输出

00:03:56.800 --> 00:03:58.600
当然超过 2 个

00:03:58.599 --> 00:04:00.794
不过为了简化概略图

00:04:00.794 --> 00:04:03.704
我们目前只保留 2 个输出

00:04:03.705 --> 00:04:09.075
如果你具备工程或计算机科学背景

00:04:09.074 --> 00:04:10.799
这可能让你联想到

00:04:10.800 --> 00:04:16.020
综合了逻辑和存储的简单状态机

00:04:16.019 --> 00:04:19.799
输出不仅取决于外部输入

00:04:19.800 --> 00:04:24.180
还取决于存储单元中之前的输入

00:04:24.180 --> 00:04:27.280
从概念上来说 循环神经网络很类似

00:04:27.279 --> 00:04:29.159
不过我们例子中

00:04:29.160 --> 00:04:31.305
幸好系统可以自我训练

00:04:31.305 --> 00:04:36.720
学会如何最优化权重矩阵 实现网络

