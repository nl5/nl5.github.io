WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.485
这节课中 我们将重点关注循环神经网络

00:00:04.485 --> 00:00:07.185
或者简称为 RNN

00:00:07.184 --> 00:00:10.544
许多应用涉及时间依赖

00:00:10.544 --> 00:00:14.189
或基于时间的依赖 这是什么意思呢？

00:00:14.189 --> 00:00:19.634
这表示我们当前输出不仅取决于当前输入

00:00:19.635 --> 00:00:22.493
还依赖于过去的输入

00:00:22.492 --> 00:00:25.559
例如 如果我们想今晚做饭

00:00:25.559 --> 00:00:27.689
我昨天吃了披萨

00:00:27.690 --> 00:00:29.960
可能我应该考虑下沙拉

00:00:29.960 --> 00:00:33.030
从本质上来说 我们所得到的网络

00:00:33.030 --> 00:00:36.675
类似于之前见过的前馈神经网络

00:00:36.674 --> 00:00:39.334
可是包含存储

00:00:39.335 --> 00:00:40.960
你可能已经注意到

00:00:40.960 --> 00:00:43.270
在之前见到的应用中

00:00:43.270 --> 00:00:45.273
只有当前的输入非常重要

00:00:45.273 --> 00:00:49.520
例如对一个图片进行分类 这是不是一只猫？

00:00:49.520 --> 00:00:53.000
不过也许这不是静止的猫

00:00:53.000 --> 00:00:56.229
也许拍照时 猫在移动

00:00:56.229 --> 00:00:58.179
从单独这张照片来看

00:00:58.179 --> 00:01:02.894
很难判断它是走还是跑

00:01:02.895 --> 00:01:07.865
也许这是个极具才华的猫 站成滑稽的姿势 两只脚悬在空中

00:01:07.864 --> 00:01:10.854
当我们一帧帧观察这只猫时

00:01:10.855 --> 00:01:13.387
我们记起来之前看到的

00:01:13.387 --> 00:01:16.707
所以我们知道这只猫是静止还是走动的

00:01:16.707 --> 00:01:19.495
我们也可以区分走和跑

00:01:19.495 --> 00:01:22.918
但是机器也可以做到这样吗？

00:01:22.918 --> 00:01:27.960
循环神经网络 (RNN) 应运而生

00:01:27.959 --> 00:01:33.059
循环神经网络是人工神经网络

00:01:33.060 --> 00:01:35.385
可以采集时间依赖

00:01:35.385 --> 00:01:37.965
或称为基于时间的依赖

00:01:37.965 --> 00:01:41.320
如果你查询循环这个词的定义

00:01:41.319 --> 00:01:46.614
你会发现这个词语只是表示不断出现或重复出现

00:01:46.614 --> 00:01:50.250
那么为什么这些网络叫做循环神经网络呢？

00:01:50.250 --> 00:01:53.765
这是因为利用循环神经网络 我们在输入序列中

00:01:53.765 --> 00:01:57.325
对每个单元执行相同的任务

00:01:57.325 --> 00:02:01.049
这节课中我们首先介绍一下历史

00:02:01.049 --> 00:02:04.536
了解循环神经网络的演进非常有趣

00:02:04.536 --> 00:02:08.259
我们会提醒自己前馈网络是什么

00:02:08.259 --> 00:02:11.039
你会发现循环神经网络的理念

00:02:11.039 --> 00:02:14.965
与前馈网络的理念非常相似

00:02:14.965 --> 00:02:17.775
所以我们一旦明确掌握了基本原理

00:02:17.775 --> 00:02:21.180
就会很容易地理解下一步

00:02:21.180 --> 00:02:24.920
在关注前馈网络时 你会学习到

00:02:24.919 --> 00:02:29.299
非线性函数逼近 利用反向传播算法进行训练

00:02:29.300 --> 00:02:33.671
或称为随机梯度下降 以及估值

00:02:33.670 --> 00:02:36.343
你应该很熟悉这些内容

00:02:36.343 --> 00:02:40.120
当然我们重点要了解循环神经网络

00:02:40.120 --> 00:02:43.615
对于我们需要循环神经网络的原因 我已经提供了一个较好示例

00:02:43.615 --> 00:02:47.064
即上面猫的例子 不过还有其他许多应用

00:02:47.064 --> 00:02:49.425
我们也会关注其他应用

00:02:49.425 --> 00:02:53.785
我们会了解简单循环神经网络 即 Elman 网络

00:02:53.784 --> 00:02:56.319
学习如何训练网络

00:02:56.319 --> 00:02:58.989
我们也要明白简单循环神经网络的缺点

00:02:58.990 --> 00:03:04.840
以及如何利用长短期记忆网络避免这些缺点

00:03:04.840 --> 00:03:08.680
不要担心 你不必立即记住这些名称

00:03:08.680 --> 00:03:11.170
我们将会慢慢讲解每个概念

00:03:11.169 --> 00:03:14.839
随后详细探讨这些内容

