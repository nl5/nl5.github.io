WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.498
我们完成了第一步

00:00:02.498 --> 00:00:06.960
现在开始第二步 找到输出 y

00:00:06.960 --> 00:00:10.710
利用我们刚刚计算得出的值 h

00:00:10.710 --> 00:00:13.410
由于我们有多个输入

00:00:13.410 --> 00:00:15.720
y 也是向量

00:00:15.720 --> 00:00:17.538
我们有初始输入 h

00:00:17.538 --> 00:00:21.570
我们想要找出输出值 y

00:00:21.570 --> 00:00:27.875
从数学角度来说 这个想法相当于我们刚刚在第一步看到的

00:00:27.875 --> 00:00:30.000
现在我们有不同的输入

00:00:30.000 --> 00:00:34.140
把它们叫做 h 以及另一个权重矩阵

00:00:34.140 --> 00:00:36.125
把它叫做 W2

00:00:36.125 --> 00:00:38.784
输出是向量 y

00:00:38.784 --> 00:00:41.619
注意权重矩阵有三行

00:00:41.619 --> 00:00:44.459
因为隐藏层包括 3 个神经元

00:00:44.460 --> 00:00:48.630
因为我们只有两个输出 所以有两列

00:00:48.630 --> 00:00:52.605
另外我们通过矩阵乘法得到一个向量

00:00:52.604 --> 00:00:56.979
即向量 h 乘以权重矩阵 W2

00:00:56.979 --> 00:00:59.699
可以得到输出向量 y

00:00:59.700 --> 00:01:02.155
我们可以用一个简单方程式表示

00:01:02.155 --> 00:01:08.269
即 y = h*W 一旦我们得到输出

00:01:08.269 --> 00:01:11.789
就不再需要激活函数

00:01:11.790 --> 00:01:17.594
在一些应用中 使用例如 Softmax 函数是十分有帮助的

00:01:17.594 --> 00:01:19.769
即所谓的 σx

00:01:19.769 --> 00:01:24.239
如果我们希望输出值在 0 到 1 之间

00:01:24.239 --> 00:01:30.009
在看完这个视频后 你可以在下面的文字说明里了解更多关于这个课题的信息

00:01:30.010 --> 00:01:33.015
为了得到输出 y 的充分逼近

00:01:33.015 --> 00:01:36.045
我们需要不止一个隐藏层

00:01:36.045 --> 00:01:38.400
甚至可能需要 10 个或更多

00:01:38.400 --> 00:01:42.450
在这幅图中 我使用常规数字 P

00:01:42.450 --> 00:01:46.269
每层神经元的数量各不相同

00:01:46.269 --> 00:01:48.104
正如我之前提到的

00:01:48.105 --> 00:01:49.890
甚至可能有几千个

00:01:49.890 --> 00:01:52.890
从本质上来说 你看到这些神经元

00:01:52.890 --> 00:01:56.204
如同堆叠的建筑砖块或乐高积木一样

00:01:56.204 --> 00:01:58.739
那么如何从数学角度完成这个呢？

00:01:58.739 --> 00:02:00.420
与之前一样

00:02:00.420 --> 00:02:02.924
通过矩阵乘法得到简单向量

00:02:02.924 --> 00:02:05.415
接着利用激活函数

00:02:05.415 --> 00:02:08.324
其中这个向量表示输入

00:02:08.324 --> 00:02:13.884
这个矩阵表示一层连接到下一层的权重

00:02:13.884 --> 00:02:16.525
为了归纳这个模型

00:02:16.526 --> 00:02:19.550
我们观察一个随机的 K F 层

00:02:19.550 --> 00:02:23.330
K F 层的权重 Wij

00:02:23.330 --> 00:02:24.950
和 K+1

00:02:24.949 --> 00:02:27.709
对应 I F 输入

00:02:27.710 --> 00:02:31.140
然后得到 J F 输出

00:02:31.139 --> 00:02:33.544
这里你可能需要用一支笔

00:02:33.544 --> 00:02:37.119
因为这些是重要的数学推导

00:02:37.120 --> 00:02:38.490
当你进行数学运算时

00:02:38.490 --> 00:02:42.800
暂停视频 在你的注释里写下推导

00:02:42.800 --> 00:02:46.189
在整个课程中尽量这样做

00:02:46.189 --> 00:02:48.789
我们来处理 K 层

00:02:48.789 --> 00:02:54.875
因为输入 x 和 K+1 层作为隐藏层的输出 h

00:02:54.875 --> 00:03:02.794
我们有 n 个输入项 x 和 m 个输出项 h

00:03:02.794 --> 00:03:06.199
顺便说一句 如果我们面对的不是输入

00:03:06.199 --> 00:03:09.889
而是上个隐藏层的输出

00:03:09.889 --> 00:03:14.044
只有我们选择利用的字母才会发生变化

00:03:14.044 --> 00:03:16.609
不过计算还是一样的

00:03:16.610 --> 00:03:19.015
为了简化符号

00:03:19.014 --> 00:03:26.834
我们还是保留 x h1 是对激活函数求和的输出

00:03:26.835 --> 00:03:30.930
其中总和是每个输入 xi 乘以

00:03:30.930 --> 00:03:35.510
对应的权重 Wi1 的结果

00:03:35.509 --> 00:03:41.120
同样 hm 是对激活函数求和的输出

00:03:41.120 --> 00:03:45.490
其中总和是每个输入 xi 乘以

00:03:45.490 --> 00:03:49.534
对应的权重 Wim 的结果

00:03:49.534 --> 00:03:53.194
例如有三个输入

00:03:53.194 --> 00:03:55.954
我们想要计算 h1

00:03:55.955 --> 00:04:02.590
这是线性组合后激活函数的输出

00:04:02.590 --> 00:04:08.500
这些单一要素的计算有利于理解反向传播

00:04:08.500 --> 00:04:11.550
这就是我们也要理解它们的原因

00:04:11.550 --> 00:04:13.930
不过与之前一样 我们也会观察

00:04:13.930 --> 00:04:18.811
通过矩阵乘法 计算向量

00:04:18.810 --> 00:04:23.659
顺便说一下 你大概也注意到我没有强调这里的偏差输入

00:04:23.660 --> 00:04:26.985
偏差不会改变这里的任何计算

00:04:26.985 --> 00:04:31.389
简单把它看做常量输入 通常是 1

00:04:31.389 --> 00:04:36.490
也可以通过权重连接到隐藏层的每个神经元

00:04:36.490 --> 00:04:40.410
偏差和其他输入的唯一差别在于

00:04:40.410 --> 00:04:45.235
当其他输入发生变化时 偏差保持不变

00:04:45.235 --> 00:04:47.830
另外与其他输入一样

00:04:47.829 --> 00:04:52.359
连接输入和下一层的权重也不断更新

00:04:52.360 --> 00:04:54.250
我们先暂停一下

00:04:54.250 --> 00:04:56.355
再说一下我们的目标是什么呢？

00:04:56.355 --> 00:04:59.305
我们要找到最适合的权重

00:04:59.305 --> 00:05:02.095
能够最后得出理想的输出

00:05:02.095 --> 00:05:03.742
对不对？ 最后

00:05:03.742 --> 00:05:07.405
我们要找到根据特定输入 得到正确输出的

00:05:07.404 --> 00:05:10.164
系统

00:05:10.165 --> 00:05:12.290
在训练阶段

00:05:12.290 --> 00:05:15.819
我们实际上知道特定输入的输出结果

00:05:15.819 --> 00:05:20.899
我们计算系统的输出 为了调整权重

00:05:20.899 --> 00:05:23.589
通过找出误差 来调整权重

00:05:23.589 --> 00:05:25.869
然后尽量最小化误差

00:05:25.870 --> 00:05:30.485
训练阶段的每次迭代都会一点点降低误差

00:05:30.485 --> 00:05:35.050
直到我们最终确定误差足够小

00:05:35.050 --> 00:05:38.199
我们重点关注直观误差的计算

00:05:38.199 --> 00:05:42.144
通过计算得出的结果和理想的输出之间的差距

00:05:42.144 --> 00:05:44.974
即可找到区别

00:05:44.975 --> 00:05:46.945
这是我们的基本误差

00:05:46.944 --> 00:05:50.040
在我们反向传播计算中

00:05:50.040 --> 00:05:51.834
我们会使用平方误差

00:05:51.834 --> 00:05:55.000
也叫做损失函数

