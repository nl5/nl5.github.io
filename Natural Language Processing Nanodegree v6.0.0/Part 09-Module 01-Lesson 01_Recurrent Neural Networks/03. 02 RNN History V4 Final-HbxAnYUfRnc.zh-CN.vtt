WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.440
上世纪 80 年代中叶掀起第一轮人工神经网络热潮后

00:00:04.440 --> 00:00:07.200
显然前馈网络存在缺点

00:00:07.200 --> 00:00:11.370
因为它们无法采集时间依赖

00:00:11.369 --> 00:00:13.244
正如我们前面提到的

00:00:13.244 --> 00:00:16.379
即历经时间的依赖

00:00:16.379 --> 00:00:21.382
在大多数现实世界的应用中 时态数据的建模很关键

00:00:21.382 --> 00:00:25.500
因为语音和视频等自然信号具有

00:00:25.500 --> 00:00:31.009
随着时间而变化的属性 具有依赖时间的特点

00:00:31.010 --> 00:00:34.785
另外生物神经网络具有反复出现的连接

00:00:34.784 --> 00:00:41.144
所以把循环应用到人工前馈神经网络中可以行得通

00:00:41.145 --> 00:00:46.515
首先向神经网络中添加存储的是时延神经网络

00:00:46.515 --> 00:00:48.855
简称 TDNN

00:00:48.854 --> 00:00:54.019
在网络输入中 时延神经网络中前期的输入也被引入到网络中

00:00:54.020 --> 00:00:57.350
改变实际外部输入

00:00:57.350 --> 00:00:59.760
显然这样做的好处是

00:00:59.759 --> 00:01:02.835
可以让网络不仅仅考虑当前的时间步长

00:01:02.835 --> 00:01:05.594
但同时也具有明显的劣势

00:01:05.594 --> 00:01:11.170
因为时间依赖受限于所选的时间窗口

00:01:11.170 --> 00:01:15.920
简单循环神经网络 又称 Elman 网络和 Jordan 网络

00:01:15.920 --> 00:01:17.234
是接下来的内容

00:01:17.234 --> 00:01:20.340
我们会在这节课后面探讨这些

00:01:20.340 --> 00:01:25.275
上世纪 90 年代初人们认识到这些网络都存在所谓的

00:01:25.275 --> 00:01:27.060
梯度消失问题

00:01:27.060 --> 00:01:31.865
即信息的贡献随着时间出现几何级消失

00:01:31.864 --> 00:01:34.409
那么采集

00:01:34.409 --> 00:01:39.054
超过 8 或 10 步长的关系实际上是不可能的

00:01:39.055 --> 00:01:41.340
虽然这些网络很精确

00:01:41.340 --> 00:01:44.185
但是都具有这个关键缺点

00:01:44.185 --> 00:01:48.045
我们随后会详细讨论梯度消失问题

00:01:48.045 --> 00:01:52.585
你也可以在这个视频下方找到这一话题的更多信息

00:01:52.584 --> 00:01:56.309
90 年代中期 人们发明了长短期记忆单元

00:01:56.310 --> 00:01:58.530
简称 LSTM

00:01:58.530 --> 00:02:02.125
解决这一问题

00:02:02.125 --> 00:02:06.269
长短期记忆网络的主要新颖之处在于一些信号

00:02:06.269 --> 00:02:08.115
即所谓的状态变量

00:02:08.115 --> 00:02:11.129
可以通过使用 Gates 进行储存

00:02:11.129 --> 00:02:16.030
并且在未来合适的时间里 通过门被重新引入到网络中

00:02:16.030 --> 00:02:20.219
因此任意时间间隔都可以体现出来

00:02:20.219 --> 00:02:23.685
可以采集到时间依赖

00:02:23.685 --> 00:02:25.754
不要担心这个介绍

00:02:25.754 --> 00:02:30.359
我们随后在这节课将详细介绍长短期记忆网络的所有内容

00:02:30.360 --> 00:02:34.275
长短期记忆网络的变体包括门控循环网络

00:02:34.275 --> 00:02:35.865
简称 GRU

00:02:35.865 --> 00:02:38.550
进一步完善了这一主题

00:02:38.550 --> 00:02:42.240
如今成为实现循环神经网络的另一大主流方法

