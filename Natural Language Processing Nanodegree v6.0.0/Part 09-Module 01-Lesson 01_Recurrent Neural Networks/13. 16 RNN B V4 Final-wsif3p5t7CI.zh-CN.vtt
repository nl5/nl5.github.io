WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.384
在前馈神经网络中

00:00:02.384 --> 00:00:09.274
任何时刻的输出都是当前输入和权重的函数

00:00:09.275 --> 00:00:12.690
我们假设输入是相互独立的

00:00:12.689 --> 00:00:16.259
所以这对序列没有意义

00:00:16.260 --> 00:00:21.465
我们实际上通过随机选取输入和目标对训练系统

00:00:21.464 --> 00:00:25.320
在循环神经网络中 我们在时间 t 的输出

00:00:25.320 --> 00:00:29.204
不仅取决于当前输入和权重

00:00:29.204 --> 00:00:32.054
还依赖于之前的输入

00:00:32.054 --> 00:00:35.354
如输入 T-1

00:00:35.354 --> 00:00:38.129
t-2 等等

00:00:38.130 --> 00:00:40.285
那么我们如何进行可视化呢？

00:00:40.284 --> 00:00:43.444
我们再次观察这个神经网络

00:00:43.445 --> 00:00:50.774
我们有输入 x 输出 y 和一个隐藏层 s 还记得 s 吗？

00:00:50.774 --> 00:00:52.259
它代表状态

00:00:52.259 --> 00:00:55.179
系统包含存储时 我们使用这个术语

00:00:55.179 --> 00:00:58.590
Wx 代表

00:00:58.590 --> 00:01:01.745
连接输入和状态的权重矩阵

00:01:01.744 --> 00:01:04.590
Wy 代表

00:01:04.590 --> 00:01:10.245
连接状态和输出的权重矩阵 Ws 代表

00:01:10.245 --> 00:01:16.035
连接之前时间步长和接下来时间步长状态的权重矩阵

00:01:16.034 --> 00:01:19.724
注意只有一个 s 反馈到系统

00:01:19.724 --> 00:01:21.746
在每个单独的时间步长中

00:01:21.746 --> 00:01:23.900
系统看起来是相同的

00:01:23.900 --> 00:01:27.560
我们把它叫做折叠模型

00:01:27.560 --> 00:01:31.490
由于输入随着时间是分散的

00:01:31.489 --> 00:01:35.104
我们在输入语句中为每个要素执行相同任务

00:01:35.105 --> 00:01:41.090
我们可以恰当展开模型 把它表示成下列方式

00:01:41.090 --> 00:01:43.564
正如例子所示

00:01:43.564 --> 00:01:46.984
你会看到时间 T+2 的输出

00:01:46.984 --> 00:01:51.709
取决于时间 t+2 的输入

00:01:51.709 --> 00:01:57.794
以及所有之前的输入

00:01:57.795 --> 00:02:00.545
我们使用以下定义

00:02:00.545 --> 00:02:05.150
Xt 是时间 t 的输入向量

00:02:05.150 --> 00:02:11.689
Yt 是时间 t 的输出向量 St 是

00:02:11.689 --> 00:02:17.954
是时间 t 的状态向量 在前馈神经网络中

00:02:17.954 --> 00:02:22.590
我们使用激活函数得到隐藏层 h

00:02:22.590 --> 00:02:28.495
我们只需要输入 以及连接输入和隐藏层的权重

00:02:28.495 --> 00:02:33.045
在循环神经网络中 我们使用激活函数得到 s

00:02:33.044 --> 00:02:35.219
不过存在些微差别

00:02:35.219 --> 00:02:40.694
现在输入到激活函数是一个总和

00:02:40.694 --> 00:02:45.329
即输入及其对应权重矩阵 Wx 的乘积

00:02:45.330 --> 00:02:49.410
加上

00:02:49.409 --> 00:02:55.004
之前激活值及其对应权重矩阵 Ws 的乘积

00:02:55.004 --> 00:03:01.500
输出向量的计算方法与前馈神经网络中完全相同

00:03:01.500 --> 00:03:04.155
它等于通向对应权重矩阵 Wy 的每个输出节点

00:03:04.155 --> 00:03:07.140
输入的线性组合

00:03:07.139 --> 00:03:14.394
例如相同线性组合的 Softmax 函数

00:03:14.395 --> 00:03:19.825
注意在每个时间步长中 循环神经网络的参数相同

00:03:19.824 --> 00:03:23.159
所以虽然从直观上来看 循环神经网络看似

00:03:23.159 --> 00:03:27.689
比前馈神经网络更加复杂 因为他们包含存储

00:03:27.689 --> 00:03:32.659
实际上需要学习的参数数量并不多

00:03:32.659 --> 00:03:36.240
我们提到的展开模型是通用的

00:03:36.240 --> 00:03:41.129
可以根据我们创建神经网络结构的目标进行调整

00:03:41.129 --> 00:03:45.000
我们可以决定所需要的输入和输出数量

00:03:45.000 --> 00:03:48.018
例如在情感分析中

00:03:48.018 --> 00:03:55.199
我们有多个输入和一个输出 表达从高兴到伤心的图谱

00:03:55.199 --> 00:03:58.155
另一个例子就是时间序列预测

00:03:58.155 --> 00:04:03.775
其中包括许多不对齐的输入和输出

00:04:03.775 --> 00:04:06.563
循环神经网络可以和乐高一样堆叠

00:04:06.563 --> 00:04:09.120
与前馈神经网络类似

00:04:09.120 --> 00:04:14.099
比如说 我们拥有单个循环神经网络的输出

00:04:14.099 --> 00:04:20.805
如向量 y 成为第二层的输入 而第二层的输出是向量 o

00:04:20.805 --> 00:04:24.439
每一层都和另一层独立运算

00:04:24.439 --> 00:04:27.360
如同建筑一样 输入和

00:04:27.360 --> 00:04:31.259
输出的方向并不会产生影响

