WEBVTT
Kind: captions
Language: zh-CN

00:00:00.500 --> 00:00:01.229
好吧

00:00:01.229 --> 00:00:03.846
现在我们要构建图表

00:00:03.846 --> 00:00:05.220
首先我们需要做的是

00:00:05.219 --> 00:00:07.330
定义我们的超参数

00:00:07.330 --> 00:00:09.730
第一个超参数是 LSTM 大小

00:00:09.730 --> 00:00:11.880
这是 LSTM 细胞的隐藏层中

00:00:11.880 --> 00:00:14.170
的单元数量

00:00:14.169 --> 00:00:17.039
LSTM 细胞中实际上有四个不同的

00:00:17.039 --> 00:00:17.759
网络层

00:00:17.760 --> 00:00:21.100
有三个 sigma 层和一个 TNH 层

00:00:21.100 --> 00:00:24.609
这是每一层中的单元数

00:00:24.609 --> 00:00:28.710
如果你将它设置成 256

00:00:28.710 --> 00:00:32.310
那么这四层中的每一层就有 256 个单元

00:00:32.310 --> 00:00:34.560
或者 你基本上可以将它想象成

00:00:34.560 --> 00:00:37.380
设置隐藏层中的单元数量

00:00:37.380 --> 00:00:39.990
差不多就是这样 只是 LSTM 单元

00:00:39.990 --> 00:00:42.900
要比常规隐藏层复杂的多

00:00:42.899 --> 00:00:44.640
不过 思路是一样的

00:00:44.640 --> 00:00:46.230
我是说 你要做的就是

00:00:46.229 --> 00:00:47.709
设置隐藏层中的单元数量

00:00:47.710 --> 00:00:51.840
你现在也许已经知道了 隐藏层中的

00:00:51.840 --> 00:00:54.420
单元数量越多 你获得的

00:00:54.420 --> 00:00:56.670
网络模型就越好

00:00:56.670 --> 00:01:00.060
当然这需要更多的运算

00:01:00.060 --> 00:01:03.910
接下来是我们的 LSTM 层超参数

00:01:03.909 --> 00:01:06.209
这是你用于 LSTM 的

00:01:06.209 --> 00:01:09.779
层数

00:01:09.780 --> 00:01:11.640
这就是深度学习层

00:01:11.640 --> 00:01:14.769
我们要做的是设置我们拥有的层数

00:01:14.769 --> 00:01:17.369
我们使用的 LSTM 细胞中的隐藏层的数量

00:01:17.370 --> 00:01:19.710
再次强调 层数越多

00:01:19.709 --> 00:01:21.959
通常来讲 性能就会越好

00:01:21.959 --> 00:01:24.599
但你也会遇到过拟合的情况

00:01:24.599 --> 00:01:27.059
就我的经验来说 一个良好的实践方法是

00:01:27.060 --> 00:01:33.750
使你的网络足够庞大 并且能调节过拟合

00:01:33.750 --> 00:01:36.810
比如添加正则化方法 如丢弃

00:01:36.810 --> 00:01:39.760
来控制过拟合

00:01:39.760 --> 00:01:43.359
说到这里 我先从 1 开始 看看效果怎么样

00:01:43.359 --> 00:01:49.599
如果你感觉你需要再加一层

00:01:49.599 --> 00:01:51.879
你可以加上 然后看看

00:01:51.879 --> 00:01:53.469
验证准确度如何

00:01:53.469 --> 00:01:55.438
接下来是批大小

00:01:55.438 --> 00:01:56.979
这是我们要在一次训练中输出网络的

00:01:56.980 --> 00:02:00.130
影评数量

00:02:00.129 --> 00:02:02.530
通常 只要内存够用 你会想让

00:02:02.530 --> 00:02:05.000
这个数字尽可能高

00:02:05.000 --> 00:02:08.288
因为我们的网络和 TensorFlow

00:02:08.288 --> 00:02:13.170
非常擅长进行矩阵计算

00:02:13.169 --> 00:02:14.829
我们需要传入的矩阵越少

00:02:14.830 --> 00:02:18.130
要花费的计算时间

00:02:18.129 --> 00:02:19.539
就越少

00:02:19.539 --> 00:02:23.469
然而 如果你的网络中有很多权重和

00:02:23.469 --> 00:02:26.199
很多连接 那么

00:02:26.199 --> 00:02:32.709
很快就会达到 GPU 或计算机的内存限制

00:02:32.710 --> 00:02:35.230
所以基本上 方法就是在内存允许情况下

00:02:35.229 --> 00:02:38.569
使它尽可能大

00:02:38.569 --> 00:02:40.389
最后一个 是学习率

00:02:40.389 --> 00:02:44.589
这是创建网络的第一步

00:02:44.590 --> 00:02:48.460
那就是定义输入和标签的占位符

00:02:48.460 --> 00:02:51.159
然后我们还要在此使用丢弃

00:02:51.159 --> 00:02:54.939
我们要为丢弃层里的保留概率

00:02:54.939 --> 00:02:57.349
创建一个占位符

00:02:57.349 --> 00:02:59.669
好的 这部分就留给你了

00:02:59.669 --> 00:03:02.449
接下来是我们的嵌入层

00:03:02.449 --> 00:03:06.919
在我们的图表中 这一层为黄色

00:03:06.919 --> 00:03:08.619
再次 我们要处理

00:03:08.620 --> 00:03:11.900
词汇表当中的 74,000 个单词

00:03:11.900 --> 00:03:13.599
如果你试着编码这部分的话

00:03:13.599 --> 00:03:16.000
会非常低效

00:03:16.000 --> 00:03:18.762
那么 我们要做的是传入整数

00:03:18.762 --> 00:03:20.469
然后使用一个嵌入层

00:03:20.469 --> 00:03:23.199
并将这一层用作查找表

00:03:23.199 --> 00:03:26.239
你已在 word2vec 课程中看了我们的做法

00:03:26.240 --> 00:03:29.469
那么你可以训练一个具有 word2vec 的

00:03:29.469 --> 00:03:32.319
嵌入层 然后将它导入到这里

00:03:32.319 --> 00:03:36.250
但是 事实是在大多数情况下

00:03:36.250 --> 00:03:38.419
除非你有大量的数据

00:03:38.419 --> 00:03:42.009
一般情况下直接创建一个新的嵌入层也挺方便

00:03:42.009 --> 00:03:44.469
然后让网络训练这些连接

00:03:44.469 --> 00:03:47.449
并且自己寻找表示

00:03:47.449 --> 00:03:51.519
那么 这里我留给你来使用 tf.variable

00:03:51.520 --> 00:03:58.310
创建嵌入矩阵 以及使用 tf.nn.embedding_lookup

00:03:58.310 --> 00:04:02.900
获得将传入 LSTM 细胞的向量

00:04:02.900 --> 00:04:04.390
如果你需要这方面的帮助

00:04:04.389 --> 00:04:07.659
可回看 word2vec 课程

00:04:07.659 --> 00:04:08.859
看看我当时是怎么做的

00:04:08.860 --> 00:04:11.650
你也可以点击此处阅读

00:04:11.650 --> 00:04:13.490
关于 embedding_lookup 的文档

00:04:13.490 --> 00:04:16.879
接下来 我们要定义 LSTM 细胞

00:04:16.879 --> 00:04:18.990
我们实际上并不会在这部分

00:04:18.990 --> 00:04:20.490
构建网络 而只是

00:04:20.490 --> 00:04:24.079
构建这些 LSTM 细胞中的东西

00:04:24.079 --> 00:04:28.389
你可以查阅这里关于 TensorFlow 的

00:04:28.389 --> 00:04:30.769
循环网络函数的文档

00:04:30.769 --> 00:04:33.129
我们首先要使用的是

00:04:33.129 --> 00:04:34.959
基本的 LSTM 细胞

00:04:34.959 --> 00:04:39.489
你可以通过 tf.contrib.rnn.BasicLSTMCell 获取它

00:04:39.490 --> 00:04:43.000
这是函数文档的样子

00:04:43.000 --> 00:04:45.279
基本上你只需创建这个细胞

00:04:45.279 --> 00:04:47.529
然后告诉它要使用的单元数量

00:04:47.529 --> 00:04:53.089
在我们的例子中 我将其称作 LSTM 大小 就在上面这里

00:04:53.089 --> 00:04:57.259
你可以说 LSTM 等于

00:04:57.259 --> 00:05:02.069
这些基本的 LSTM 细胞 LSTM 大小 这就可以了

00:05:02.069 --> 00:05:03.689
看起来就是这样

00:05:03.689 --> 00:05:08.160
接下来 我们要向 LSTM 细胞添加丢弃

00:05:08.160 --> 00:05:12.020
这很有用 因为在一定程度上

00:05:12.019 --> 00:05:17.069
你不费什么力气就提高了你网络的性能

00:05:17.069 --> 00:05:20.629
如果你记得 丢弃是一项正则化技术

00:05:20.629 --> 00:05:23.060
可以帮助我们防止过拟合

00:05:23.060 --> 00:05:26.360
对于这些 LSTM 单元

00:05:26.360 --> 00:05:31.400
很容易通过这个名为 dropout_wrapper 的类使用丢弃

00:05:31.399 --> 00:05:33.419
所以基本上 你只要传入细胞

00:05:33.420 --> 00:05:35.810
传入关键概率

00:05:35.810 --> 00:05:38.030
我们在上面为它定义了占位符

00:05:38.029 --> 00:05:41.449
然后是这个丢弃

00:05:41.449 --> 00:05:44.420
它基本上就是一个 LSTM 细胞

00:05:44.420 --> 00:05:48.750
只是现在 你的 LSTM 细胞在输出上有了丢弃

00:05:48.750 --> 00:05:52.670
再次 你可以选择添加更多的层

00:05:52.670 --> 00:05:57.410
让 TensorFlow 令这一切变得很简单

00:05:57.410 --> 00:06:01.550
可以使用 tf.contrib.rnn.MultiRNNCell

00:06:01.550 --> 00:06:06.290
那么 这里你只需传入一列单元

00:06:06.290 --> 00:06:09.050
在我们的例子中 我们有了丢弃

00:06:09.050 --> 00:06:11.555
正如我说的 它只是一个常规的 LSTM 单元

00:06:11.555 --> 00:06:13.699
这就像是包裹在丢弃内一样

00:06:13.699 --> 00:06:15.589
将丢弃加在末尾

00:06:15.589 --> 00:06:17.899
现在我们创建这个列表

00:06:17.899 --> 00:06:20.810
不管它有多少个层那么长

00:06:20.810 --> 00:06:23.209
然后将其传给 multi RNN cell

00:06:23.209 --> 00:06:27.889
这会创建一列单元

00:06:27.889 --> 00:06:30.870
然后为我们构建层

00:06:30.870 --> 00:06:35.480
这里应用所有的这些来构建 LSTM 单元

00:06:35.480 --> 00:06:38.165
在这里 你将构建基本的 LSTM 单元

00:06:38.165 --> 00:06:40.040
然后对它添加丢弃

00:06:40.040 --> 00:06:43.220
然后在多个 LSTM 层上将它们堆叠起来

00:06:43.220 --> 00:06:46.980
这里我使用此单元获得初始状态

00:06:46.980 --> 00:06:50.120
如果你记得 LSTM 具有单元状态

00:06:50.120 --> 00:06:54.050
它们在 LSTM 单元之间进行传递

00:06:54.050 --> 00:06:57.031
这只是创建都为 0 的初始单元

00:06:57.031 --> 00:06:57.530
状态

00:06:57.529 --> 00:07:00.049
这会通过在一个序列中训练和传入数据

00:07:00.050 --> 00:07:03.170
来进行更新

00:07:03.170 --> 00:07:08.223
最后 我们将构建 RNN 前向传递

00:07:08.223 --> 00:07:10.639
这意味着我们要传入我们的数据

00:07:10.639 --> 00:07:12.289
然后它会通过这里

00:07:12.290 --> 00:07:15.170
然后实际计算输出

00:07:15.170 --> 00:07:19.370
我在这里使用 RNN 的主要原因是

00:07:19.370 --> 00:07:21.860
我们可以实际得到

00:07:21.860 --> 00:07:28.400
关于我们的单词和影评的序列信息

00:07:28.399 --> 00:07:31.279
你可能记得 在 Andrew Trask 的课程中

00:07:31.279 --> 00:07:33.109
他只用了一个前馈网络

00:07:33.110 --> 00:07:35.150
在那个例子中

00:07:35.149 --> 00:07:38.419
网络只知道影评中的单个单词

00:07:38.420 --> 00:07:41.069
但在这个例子中 不仅有影评中

00:07:41.069 --> 00:07:43.519
的单个单词 还有这些单词

00:07:43.519 --> 00:07:44.689
出现的序列

00:07:44.689 --> 00:07:48.110
每个单词都通过这里

00:07:48.110 --> 00:07:51.590
通过嵌入层 沿着网络中的

00:07:51.589 --> 00:07:53.419
垂直路径传输

00:07:53.420 --> 00:07:56.300
但是 由于我们使用的是循环神经网络

00:07:56.300 --> 00:08:00.079
我们还有通过隐藏层

00:08:00.079 --> 00:08:02.509
传递的连续信息

00:08:02.509 --> 00:08:05.539
所以我们这里的输出 "positive," 知道

00:08:05.540 --> 00:08:09.010
"best" 和 "movie" 在 "ever," 之前出现

00:08:09.009 --> 00:08:13.789
现在我们就有了影评中

00:08:13.790 --> 00:08:14.720
单词顺序的信息

00:08:14.720 --> 00:08:17.600
所以这使循环神经网络

00:08:17.600 --> 00:08:19.010
的实际性能要优于

00:08:19.009 --> 00:08:20.959
常规的前馈网络

00:08:20.959 --> 00:08:23.659
我们实际上要用 RNN 来跑我们的数据

00:08:23.660 --> 00:08:25.850
要使用 tf.nn.DynamicRNN 这个函数

00:08:29.879 --> 00:08:31.550
那么你在这里需要做的就是

00:08:31.550 --> 00:08:34.788
传入我们已经创建的单元

00:08:34.788 --> 00:08:38.629
以及输入 在这个例子中

00:08:38.629 --> 00:08:40.610
它们来自嵌入层

00:08:40.610 --> 00:08:42.206
而传入隐藏层

00:08:42.206 --> 00:08:43.580
这就是这里输入的意思

00:08:43.580 --> 00:08:45.600
它不是指网络的输入

00:08:45.600 --> 00:08:48.639
而是 LSTM 细胞的输入

00:08:48.639 --> 00:08:51.259
是隐藏层的输入

00:08:51.259 --> 00:08:53.240
然后你还要给它初始状态

00:08:53.240 --> 00:08:56.720
它的作用是通过整个网络

00:08:56.720 --> 00:09:00.769
然后计算每个隐藏层的状态

00:09:00.769 --> 00:09:04.100
将其传至下一层 并且计算

00:09:04.100 --> 00:09:06.262
每个隐藏层的输出

00:09:06.261 --> 00:09:07.719
当它完成这一过程时

00:09:07.720 --> 00:09:10.279
会给我们一个输出列表

00:09:10.279 --> 00:09:11.399
以及最终状态

00:09:11.399 --> 00:09:17.480
那么我就留给你来构建这个动态 RNN

00:09:17.480 --> 00:09:21.250
并计算输出和最终状态

