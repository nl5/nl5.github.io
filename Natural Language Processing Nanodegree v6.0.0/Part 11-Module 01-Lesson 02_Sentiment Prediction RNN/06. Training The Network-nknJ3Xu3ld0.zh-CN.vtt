WEBVTT
Kind: captions
Language: zh-CN

00:00:00.790 --> 00:00:04.450
现在我将计算输出

00:00:04.450 --> 00:00:06.820
正如之前提到的

00:00:06.820 --> 00:00:09.080
我们真正关心的是最终输出

00:00:09.080 --> 00:00:12.160
我们将用输出来预测我们的情感

00:00:12.160 --> 00:00:13.839
我们需要获取最后一个输出

00:00:13.839 --> 00:00:16.839
可以使用 outputs() 进行获取

00:00:16.839 --> 00:00:19.929
这个表示提供所有的行 所有批次

00:00:19.929 --> 00:00:23.774
这个表示提供最后一列

00:00:23.774 --> 00:00:25.690
我们同时传入一系列的影评

00:00:25.690 --> 00:00:28.450
因为这是我们的批次内容

00:00:28.449 --> 00:00:34.079
最后一列是 LSTM 单元的最后一个

00:00:34.079 --> 00:00:35.390
输出

00:00:35.390 --> 00:00:37.890
这个箭头表示这个最后输出

00:00:37.890 --> 00:00:40.039
LSTM 单元的最后输出

00:00:40.039 --> 00:00:42.939
我们将忽略所有这些其他输出

00:00:42.939 --> 00:00:48.699
LSTM 中的这些输出

00:00:48.700 --> 00:00:52.179
将进入完全连接层

00:00:52.179 --> 00:00:53.920
该层级只有 1 个单元

00:00:53.920 --> 00:00:57.219
因为我们只有一个 Sigmoid 预测器单元

00:00:57.219 --> 00:01:01.210
这是激活函数 tf.sigmoid

00:01:01.210 --> 00:01:03.200
这是我们的预测

00:01:03.200 --> 00:01:05.730
Sigmoid函数的输出是 0 到 1 之间

00:01:05.730 --> 00:01:09.760
你可以将其当做结果为 true 的概率

00:01:09.760 --> 00:01:12.160
也就是结果为 positive 的概率

00:01:12.159 --> 00:01:14.679
这里 我们使用标签和预测

00:01:14.680 --> 00:01:17.680
计算代价 用均方误差表示

00:01:17.680 --> 00:01:20.030
最后是 AdamOptimizer

00:01:20.030 --> 00:01:23.329
它将用来减小代价并训练权重

00:01:23.329 --> 00:01:26.890
这里我添加了一小段代码

00:01:26.890 --> 00:01:29.109
用来计算验证的准确性

00:01:29.109 --> 00:01:31.719
我对预测四舍五入

00:01:31.719 --> 00:01:35.879
因为预测在 0 到 1 之间

00:01:35.879 --> 00:01:38.949
但是我们希望它们为 0 和 1

00:01:38.950 --> 00:01:41.659
这样可以判断是否等于我们的标签

00:01:41.659 --> 00:01:43.390
这里我对预测四舍五入

00:01:43.390 --> 00:01:47.239
使其等于 0 或 1 具体取决于值为多少

00:01:47.239 --> 00:01:50.259
然后更改数据类型

00:01:50.260 --> 00:01:53.340
我将其转型为整型 因为标签是整型

00:01:53.340 --> 00:01:56.439
我们希望它们的数据类型相同

00:01:56.439 --> 00:02:01.780
这样才能检测是否相等

00:02:01.780 --> 00:02:04.853
我们传入一系列批次

00:02:04.853 --> 00:02:07.269
我们有一个批次 我们在每个批次

00:02:07.269 --> 00:02:08.508
传入一批影评

00:02:08.508 --> 00:02:10.870
现在我们想计算

00:02:10.870 --> 00:02:13.509
所有这些准确性的均值

00:02:13.509 --> 00:02:16.479
为此 我们将使用 tf.reduce_mean

00:02:16.479 --> 00:02:19.780
该函数将计算所有预测的均值

00:02:19.780 --> 00:02:21.860
并得出准确性

00:02:21.860 --> 00:02:24.970
correct_pred 是布尔型

00:02:24.969 --> 00:02:28.870
这是 equal 的返回数据类型

00:02:28.870 --> 00:02:30.730
我们需要将这个转型为浮点型

00:02:30.729 --> 00:02:32.439
这样才能用于 reduce_mean

00:02:32.439 --> 00:02:35.310
这段代码就是这些作用

00:02:35.310 --> 00:02:39.210
这个函数负责获取批次

00:02:39.210 --> 00:02:41.760
你可以阅读这段文字并查看代码

00:02:41.759 --> 00:02:44.099
但是代码很典型

00:02:44.099 --> 00:02:45.960
我有两个数组

00:02:45.960 --> 00:02:50.250
x 和 y 这里我确保

00:02:50.250 --> 00:02:52.530
每个批次都是完整批次

00:02:52.530 --> 00:02:54.289
我去除了额外数据

00:02:54.289 --> 00:02:57.060
然后循环访问每个数组

00:02:57.060 --> 00:03:01.469
并输出批次

00:03:01.469 --> 00:03:04.349
最后 这段代码是训练代码

00:03:04.349 --> 00:03:08.219
代码很长 非常标准

00:03:08.219 --> 00:03:10.229
这基本上是我

00:03:10.229 --> 00:03:12.509
在每个 notebook 中都会写的类似代码

00:03:12.509 --> 00:03:15.840
如果你想挑战下自己

00:03:15.840 --> 00:03:18.200
可以删了整个代码段

00:03:18.199 --> 00:03:22.560
并自己去实现

00:03:22.560 --> 00:03:25.620
最佳学习方法就是

00:03:25.620 --> 00:03:28.050
自己编写所有代码

00:03:28.050 --> 00:03:30.180
因为这就是教程的特征

00:03:30.180 --> 00:03:32.040
你阅读了这些教程 以为学会了

00:03:32.039 --> 00:03:33.789
但是当你去尝试的时候

00:03:33.789 --> 00:03:36.609
实际上并没学会 因为你并没有亲自去写这些代码

00:03:36.610 --> 00:03:39.660
所以最好是

00:03:39.659 --> 00:03:43.199
删掉所有这些训练代码

00:03:43.199 --> 00:03:43.989
自己去实现

00:03:43.990 --> 00:03:45.990
当然了 你可以将这段代码复制到另一个文件中

00:03:45.990 --> 00:03:49.409
可以对比看看你的编写效果

00:03:49.409 --> 00:03:52.049
最后 下面的是测试代码

00:03:52.050 --> 00:03:56.160
我将请你编写所有这些 完成练习

00:03:56.159 --> 00:03:58.359
并运行这段代码

00:03:58.360 --> 00:04:00.660
如果你遇到了问题

00:04:00.659 --> 00:04:03.030
我将提供解决方案视频和解决方案 notebook 

00:04:03.030 --> 00:04:05.180
你可以看看我是如何完成的

00:04:05.180 --> 00:04:07.210
加油！

