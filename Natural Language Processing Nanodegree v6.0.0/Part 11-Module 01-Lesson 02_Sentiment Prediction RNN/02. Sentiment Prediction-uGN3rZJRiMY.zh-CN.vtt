WEBVTT
Kind: captions
Language: zh-CN

00:00:00.500 --> 00:00:02.070
你好 我们又见面了

00:00:02.069 --> 00:00:04.029
本周我们将讨论

00:00:04.030 --> 00:00:07.880
利用循环神经网络完成情感分析

00:00:07.879 --> 00:00:10.990
希望这会让你对循环神经网络

00:00:10.990 --> 00:00:14.890
如何工作有更深入的了解

00:00:14.890 --> 00:00:17.769
你之前已经多次看过这个了

00:00:17.769 --> 00:00:22.119
我们将使用 Andrew Trask 所使用的数据

00:00:22.120 --> 00:00:25.000
但在他的例子中 他并没有用循环神经网络

00:00:25.000 --> 00:00:27.219
他使用的只是一个普通的前馈网络

00:00:27.219 --> 00:00:30.009
Sarraj 也
在他的视频中实施了一个网络

00:00:30.010 --> 00:00:33.100
与这个网络非常类似

00:00:33.100 --> 00:00:35.929
这是网络的示意图

00:00:35.929 --> 00:00:39.200
我们要传递一些影评中的单词

00:00:39.200 --> 00:00:42.640
之后为这些影评

00:00:42.640 --> 00:00:43.990
加上标签

00:00:43.990 --> 00:00:45.530
这些标签是正面或负面的

00:00:45.530 --> 00:00:47.410
我们要利用这个网络

00:00:47.409 --> 00:00:51.009
获取我们尚未看到的其他影评并预测

00:00:51.009 --> 00:00:53.699
这些影评是正面的还是负面的

00:00:53.700 --> 00:00:55.450
我们要传递一些带有标签

00:00:55.450 --> 00:00:58.780
的影评并

00:00:58.780 --> 00:01:00.670
训练网络

00:01:00.670 --> 00:01:03.679
思路是这样的 这就是我们的网络

00:01:03.679 --> 00:01:06.385
我们有个典型的设置

00:01:06.385 --> 00:01:08.259
首先我们有了这些词语 这些词语会

00:01:08.260 --> 00:01:10.359
进入嵌入层

00:01:10.359 --> 00:01:12.400
我们在实施

00:01:12.400 --> 00:01:14.800
 Word2Vec 之前了解了嵌入

00:01:14.799 --> 00:01:18.039
现在的问题是 已经有了词语之后

00:01:18.040 --> 00:01:21.160
意味着有了成千上万的不同类别

00:01:21.159 --> 00:01:23.200
所以 如果尝试独热编码

00:01:23.200 --> 00:01:25.120
那么效率将会很低下

00:01:25.120 --> 00:01:29.502
因此 相反 我们要将词语编码为整数

00:01:29.501 --> 00:01:31.209
然后将其传递到一个嵌入层

00:01:31.209 --> 00:01:33.849
这样我们就可以获得这些嵌入式向量

00:01:33.849 --> 00:01:37.089
这些向量表示我们的词语

00:01:37.090 --> 00:01:40.070
然后 这些都被传递给 LSTM 单元

00:01:40.069 --> 00:01:42.879
这些LSTM单元

00:01:42.879 --> 00:01:46.269
它们基本上就相当于一个隐藏层

00:01:46.269 --> 00:01:48.979
因为这个神经网络是循环式的

00:01:48.980 --> 00:01:52.960
我们将隐藏层的状态

00:01:52.959 --> 00:01:55.359
传递到我们的循环神经网络中的下一步

00:01:55.359 --> 00:01:58.780
进入下一步 并最终输出

00:01:58.780 --> 00:02:00.489
而对于输出层 我们

00:02:00.489 --> 00:02:03.475
只使用一个单元 并经过Sigmoid函数激活

00:02:03.475 --> 00:02:05.349
再一次 我们只是试图将影评

00:02:05.349 --> 00:02:07.039
分为正面的或负面的

00:02:07.040 --> 00:02:11.740
在本例中 我们只需要得到一个为零或者为一的输出

00:02:11.740 --> 00:02:13.713
Sigmoid 所做的就是把输入的东西

00:02:13.713 --> 00:02:15.129
输出成一个  0 到 1 之间的数

00:02:15.129 --> 00:02:18.430
这样你就知到影评是正面的概率有多大

00:02:18.430 --> 00:02:19.950
或者负面概率是多大

00:02:19.949 --> 00:02:20.560
清楚了吗？

00:02:20.560 --> 00:02:23.969
所以 我们会在输出层使用 Sigmoid 函数

00:02:23.969 --> 00:02:26.020
这里 你

00:02:26.020 --> 00:02:28.450
可以看到 这些输出层没有任何标签

00:02:28.449 --> 00:02:32.079
我们真正关心的只是最后的输出

00:02:32.080 --> 00:02:34.690
所以我们只需传递一段词语序列 然后

00:02:34.689 --> 00:02:38.810
得到最后的输出 此输出将作为我们的标签或目标来使用

00:02:38.810 --> 00:02:43.360
在本例中 有史以来最好的电影可能是一条正面评价

00:02:43.360 --> 00:02:45.610
所以 我们就给它一个正面的标签

00:02:45.610 --> 00:02:47.555
这就是我们的神经网络的大致机理

00:02:47.555 --> 00:02:48.930
我们只需将这个传递进来

00:02:48.930 --> 00:02:50.763
它经过我们的嵌入层

00:02:50.763 --> 00:02:55.180
到 LSTM 隐藏层 然后到 Sigmoid 输出层

00:02:55.180 --> 00:02:57.400
接着我们将根据最后一个输出的结果来计算

00:02:57.400 --> 00:02:59.210
成本

00:02:59.210 --> 00:02:59.710
好

00:02:59.710 --> 00:03:02.379
我们开始着手这项工作

00:03:02.379 --> 00:03:05.409
在这里我只导入我需要的库 NumPy

00:03:05.409 --> 00:03:11.525
和 TensorFlow 然后加载数据、评价和标签

00:03:11.525 --> 00:03:14.451
在这里 你可以看到某些评价的示例

00:03:14.451 --> 00:03:15.909
有一件事我没有注意到

00:03:15.909 --> 00:03:18.699
你看这些 \n

00:03:18.699 --> 00:03:20.393
这些都是新行字符

00:03:20.394 --> 00:03:22.060
它们的实际作用是

00:03:22.060 --> 00:03:26.659
作为
此数据集中各条评价的分隔符

00:03:26.659 --> 00:03:30.311
这个 \n 表示第二条评论

00:03:30.312 --> 00:03:32.020
我们在这里又看到一个 \n

00:03:32.020 --> 00:03:33.670
这是第三条评论

00:03:33.669 --> 00:03:35.500
因此 我们通过这个知道

00:03:35.500 --> 00:03:37.870
如何分开各个评价

00:03:37.870 --> 00:03:41.409
例如 这个巨大的文字块包含两条单独的评价

00:03:41.409 --> 00:03:48.310
如果我们用换行符将这个巨大的字符串分开的话 就可以清楚地看到这一点

00:03:48.310 --> 00:03:50.379
正如你之前可能看到过的 在

00:03:50.379 --> 00:03:52.180
我们查看所有文字时 可以看到

00:03:52.180 --> 00:03:54.730
很多句号和其他标点符号

00:03:54.729 --> 00:03:56.756
我们首先要去掉这些符号

00:03:56.756 --> 00:03:57.879
这很容易做到

00:03:57.879 --> 00:04:00.669
我们会得到

00:04:00.669 --> 00:04:02.709
所有标点的字符串 然后

00:04:02.710 --> 00:04:04.480
可以查看我们的评论

00:04:04.479 --> 00:04:08.488
如果评论中的一些字符不是标点符号

00:04:08.489 --> 00:04:09.280
则可以保留这些字符

00:04:09.280 --> 00:04:11.840
否则 我们就去除这些字符

00:04:11.840 --> 00:04:15.400
之后我们可以将文本
分为多条评论

00:04:15.400 --> 00:04:19.149
只需在换行符位置将它们分开

00:04:19.149 --> 00:04:23.980
之后我们将这些评论重新组织为

00:04:23.980 --> 00:04:26.480
全文

00:04:26.480 --> 00:04:28.930
我这样做的原因是如果我们直接

00:04:28.930 --> 00:04:31.480
尝试将这些东西分成多个词语 那么这就是一个词语

00:04:31.480 --> 00:04:35.200
这个换行符homelessness 是一个词

00:04:35.199 --> 00:04:38.149
这个换行符story 又是一个词

00:04:38.149 --> 00:04:39.789
当我们试图查找词语时

00:04:39.790 --> 00:04:40.960
这些包含换行符的单词就会在我们的词语库中

00:04:40.959 --> 00:04:42.375
但我们并不希望词语里面包含

00:04:42.375 --> 00:04:43.599
有换行符

00:04:43.600 --> 00:04:47.980
我所做的就是只是将它分成两条评论

00:04:47.980 --> 00:04:50.680
然后我将它们放在一起 这样就把新行分隔符去掉了

00:04:50.680 --> 00:04:52.840
现在我们已经获得了

00:04:52.839 --> 00:04:58.079
所有文本 即一个很长的字符串

00:04:58.079 --> 00:05:00.879
之后 我们可以再次通过分隔操作获得词语

00:05:00.879 --> 00:05:03.189
现在这些就是我们的所有词语

00:05:03.189 --> 00:05:05.139
你在这里 可以看到所有文字

00:05:05.139 --> 00:05:09.310
我们的所有文字是一个长字符串

00:05:09.310 --> 00:05:12.459
而且没有句号、没有换行符

00:05:12.459 --> 00:05:15.098
我们可以从这些文本里面得道所有的单词

00:05:15.098 --> 00:05:16.389
你可以在这里看到这些单词

00:05:16.389 --> 00:05:20.519
Brummel High 是一部卡通喜剧 等等
最新课程跟课件还有一对一辅导请加wx：udacity6
