WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:06.339
欢迎回来 这是你要自己实现的最后一部分

00:00:06.339 --> 00:00:08.070
在结构里

00:00:08.070 --> 00:00:12.660
输出有 softmax 层

00:00:12.660 --> 00:00:15.060
因为我们要处理数万单词

00:00:15.060 --> 00:00:18.199
所以 softmax 层会有数万个单元

00:00:18.199 --> 00:00:19.975
但是

00:00:19.975 --> 00:00:26.129
任何输入都只有一个标签为真

00:00:26.129 --> 00:00:29.699
也就是说 虽然权重有数百万个 但我们要变的东西很少

00:00:29.699 --> 00:00:33.329
因为我们只有一个例子为真

00:00:33.329 --> 00:00:35.340
所以只有极少数权重的更新

00:00:35.340 --> 00:00:38.225
有意义

00:00:38.225 --> 00:00:41.280
所以我们不这么做 相反 我们要估算

00:00:41.280 --> 00:00:44.579
softmax 层的损失

00:00:44.579 --> 00:00:50.829
因此 我们可以在所有权重里取一个小小的样本子集

00:00:50.829 --> 00:00:54.460
虽然我们要更新权重 使其具备正确的标签

00:00:54.460 --> 00:00:59.500
但我们只对一个小小的错误标签样本进行处理

00:00:59.500 --> 00:01:01.783
这个样本的大小通常在 100 左右

00:01:01.783 --> 00:01:03.969
这就叫负采样

00:01:03.969 --> 00:01:04.993
详情

00:01:04.993 --> 00:01:06.890
请查看这个链接

00:01:06.890 --> 00:01:11.108
TensorFlow 有函数可以实现这一点

00:01:11.108 --> 00:01:14.113
那就是 tf.nn.sampled_softmax_loss

00:01:14.114 --> 00:01:18.155
你可以看看这篇文献 了解这个函数的原理

00:01:18.155 --> 00:01:21.504
然后在这下面为 softmax 层创建权重和偏置

00:01:21.504 --> 00:01:26.200
然后用权重和偏置来算出损失

00:01:26.200 --> 00:01:30.067
计算函数还是一样 是目标 也就是 sampled_softmax_loss

00:01:30.067 --> 00:01:32.078
将结果传递给损失函数 cost

00:01:32.078 --> 00:01:37.559
函数 AdamOptimizer 会如常把损失最小化

00:01:37.560 --> 00:01:40.394
好了 试着动手实现这些操作吧

00:01:40.394 --> 00:01:42.984
一样的 如果需要帮助

00:01:42.983 --> 00:01:47.000
遇到难题或想知道我是怎么实现的 可以参考我的方案 加油

