WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:05.230
欢迎回来 本周我将带你探索嵌入

00:00:05.230 --> 00:00:10.390
和 word2vec 模型的执行 加深你对这些内容的理解

00:00:10.390 --> 00:00:16.460
在这个 notebook 里 你将使用 TensorFlow 来实现 word2vec 模型

00:00:16.460 --> 00:00:20.160
这些是可以阅读的资源

00:00:20.160 --> 00:00:23.449
在开始学习之前 你得先看一看

00:00:23.449 --> 00:00:27.059
或者边学边看 这样可以加深你的理解

00:00:27.059 --> 00:00:32.314
这是 Chris McCormick 写的 word2vec 概念介绍 很不错

00:00:32.314 --> 00:00:38.118
这是两篇论文 作者是 Mikolov 等人 他们都在研究这个架构

00:00:38.118 --> 00:00:41.149
第一篇是原论文 这篇涉及的内容则有所修改

00:00:41.149 --> 00:00:45.140
到时候你也会在这个 notebook 里进行实现

00:00:45.140 --> 00:00:52.880
这两个是实现案例 这个来自 TensorFlow 的文献

00:00:52.880 --> 00:00:57.679
当你在处理文本时 你会把文本分成若干个词

00:00:57.679 --> 00:01:03.314
因此你很可能会得到数万个不同的单词 那是个很大的数据集

00:01:03.314 --> 00:01:05.750
所以当你把这些单词作为输入时

00:01:05.750 --> 00:01:08.534
通常你都需要对它们进行独热编码处理

00:01:08.534 --> 00:01:11.060
也就是说 你会有这些巨大的向量

00:01:11.060 --> 00:01:13.715
比如有 5 万个单位长

00:01:13.715 --> 00:01:18.635
5 万个元素长 但只有一个元素设为 1 其它都设为 0

00:01:18.635 --> 00:01:21.219
所以当你进行矩阵相乘

00:01:21.218 --> 00:01:25.238
以便获取隐藏层的值时

00:01:25.239 --> 00:01:27.415
你就得进行特别庞大的乘法运算

00:01:27.415 --> 00:01:34.450
比如 对于这个庞大的矩阵 你需要让 5 万个元素乘以 300 个不同的权重参数

00:01:34.450 --> 00:01:39.290
然而当你进行矩阵相乘时 因为只有一个元素会被设为 1

00:01:39.290 --> 00:01:46.443
所以大多数乘积都会是 0

00:01:46.442 --> 00:01:52.259
这样的计算几乎没有效率

00:01:52.260 --> 00:01:58.319
为了解决这个问题 嵌入就派上了用场

00:01:58.319 --> 00:02:05.105
嵌入基本上就是在矩阵相乘时走捷径

00:02:05.105 --> 00:02:07.525
因为所有东西基本是一样的

00:02:07.525 --> 00:02:09.525
隐藏层一样

00:02:09.525 --> 00:02:12.639
在输入和隐藏层之间的权重矩阵一样

00:02:12.639 --> 00:02:14.430
唯一的不同只在于

00:02:14.430 --> 00:02:17.930
隐藏层值的实际获取方式

00:02:17.930 --> 00:02:21.245
所以我们有点像是要直接跳掉

00:02:21.245 --> 00:02:25.128
整个矩阵相乘的过程

00:02:25.128 --> 00:02:29.758
直接在一个表格查询隐藏层的值

00:02:29.758 --> 00:02:35.149
之所以可以这么做 是因为我们是用矩阵和一个独热编码向量相乘

00:02:35.150 --> 00:02:40.270
你只需要找到与元素相对应的那行

00:02:40.270 --> 00:02:42.819
比如 如果这里的第四个元素是 1

00:02:42.818 --> 00:02:46.239
剩下的都是 0 那你只需要在这个矩阵里

00:02:46.240 --> 00:02:48.935
找到第四行

00:02:48.935 --> 00:02:52.710
因为其它乘积都会是 0

00:02:52.710 --> 00:02:59.145
基本上也就意味着要进行这个矩阵乘法

00:02:59.145 --> 00:03:01.340
我们知道这是第四个元素

00:03:01.340 --> 00:03:05.069
所以我们可以直接找到第四行 得出这个答案

00:03:05.068 --> 00:03:08.609
实际上根本没有必要进行矩阵相乘

00:03:08.610 --> 00:03:13.229
然后我们要做的就是把词设成令牌

00:03:13.229 --> 00:03:17.729
也就是说 把词转为整数

00:03:17.729 --> 00:03:22.064
而这个位于输入和隐藏层之间的嵌入权重矩阵依旧存在

00:03:22.063 --> 00:03:27.000
但现在我们把它叫作查询表 表如其名 因为我们会去查询矩阵里

00:03:27.000 --> 00:03:32.425
与词相对应的那一行

00:03:32.425 --> 00:03:36.830
而那就是隐藏层的值

00:03:36.830 --> 00:03:41.094
隐藏层的单元数则称为嵌入维数

00:03:41.093 --> 00:03:44.114
本质上 你就是为每个词取得另一个向量

00:03:44.115 --> 00:03:48.490
而向量的大小就是嵌入维数

00:03:48.490 --> 00:03:53.085
通过这种方式 我们就完全跳过了矩阵相乘这一步

00:03:53.085 --> 00:03:59.064
我们只需要说词核是这个整数 958

00:03:59.063 --> 00:04:02.318
然后矩阵乘法就会给出这一行 所以我们只需查询第 958 行

00:04:02.318 --> 00:04:07.573
就能得到隐藏层的值

00:04:07.574 --> 00:04:11.930
所以嵌入其实没什么神奇的

00:04:11.930 --> 00:04:14.919
嵌入查询表其实就是权重矩阵

00:04:14.919 --> 00:04:17.350
嵌入层就是隐藏层

00:04:17.350 --> 00:04:22.588
这种查询其实是矩阵相乘的捷径

00:04:22.588 --> 00:04:25.615
由于查询表就是权重矩阵

00:04:25.615 --> 00:04:30.639
所以你可以像处理之前遇到的其它权重矩阵一样训练它

00:04:30.639 --> 00:04:34.000
比如用反向传播来更好地学习参数

00:04:34.000 --> 00:04:38.254
以便分类、预测等等

00:04:38.254 --> 00:04:41.745
嵌入不仅可处理词

00:04:41.745 --> 00:04:46.134
基本上只要你遇到的类数太庞大 你都可以用嵌入来处理

00:04:46.134 --> 00:04:52.259
在这个 notebook 里 我们要讲一种模型

00:04:52.259 --> 00:04:55.230
叫做 word2vec 该模型会用嵌入层来

00:04:55.230 --> 00:04:58.564
从这些向量中获取这类词表征

00:04:58.564 --> 00:05:01.560
所以这些向量代表的不是一个词

00:05:01.560 --> 00:05:05.670
而是这些词的语义

00:05:05.670 --> 00:05:09.245
word2vec 是一种算法

00:05:09.245 --> 00:05:15.605
该算法会用嵌入层的向量来表示单词 从而实现更佳的表征方式

00:05:15.605 --> 00:05:17.399
一旦算法得到充分训练

00:05:17.399 --> 00:05:19.920
向量就能包含语义信息

00:05:19.920 --> 00:05:23.475
也就是说 处于类似语境的单词

00:05:23.475 --> 00:05:25.949
比如颜色 比如黑色、白色和红色

00:05:25.949 --> 00:05:29.220
这些单词会出现在相似的语境中

00:05:29.220 --> 00:05:32.579
所以它们会有类似的表征

00:05:32.579 --> 00:05:35.370
而这些颜色单词对应的向量值会很接近

00:05:35.370 --> 00:05:39.060
因为单词语境是相似的

00:05:39.060 --> 00:05:42.884
word2vec 有两种实现结构

00:05:42.884 --> 00:05:44.410
一种是 CBOW

00:05:44.410 --> 00:05:47.213
即连续词袋

00:05:47.213 --> 00:05:49.699
一种是 Skip-gram

00:05:49.699 --> 00:05:53.225
我们要细讲的是 Skip-gram

00:05:53.225 --> 00:05:57.990
因为事实表明它比连续词袋效果好

00:05:57.990 --> 00:06:00.540
总的思路是

00:06:00.540 --> 00:06:04.649
你对数据集里一些单词感兴趣

00:06:04.649 --> 00:06:07.410
对吧？ 假设这个单词在数据集的 t 处

00:06:07.410 --> 00:06:12.899
你还要观察目标单词周围的单词 因为是在 t 周围 假设在 t - 2 处

00:06:12.899 --> 00:06:15.418
也就是在目标单词前两位的单词

00:06:15.418 --> 00:06:19.524
还有在 t + 1 处 也就是在目标单词后一位的单词

00:06:19.524 --> 00:06:23.235
这样在某种程度上就代表了这个词所在的语境

00:06:23.235 --> 00:06:25.269
在 CBOW 里

00:06:25.269 --> 00:06:28.389
你的输入就是目标单词所在的上下文 你试着根据上下文来预测目标单词

00:06:28.389 --> 00:06:33.069
有点像你在透过一些文本窗口进行观察

00:06:33.069 --> 00:06:36.338
这些窗口就是语境

00:06:36.338 --> 00:06:40.434
你要预测的就是出现在窗口中间的单词

00:06:40.434 --> 00:06:43.175
而在 Skip-gram 则反过来

00:06:43.175 --> 00:06:45.730
你是要输入一些单词

00:06:45.730 --> 00:06:49.660
然后试着预测在文本里 这个单词周围有什么单词

00:06:49.660 --> 00:06:53.896
也就是预测这个词所在的语境

00:06:53.896 --> 00:06:57.471
所以在 skip-gram 里

00:06:57.471 --> 00:07:03.879
你是要训练网络 使其理解上下文和词义

00:07:03.879 --> 00:07:05.379
还是以黑白红为例

00:07:05.379 --> 00:07:08.199
这些词会出现在相似的语境里

00:07:08.199 --> 00:07:10.814
它们的周围会有相似的单词

00:07:10.814 --> 00:07:13.834
所以输入黑色、白色和红色

00:07:13.834 --> 00:07:17.800
就能预测相同的单词和语境

00:07:17.800 --> 00:07:20.379
所以这些词会有类似的映射

00:07:20.379 --> 00:07:24.144
会有相似的嵌入向量

00:07:24.144 --> 00:07:27.524
好了 我们开始创建吧

00:07:27.524 --> 00:07:30.649
首先 导入要用的包

00:07:30.649 --> 00:07:33.680
我们要用 text8 数据集

00:07:33.680 --> 00:07:37.399
这是一批维基百科的文章

00:07:37.399 --> 00:07:42.014
已经清理好了 可以用来建立 Skip-gram 了

00:07:42.014 --> 00:07:44.959
要训练 word2vec

00:07:44.959 --> 00:07:48.949
你需要一个十分庞大的文本数据集

00:07:48.949 --> 00:07:51.439
如维基百科的文章、

00:07:51.439 --> 00:07:54.550
新闻报道、书籍等等

00:07:54.550 --> 00:07:58.769
因为这些文本有很多语言版本

00:07:58.769 --> 00:08:03.410
所以你可以得到这些词的优质表征和大量完整的

00:08:03.410 --> 00:08:05.435
不同语言版本

00:08:05.435 --> 00:08:08.459
并将这些表征数据作为任意模型的嵌入层

00:08:08.459 --> 00:08:10.939
现在我要挑一种语言的嵌入层来讲训练方法

00:08:10.939 --> 00:08:15.310
以便你掌握相关知识 在未来用于自己的模型

00:08:15.310 --> 00:08:18.555
这些代码的作用是进行一些预处理

00:08:18.555 --> 00:08:22.040
也就是清理标点符号

00:08:22.040 --> 00:08:24.620
将标点符号转化为令牌

00:08:24.620 --> 00:08:30.699
比如说把句号转化为 PERIOD 并用括号将其括起来

00:08:30.699 --> 00:08:37.519
这么做是因为你得把任何东西都以单词表示出来

00:08:37.519 --> 00:08:41.120
因为当你遇到一些文本 比如说令牌和这个逗号

00:08:41.120 --> 00:08:46.070
你需要用同一个词表示没有接着逗号的令牌和接着逗号的令牌

00:08:46.070 --> 00:08:48.409
所以你得把这个逗号分离出来

00:08:48.408 --> 00:08:51.319
用单词 “COMMA” 来替换这个逗号

00:08:51.320 --> 00:08:57.879
这还有利于你解决其它问题 比如生成新文本等

00:08:57.879 --> 00:09:02.149
我还要把出现次数小于或等于五次的单词删掉

00:09:02.149 --> 00:09:07.673
因为这类单词很少见 只会增加数据噪声

00:09:07.673 --> 00:09:11.110
损害向量表征的质量

00:09:11.110 --> 00:09:14.710
毕竟它们出现的频率并不高

00:09:14.710 --> 00:09:18.889
会对训练造成不良影响

00:09:18.889 --> 00:09:25.019
这些操作我都写在这个 utils 模块里了 你可以看看

00:09:25.019 --> 00:09:29.870
然后创建两个字典

00:09:29.870 --> 00:09:35.649
一个把单词转为整数 一个把整数转回单词

00:09:35.649 --> 00:09:38.465
用字典把所有文本单词转为整数

00:09:38.465 --> 00:09:42.340
所得结果就是 int_words

00:09:42.340 --> 00:09:46.418
Int_words 其实是一个记录数据集所有单词的长列表

00:09:46.418 --> 00:09:48.879
但因为单词已经转为整数

00:09:48.879 --> 00:09:52.514
所以我们可以将其输入到网络里了

00:09:52.514 --> 00:09:56.879
记住 在做查询表时

00:09:56.879 --> 00:09:58.259
实际上我们输入的是整数

00:09:58.259 --> 00:09:59.490
而不是单词

00:09:59.490 --> 00:10:02.058
所以我把所有单词都令牌化了

00:10:02.058 --> 00:10:06.073
然后等到我们开始创建网络 建立嵌入查询表时

00:10:06.073 --> 00:10:11.209
我们就可以输入整数了 这样就能用整数建立查询表了

00:10:11.210 --> 00:10:13.980
接下来你要做的第一件事

00:10:13.980 --> 00:10:18.778
文本里有很多单词

00:10:18.778 --> 00:10:21.720
没什么意思

00:10:21.720 --> 00:10:25.230
或不能为周围的单词提供什么语境信息

00:10:25.230 --> 00:10:27.870
比如 the、of、and 和 for

00:10:27.870 --> 00:10:31.168
这种单词出现频率很高

00:10:31.168 --> 00:10:35.829
但不能为周围的单词提供什么语境信息

00:10:35.830 --> 00:10:41.075
所以我们可以直接删除这类高频词

00:10:41.075 --> 00:10:44.570
这样就能减少数据噪声

00:10:44.570 --> 00:10:50.000
提高训练率 最后得到的表征效果也更好

00:10:50.000 --> 00:10:55.168
这种舍弃高频单词的过程叫做子采样

00:10:55.168 --> 00:10:58.860
子采样时 我们要计算频率

00:10:58.860 --> 00:11:03.928
从而获得一个舍弃每个高频单词的概率

00:11:03.928 --> 00:11:09.480
t 是阀值参数 能让我们设定阀值

00:11:09.480 --> 00:11:17.100
也就是设定什么频率的单词会被以某种概率舍弃

00:11:17.100 --> 00:11:21.930
基本思路就是 计算每个单词的频率

00:11:21.929 --> 00:11:27.269
进而计算要舍弃该词的概率

00:11:27.269 --> 00:11:31.235
注意 这是个概率

00:11:31.235 --> 00:11:33.668
所以浏览文本

00:11:33.668 --> 00:11:36.519
你看到的单词有一定概率会被舍弃

00:11:36.519 --> 00:11:43.778
也有一定概率得到保留 整个数据集的随机舍弃概率应该相同

00:11:43.778 --> 00:11:46.134
所以当我们成批输入时

00:11:46.134 --> 00:11:48.759
第一批

00:11:48.759 --> 00:11:54.028
和最后一批单词被舍弃的概率应该相同

00:11:54.028 --> 00:11:57.480
好了 接下来的事就交给你了

00:11:57.480 --> 00:12:01.985
你需要实现子采样 处理 int_words 里的单词

00:12:01.985 --> 00:12:08.730
虽然这更像是编程挑战 而不像实际的深度学习

00:12:08.730 --> 00:12:14.350
但对你吸取数据准备经验是很重要的

00:12:14.350 --> 00:12:16.500
在处理深度学习和机器学习时

00:12:16.500 --> 00:12:20.528
准备数据是常事

00:12:20.528 --> 00:12:22.960
子采样的思路就是遍历数据集 int_words

00:12:22.960 --> 00:12:26.004
以这里给定的概率舍弃每个单词

00:12:26.004 --> 00:12:32.705
然后将子采样的数据结果赋给数据集 train_words

00:12:32.705 --> 00:12:36.663
请放手去做吧 我在 notebook 里写了解决方案

00:12:36.663 --> 00:12:37.750
如果你想知道我是怎么实现的

00:12:37.750 --> 00:12:43.019
我也做了一个方案视频 做完练习后你可以去观看一下 加油！

