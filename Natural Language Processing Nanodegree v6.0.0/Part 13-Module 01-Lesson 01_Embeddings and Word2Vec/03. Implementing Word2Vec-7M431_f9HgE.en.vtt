WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.230
Welcome back everyone. So this week you're going to be exploring

00:00:05.230 --> 00:00:10.390
embeddings and implementing the word2vec model to understand them.

00:00:10.390 --> 00:00:16.460
So in this notebook you'll actually be using TensorFlow to implement the word2vec model.

00:00:16.460 --> 00:00:20.160
Here are some resources for you to read.

00:00:20.160 --> 00:00:23.449
So you should check these out either beforehand or while you're

00:00:23.449 --> 00:00:27.059
working on this to get a better understanding of what's going on.

00:00:27.059 --> 00:00:32.314
So here is a great conceptual overview of word2vec by Chris McCormick and these are

00:00:32.314 --> 00:00:38.118
two papers from Mikolov and the others who worked on this architecture.

00:00:38.118 --> 00:00:41.149
So this is the first one, the original one and this is a paper with a bunch of

00:00:41.149 --> 00:00:45.140
improvements which you'll also be implementing in this notebook.

00:00:45.140 --> 00:00:52.880
Then here are two implementations and this one is from the TensorFlow documentation.

00:00:52.880 --> 00:00:57.679
So when you're dealing with text and you split things up into words

00:00:57.679 --> 00:01:03.314
you tend to have tens of thousands of different words and a large dataset.

00:01:03.314 --> 00:01:05.750
So when you're using these words as your input

00:01:05.750 --> 00:01:08.534
you're typically going to be one-hot encoding them.

00:01:08.534 --> 00:01:11.060
What that means is that you have these giant vectors that

00:01:11.060 --> 00:01:13.715
are like 50,000 units long right,

00:01:13.715 --> 00:01:18.635
elements long and only one of them is set to one and all the others are set to zero.

00:01:18.635 --> 00:01:21.219
So then when you do

00:01:21.218 --> 00:01:25.238
this matrix multiplication to get the values of the hidden layer you're doing

00:01:25.239 --> 00:01:27.415
this massive multiplication that's like

00:01:27.415 --> 00:01:34.450
50,000x300 different weight parameters in this giant matrix.

00:01:34.450 --> 00:01:39.290
And the deal is when you do this matrix multiplication only one of these is set to one.

00:01:39.290 --> 00:01:46.443
So you just get zeros out of most of these operations.

00:01:46.442 --> 00:01:52.259
So this is just completely computationally inefficient.

00:01:52.260 --> 00:01:58.319
So, to solve this problem we can use what are called embeddings.

00:01:58.319 --> 00:02:05.105
Embeddings are basically just a shortcut for doing this matrix multiplication.

00:02:05.105 --> 00:02:07.525
Everything is basically the same.

00:02:07.525 --> 00:02:09.525
You have a hidden layer and you have this,

00:02:09.525 --> 00:02:12.639
you know, weight matrix from the inputs to your hidden layer.

00:02:12.639 --> 00:02:14.430
It's just that the way that we actually get

00:02:14.430 --> 00:02:17.930
the values for this hidden layer is different.

00:02:17.930 --> 00:02:21.245
So what we do is we actually just kind of skip

00:02:21.245 --> 00:02:25.128
this entire matrix multiplication and just look

00:02:25.128 --> 00:02:29.758
up from a table what the values for this hidden layer is.

00:02:29.758 --> 00:02:35.149
And we could do this because we are multiplying a one-hot encoded vector by a matrix,

00:02:35.150 --> 00:02:40.270
you just get the row that corresponds to the element that was on.

00:02:40.270 --> 00:02:42.819
So for instance if the fourth element here

00:02:42.818 --> 00:02:46.239
is 1 and all the rest are 0 then you're only going to get

00:02:46.240 --> 00:02:48.935
the fourth row from this matrix because

00:02:48.935 --> 00:02:52.710
all the other multiplications through here are zero.

00:02:52.710 --> 00:02:59.145
So basically what that means is that to do this matrix multiplication,

00:02:59.145 --> 00:03:01.340
we know that this is the fourth element,

00:03:01.340 --> 00:03:05.069
so we can just get the fourth row and that is the result.

00:03:05.068 --> 00:03:08.609
So we don't actually have to do the matrix multiplication at all.

00:03:08.610 --> 00:03:13.229
So then what we do is we actually tokenize our words.

00:03:13.229 --> 00:03:17.729
That means we convert them into integers and then we still have this,

00:03:17.729 --> 00:03:22.064
you know, embedding weight matrix that is going from an input to a hidden layer.

00:03:22.063 --> 00:03:27.000
But now we're just calling this a lookup table because we literally just look up

00:03:27.000 --> 00:03:32.425
the row and this matrix that corresponds to our word.

00:03:32.425 --> 00:03:36.830
And that is the values of our hidden layer.

00:03:36.830 --> 00:03:41.094
So then the number of units in the hidden layer is called the embedding dimension.

00:03:41.093 --> 00:03:44.114
And basically you just get a different vector,

00:03:44.115 --> 00:03:48.490
that is the size of your embedding dimension for each of your words.

00:03:48.490 --> 00:03:53.085
So then, in this way we just completely skip this matrix multiplication.

00:03:53.085 --> 00:03:59.064
All we're doing is we're saying you know this word heart is now this integer 958 and then

00:03:59.063 --> 00:04:02.318
the matrix multiplication just gives us this row so we're just going to look up

00:04:02.318 --> 00:04:07.573
the 958 row and we're going to use that as our hidden layer values.

00:04:07.574 --> 00:04:11.930
So there's really nothing magical going on with embeddings.

00:04:11.930 --> 00:04:14.919
The embedding lookup table is just a weight matrix.

00:04:14.919 --> 00:04:17.350
The embedding layer is a hidden layer and then

00:04:17.350 --> 00:04:22.588
this lookup is just a shortcut for the matrix multiplication.

00:04:22.588 --> 00:04:25.615
And then this lookup table since it's just a weight matrix,

00:04:25.615 --> 00:04:30.639
it's trained like any other weight matrix that you've used before just using back

00:04:30.639 --> 00:04:34.000
propagation to learn better parameters

00:04:34.000 --> 00:04:38.254
that help you classify or predict whatever you're trying to do.

00:04:38.254 --> 00:04:41.745
So embeddings aren't only used for words you can

00:04:41.745 --> 00:04:46.134
basically use them anywhere you have a massive number of classes.

00:04:46.134 --> 00:04:52.259
So what we'll be doing in this notebook is looking at a particular type of model called

00:04:52.259 --> 00:04:55.230
word2vec that uses the embedding layer to get

00:04:55.230 --> 00:04:58.564
these like representations of these words from these vectors.

00:04:58.564 --> 00:05:01.560
So each of these vectors in here is going to represent

00:05:01.560 --> 00:05:05.670
a word and it's going to actually represent semantic meaning of those words.

00:05:05.670 --> 00:05:09.245
So word2vec is an algorithm that finds

00:05:09.245 --> 00:05:15.605
efficient representations by using these vectors from the embedding layer.

00:05:15.605 --> 00:05:17.399
And when it's properly trained,

00:05:17.399 --> 00:05:19.920
the vector will contain semantic information.

00:05:19.920 --> 00:05:23.475
So what we mean by that is that words that show up in similar contexts,

00:05:23.475 --> 00:05:25.949
like colors like black white and red,

00:05:25.949 --> 00:05:29.220
they're going to show up in similar context and so those words are

00:05:29.220 --> 00:05:32.579
going to have similar representations and you know these vectors,

00:05:32.579 --> 00:05:35.370
the value of the vectors is going to be similar between

00:05:35.370 --> 00:05:39.060
these colors because they show up in similar contexts.

00:05:39.060 --> 00:05:42.884
So there are two architectures for implementing word2vec.

00:05:42.884 --> 00:05:44.410
So there is CBOW.

00:05:44.410 --> 00:05:47.213
This is the Continuous Bag Of Words.

00:05:47.213 --> 00:05:49.699
There's also Skip-gram.

00:05:49.699 --> 00:05:53.225
And Skip-gram is going to be the one that we're looking at because

00:05:53.225 --> 00:05:57.990
it has been shown to work better than CBOW.

00:05:57.990 --> 00:06:00.540
So the idea here is that you

00:06:00.540 --> 00:06:04.649
have some word that you're looking at in your in your dataset,

00:06:04.649 --> 00:06:07.410
right, so at some step in your dataset t,

00:06:07.410 --> 00:06:12.899
then you are looking at words that are around that target where at t. So t-2,

00:06:12.899 --> 00:06:15.418
which is like two before it and t+1,

00:06:15.418 --> 00:06:19.524
which is the word after your target word.

00:06:19.524 --> 00:06:23.235
So this sort of represents the context that this word is showing up in.

00:06:23.235 --> 00:06:25.269
So in CBOW your input is

00:06:25.269 --> 00:06:28.389
the context around your word and then you're trying to predict the word.

00:06:28.389 --> 00:06:33.069
So you're like looking around some window in your text and you're trying

00:06:33.069 --> 00:06:36.338
using that as the context and you're

00:06:36.338 --> 00:06:40.434
trying to predict the word that shows up in the middle of this window.

00:06:40.434 --> 00:06:43.175
In skip-gram that's inverted.

00:06:43.175 --> 00:06:45.730
You're passing in some word and then you're trying to

00:06:45.730 --> 00:06:49.660
predict the words that show up around it in the text.

00:06:49.660 --> 00:06:53.896
So you're trying to predict like what context does this word appear in.

00:06:53.896 --> 00:06:57.471
So in this way you're going to be

00:06:57.471 --> 00:07:03.879
training this network to understand the context and the meaning of words.

00:07:03.879 --> 00:07:05.379
So for instance, again like black, white,

00:07:05.379 --> 00:07:08.199
and red, they're going to show up in a similar context.

00:07:08.199 --> 00:07:10.814
They have similar like words surrounding it.

00:07:10.814 --> 00:07:13.834
And so then the passing in black and white and

00:07:13.834 --> 00:07:17.800
red is going to predict to the same words and the context.

00:07:17.800 --> 00:07:20.379
And so they're going to have similar projections.

00:07:20.379 --> 00:07:24.144
They're going to have similar embedding vectors.

00:07:24.144 --> 00:07:27.524
OK, so let's get started building this.

00:07:27.524 --> 00:07:30.649
So first we just -- and part of the package that we're using -- and

00:07:30.649 --> 00:07:33.680
then here we're using the text8 data set.

00:07:33.680 --> 00:07:37.399
So this is a bunch of Wikipedia articles that have been cleaned

00:07:37.399 --> 00:07:42.014
up and they're nice to use for this purpose.

00:07:42.014 --> 00:07:44.959
So in general for training word2vec,

00:07:44.959 --> 00:07:48.949
you're going to want a really big dataset of text,

00:07:48.949 --> 00:07:51.439
so a lot of times is like Wikipedia articles,

00:07:51.439 --> 00:07:54.550
news articles, books, that sort of thing.

00:07:54.550 --> 00:07:58.769
And this has been done for a whole bunch of different languages.

00:07:58.769 --> 00:08:03.410
And so you can get these nice representations of words and a whole bunch of

00:08:03.410 --> 00:08:05.435
different languages and it just sort of

00:08:05.435 --> 00:08:08.459
use it as the embedding layer whatever model you're doing.

00:08:08.459 --> 00:08:10.939
But here I'm going to show you how to train- actually train up one of

00:08:10.939 --> 00:08:15.310
these embedding layers so that you can use it in your future models.

00:08:15.310 --> 00:08:18.555
So here just doing a little bit of pre-processing

00:08:18.555 --> 00:08:22.040
so that sort of clean up the punctuation,

00:08:22.040 --> 00:08:24.620
so turning punctuation into tokens.

00:08:24.620 --> 00:08:30.699
So for example, a period is changed to the word period with some brackets around it.

00:08:30.699 --> 00:08:37.519
So the reason want to do this is because you want to represent everything as words.

00:08:37.519 --> 00:08:41.120
And when you have something like tokens and this comma,

00:08:41.120 --> 00:08:46.070
you want to represent tokens without a comma and tokens with a comma as the same word.

00:08:46.070 --> 00:08:48.409
So basically what you do you split off this comma and then you just

00:08:48.408 --> 00:08:51.319
replace it with the word comma and then this will help

00:08:51.320 --> 00:08:57.879
you with other problems such as generating new text or something.

00:08:57.879 --> 00:09:02.149
So here I am also removing words that show up five or fewer times.

00:09:02.149 --> 00:09:07.673
So it's just really rare words that sort of just add noise to the data and

00:09:07.673 --> 00:09:11.110
they kind of harm the quality of

00:09:11.110 --> 00:09:14.710
your vector representations because they don't show very often.

00:09:14.710 --> 00:09:18.889
So that kind of confuses the training.

00:09:18.889 --> 00:09:25.019
So all these is in this utils module that I wrote, so check that out.

00:09:25.019 --> 00:09:29.870
Then here I am creating two dictionaries that

00:09:29.870 --> 00:09:35.649
convert words to integers and then integers back to words.

00:09:35.649 --> 00:09:38.465
And then using that to convert all of our words which are

00:09:38.465 --> 00:09:42.340
just text converting them into integers and so now we have int_words.

00:09:42.340 --> 00:09:46.418
So basically this is just a long list of all the words in our dataset.

00:09:46.418 --> 00:09:48.879
But they've been converted to integers and so now we

00:09:48.879 --> 00:09:52.514
can actually pass these into our network.

00:09:52.514 --> 00:09:56.879
Because remember when we're doing our lookups,

00:09:56.879 --> 00:09:58.259
we're actually passing in integers,

00:09:58.259 --> 00:09:59.490
we're not passing in the words.

00:09:59.490 --> 00:10:02.058
So I've basically tokenized all the words.

00:10:02.058 --> 00:10:06.073
And now when we build our network and actually do this embedding lookup,

00:10:06.073 --> 00:10:11.209
we're passing in integers and we can do to lookup with these integers.

00:10:11.210 --> 00:10:13.980
OK, so now the first thing for you to do.

00:10:13.980 --> 00:10:18.778
So there's a lot of words that show up in text that don't

00:10:18.778 --> 00:10:21.720
have a lot of like meaning or

00:10:21.720 --> 00:10:25.230
they don't provide a lot of context for the words that show up around it.

00:10:25.230 --> 00:10:27.870
So like the, of, and, for,

00:10:27.870 --> 00:10:31.168
you know like all these words just show up a lot but they don't really provide

00:10:31.168 --> 00:10:35.829
a lot of context for the words that are around them.

00:10:35.830 --> 00:10:41.075
So what we can do is just get rid of some of these really frequent words.

00:10:41.075 --> 00:10:44.570
And what this does it removes noise from our data and then it

00:10:44.570 --> 00:10:50.000
improves the training rate and we can get better representations in the end.

00:10:50.000 --> 00:10:55.168
This process of discarding free words is called the subsampling.

00:10:55.168 --> 00:10:58.860
And the way we do this is we calculate some frequency

00:10:58.860 --> 00:11:03.928
and we have a probability to drop each of these words.

00:11:03.928 --> 00:11:09.480
This t is a threshold parameter that basically let's us kind of set a threshold

00:11:09.480 --> 00:11:17.100
for how frequent a word needs to be for us to drop it with some probability.

00:11:17.100 --> 00:11:21.930
So the idea is that for every word you calculate the frequency and then

00:11:21.929 --> 00:11:27.269
you calculate the probability that you would drop this word.

00:11:27.269 --> 00:11:31.235
And so it's important to know that this is a probability.

00:11:31.235 --> 00:11:33.668
So basically as you're scanning through and you see

00:11:33.668 --> 00:11:36.519
a word there's some probability that you'll drop it or keep

00:11:36.519 --> 00:11:43.778
it and you want to make this like randomly uniform over the entire dataset.

00:11:43.778 --> 00:11:46.134
So when we're passing in batches,

00:11:46.134 --> 00:11:48.759
there is the equal probability that a word will be dropped in

00:11:48.759 --> 00:11:54.028
the first batch as there is in the last batch.

00:11:54.028 --> 00:11:57.480
OK, so I'm going to leave it up to you here to

00:11:57.480 --> 00:12:01.985
implement subsampling and for the words that are in int_words.

00:12:01.985 --> 00:12:08.730
So this is more of a programming challenge than anything like actually deep learning,

00:12:08.730 --> 00:12:14.350
but it's really important for you to get experience with preparing your data.

00:12:14.350 --> 00:12:16.500
This is something you'll have to do often in

00:12:16.500 --> 00:12:20.528
deep learning and just machine learning in general.

00:12:20.528 --> 00:12:22.960
So the idea here is to go through

00:12:22.960 --> 00:12:26.004
int_words and then discard each word with the probability

00:12:26.004 --> 00:12:32.705
given here and assign your new subsample data to train words.

00:12:32.705 --> 00:12:36.663
Go and work on this and I have a solution notebook,

00:12:36.663 --> 00:12:37.750
if you want to see how I did it.

00:12:37.750 --> 00:12:43.019
And I also have a solution video that you can check out after this. Cheers!

